{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10378,
     "status": "ok",
     "timestamp": 1690881402926,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "MaFkWNOXPVwt",
    "outputId": "ff6789b3-6446-45b0-ce5a-26bc638ce0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utilsADCN import mnistLoader, plotPerformance\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.autograd.variable import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "num_workers = 0\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mnist_vae\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import gc\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1690881402927,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "v2Y-j-wqkcfY"
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1690881402928,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "xK27YP6dZgPc"
   },
   "outputs": [],
   "source": [
    "seed = 216\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) \n",
    "np.random.seed(seed)  \n",
    "random.seed(seed)  \n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4800,
     "status": "ok",
     "timestamp": 1690881407722,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "OqYL8EH7Qfxw"
   },
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "# load the training and test datasets\n",
    "labeledData   = datasets.MNIST(root='data', train=False,download=True, transform=transform)\n",
    "unlabeledData = datasets.MNIST(root='data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9214,
     "status": "ok",
     "timestamp": 1690881416932,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "RPIWRijBQjkA",
    "outputId": "07a4e50a-0f26-4636-c975-dae5a0703bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output:  10\n",
      "Number of labeled data:  5000\n",
      "Number of unlabeled data:  65000\n",
      "Number of unlabeled data batch:  65\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "dataStream = mnistLoader(labeledData, unlabeledData, nEachClassSamples = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42148,
     "status": "ok",
     "timestamp": 1690881459075,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "QESft0AWQqyv",
    "outputId": "f47d1806-64c5-431c-b8ac-11cc6b2d606f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of task:  4\n",
      "Number of labeled data per task:  1250\n",
      "Number of unlabeled data per task:  15000\n",
      "Number of unlabeled data batch per task:  15\n",
      "Number of unlabeled data test:  4000\n"
     ]
    }
   ],
   "source": [
    "### Create the datastream\n",
    "dataStream.createTask(nTask = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1690881459077,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "Uvi63oIKUhUZ",
    "outputId": "efbd623e-e201-4d12-d3d0-56e0c5b52051"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataStream.unlabeledData[3][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1690881459078,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "sqx2gTOEQwGx",
    "outputId": "4fc17445-304d-4fc9-be20-7179df82017b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f248dc3ab80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.transpose(dataStream.unlabeledData[1],1,2)\n",
    "X = torch.transpose(X,2,3)\n",
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X[9887])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzg0XdCjSOA9"
   },
   "source": [
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1690881459079,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "mAagRS4B0Mk0"
   },
   "outputs": [],
   "source": [
    "### Dataloader for different tasks\n",
    "def trn_loader(task_id,batch_size):\n",
    "  # task_id = 0\n",
    "  x_samples = dataStream.unlabeledData[task_id].to(torch.float32)\n",
    "  y_samples = dataStream.unlabeledLabel[task_id].to(torch.uint8)\n",
    "\n",
    "  train_data = TensorDataset(x_samples,y_samples)\n",
    "  train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "  return train_loader, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1690881459079,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "QlvijlBRURR9"
   },
   "outputs": [],
   "source": [
    "def metrics_aa_gm(ypred, ytrue):\n",
    "    cm = confusion_matrix(ytrue, ypred)\n",
    "    # print(confusion_matrix(ytrue, ypred))\n",
    "    sum_classes = np.sum(cm, axis=1)\n",
    "    true_pred = np.diagonal(cm)\n",
    "    tp_rate = true_pred/sum_classes\n",
    "    ACSA = np.mean(tp_rate)\n",
    "    GM = np.prod(tp_rate)**(1/cm.shape[0])\n",
    "    return ACSA, GM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzeJ5Og-SlOY"
   },
   "source": [
    "#### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1690881459080,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "Myl7XuYvc7dQ"
   },
   "outputs": [],
   "source": [
    "## Loss function\n",
    "def s_loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "#     reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    w_loss = x.mean() -x_hat.mean() #mean(x,x_hat) # K.mean(y_true * y_pred)\n",
    "    beta = 1.2\n",
    "\n",
    "    return reproduction_loss + beta*KLD + w_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1690881459081,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "jfBCOcb5dNyg"
   },
   "outputs": [],
   "source": [
    "## Train VAE\n",
    "def vae_train(train_loader, model,optimzr,epochs,batchSize,x_dim):\n",
    "  # Training\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "      train_loss = 0\n",
    "      for batch_idx, (x, _) in enumerate(train_loader):\n",
    "          x = x.view(batchSize, x_dim)\n",
    "          x = x.to(DEVICE)\n",
    "          x_hat, mean, log_var = model(x)\n",
    "          loss = s_loss_function(x, x_hat, mean, log_var)\n",
    "\n",
    "          train_loss += loss.item()\n",
    "          loss.backward()\n",
    "          optimzr.step()\n",
    "          optimzr.zero_grad()\n",
    "      print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tTraining Loss: \", train_loss / (batch_idx*batchSize),)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1690881459081,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "QxLbzZI3eBjw"
   },
   "outputs": [],
   "source": [
    "## Convert to low dimensional latent data\n",
    "def latent_data(trn_load_ful,model,x_dim,btch_ful):\n",
    "  for batch_idx, (x_ful, y_ful) in enumerate(trn_load_ful):\n",
    "    x_ful = x_ful.view(btch_ful,x_dim)\n",
    "    x_ful = x_ful.to(DEVICE)\n",
    "    y_ful = y_ful.to(DEVICE)\n",
    "  print(x_ful.shape)\n",
    "\n",
    "  model.eval()\n",
    "  x_out, z_mean_vae, _ = model(x_ful)\n",
    "  return x_ful, y_ful, x_out, z_mean_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1690881459082,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "Oa-26hh0Jzaz"
   },
   "outputs": [],
   "source": [
    "#### Structure of latent data capturing\n",
    "def structure_data(real_data):\n",
    "\n",
    "  P_cov = torch.cov(real_data, correction=0)\n",
    "  D, V = torch.linalg.eigh(P_cov)\n",
    "  idx = torch.argsort(D, dim=0,descending = True)\n",
    "  d = D[idx]\n",
    "  V = V[:,idx]\n",
    "  \n",
    "  return d, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1690881459092,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "H1ocsGMK3G23"
   },
   "outputs": [],
   "source": [
    "### Mapping k-means clusters to class labels\n",
    "def asgn_clust_labl(km, tr_lbls):\n",
    "    asgn_lbls = {}\n",
    "\n",
    "    for i in range(km.n_clusters):\n",
    "\n",
    "        lbls = []\n",
    "        idx = np.where(km.labels_ == i)\n",
    "        lbls.append(tr_lbls[idx])\n",
    "\n",
    "        if (len(lbls[0]) == 1):\n",
    "            cnts = np.bincount(lbls[0])\n",
    "        else:\n",
    "            cnts = np.bincount(np.squeeze(lbls))\n",
    "\n",
    "        if np.argmax(cnts) in asgn_lbls:\n",
    "            asgn_lbls[np.argmax(cnts)].append(i)\n",
    "        else:\n",
    "            asgn_lbls[np.argmax(cnts)] = [i]\n",
    "\n",
    "    return asgn_lbls\n",
    "\n",
    "def data_lbls(X_lbls, clust_lbls):\n",
    "\n",
    "    p_lbls = np.zeros(len(X_lbls)).astype(np.uint8)\n",
    "\n",
    "    for i, clus in enumerate(X_lbls):\n",
    "        for key,value in clust_lbls.items():\n",
    "            if clus in value:\n",
    "                p_lbls[i] = key\n",
    "\n",
    "    return p_lbls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1690881459093,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "8M50rQR-NV0z"
   },
   "outputs": [],
   "source": [
    "# z_Ftrans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122322,
     "status": "ok",
     "timestamp": 1690881581382,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "vom0VEQVlOGy",
    "outputId": "f6d205ac-1407-4824-c074-141d2f2ac70f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\tEpoch 1 complete! \tTraining Loss:  542.9034174107143\n",
      "\tEpoch 2 complete! \tTraining Loss:  370.7803125\n",
      "\tEpoch 3 complete! \tTraining Loss:  257.1727633928571\n",
      "\tEpoch 4 complete! \tTraining Loss:  233.99088169642857\n",
      "\tEpoch 5 complete! \tTraining Loss:  227.52287388392858\n",
      "\tEpoch 6 complete! \tTraining Loss:  224.59891741071428\n",
      "\tEpoch 7 complete! \tTraining Loss:  222.51980691964286\n",
      "\tEpoch 8 complete! \tTraining Loss:  220.95993191964286\n",
      "\tEpoch 9 complete! \tTraining Loss:  219.37572544642856\n",
      "\tEpoch 10 complete! \tTraining Loss:  217.65664620535713\n",
      "\tEpoch 11 complete! \tTraining Loss:  215.67457700892857\n",
      "\tEpoch 12 complete! \tTraining Loss:  213.48030133928572\n",
      "\tEpoch 13 complete! \tTraining Loss:  211.76489508928572\n",
      "\tEpoch 14 complete! \tTraining Loss:  210.44920758928572\n",
      "\tEpoch 15 complete! \tTraining Loss:  209.47683035714286\n",
      "\tEpoch 16 complete! \tTraining Loss:  208.73466294642859\n",
      "\tEpoch 17 complete! \tTraining Loss:  207.68139174107142\n",
      "\tEpoch 18 complete! \tTraining Loss:  206.37354129464285\n",
      "\tEpoch 19 complete! \tTraining Loss:  204.3113013392857\n",
      "\tEpoch 20 complete! \tTraining Loss:  202.2075390625\n",
      "\tEpoch 21 complete! \tTraining Loss:  200.36679241071428\n",
      "\tEpoch 22 complete! \tTraining Loss:  198.62126227678573\n",
      "\tEpoch 23 complete! \tTraining Loss:  196.78416964285714\n",
      "\tEpoch 24 complete! \tTraining Loss:  194.57904129464285\n",
      "\tEpoch 25 complete! \tTraining Loss:  191.48322879464286\n",
      "\tEpoch 26 complete! \tTraining Loss:  188.16639732142858\n",
      "\tEpoch 27 complete! \tTraining Loss:  185.1912734375\n",
      "\tEpoch 28 complete! \tTraining Loss:  182.74903348214286\n",
      "\tEpoch 29 complete! \tTraining Loss:  180.5566908482143\n",
      "\tEpoch 30 complete! \tTraining Loss:  178.82740625\n",
      "\tEpoch 31 complete! \tTraining Loss:  177.57858147321429\n",
      "\tEpoch 32 complete! \tTraining Loss:  176.40917522321428\n",
      "\tEpoch 33 complete! \tTraining Loss:  175.41593191964284\n",
      "\tEpoch 34 complete! \tTraining Loss:  174.57226004464286\n",
      "\tEpoch 35 complete! \tTraining Loss:  173.6522578125\n",
      "\tEpoch 36 complete! \tTraining Loss:  172.69059598214287\n",
      "\tEpoch 37 complete! \tTraining Loss:  171.7379720982143\n",
      "\tEpoch 38 complete! \tTraining Loss:  170.60243526785715\n",
      "\tEpoch 39 complete! \tTraining Loss:  169.445578125\n",
      "\tEpoch 40 complete! \tTraining Loss:  168.41320535714286\n",
      "\tEpoch 41 complete! \tTraining Loss:  167.1136049107143\n",
      "\tEpoch 42 complete! \tTraining Loss:  166.00863392857144\n",
      "\tEpoch 43 complete! \tTraining Loss:  165.049296875\n",
      "\tEpoch 44 complete! \tTraining Loss:  163.94082700892858\n",
      "\tEpoch 45 complete! \tTraining Loss:  163.05527008928573\n",
      "\tEpoch 46 complete! \tTraining Loss:  162.04188839285715\n",
      "\tEpoch 47 complete! \tTraining Loss:  161.0720357142857\n",
      "\tEpoch 48 complete! \tTraining Loss:  160.4329408482143\n",
      "\tEpoch 49 complete! \tTraining Loss:  159.4404966517857\n",
      "\tEpoch 50 complete! \tTraining Loss:  158.6017388392857\n",
      "\tEpoch 51 complete! \tTraining Loss:  157.90717745535713\n",
      "\tEpoch 52 complete! \tTraining Loss:  157.0275546875\n",
      "\tEpoch 53 complete! \tTraining Loss:  156.20727901785713\n",
      "\tEpoch 54 complete! \tTraining Loss:  155.30427232142858\n",
      "\tEpoch 55 complete! \tTraining Loss:  154.52234375\n",
      "\tEpoch 56 complete! \tTraining Loss:  153.5590892857143\n",
      "\tEpoch 57 complete! \tTraining Loss:  152.65483035714286\n",
      "\tEpoch 58 complete! \tTraining Loss:  151.7448872767857\n",
      "\tEpoch 59 complete! \tTraining Loss:  150.970546875\n",
      "\tEpoch 60 complete! \tTraining Loss:  150.16962611607144\n",
      "\tEpoch 61 complete! \tTraining Loss:  149.48911495535714\n",
      "\tEpoch 62 complete! \tTraining Loss:  148.72391852678572\n",
      "\tEpoch 63 complete! \tTraining Loss:  147.89287834821428\n",
      "\tEpoch 64 complete! \tTraining Loss:  147.11449441964285\n",
      "\tEpoch 65 complete! \tTraining Loss:  146.4820814732143\n",
      "\tEpoch 66 complete! \tTraining Loss:  145.78968638392857\n",
      "\tEpoch 67 complete! \tTraining Loss:  145.19591183035715\n",
      "\tEpoch 68 complete! \tTraining Loss:  144.62962611607142\n",
      "\tEpoch 69 complete! \tTraining Loss:  143.90771651785715\n",
      "\tEpoch 70 complete! \tTraining Loss:  143.54447879464286\n",
      "\tEpoch 71 complete! \tTraining Loss:  142.94855022321428\n",
      "\tEpoch 72 complete! \tTraining Loss:  142.45899720982143\n",
      "\tEpoch 73 complete! \tTraining Loss:  141.85289676339286\n",
      "\tEpoch 74 complete! \tTraining Loss:  141.42891517857143\n",
      "\tEpoch 75 complete! \tTraining Loss:  140.93436328125\n",
      "\tEpoch 76 complete! \tTraining Loss:  140.44937109375\n",
      "\tEpoch 77 complete! \tTraining Loss:  139.89961830357143\n",
      "\tEpoch 78 complete! \tTraining Loss:  139.48322879464286\n",
      "\tEpoch 79 complete! \tTraining Loss:  139.01169642857144\n",
      "\tEpoch 80 complete! \tTraining Loss:  138.6416841517857\n",
      "\tEpoch 81 complete! \tTraining Loss:  138.16064118303572\n",
      "\tEpoch 82 complete! \tTraining Loss:  137.72447154017857\n",
      "\tEpoch 83 complete! \tTraining Loss:  137.31629910714287\n",
      "\tEpoch 84 complete! \tTraining Loss:  136.9385345982143\n",
      "\tEpoch 85 complete! \tTraining Loss:  136.5744614955357\n",
      "\tEpoch 86 complete! \tTraining Loss:  136.21217075892858\n",
      "\tEpoch 87 complete! \tTraining Loss:  135.77181752232144\n",
      "\tEpoch 88 complete! \tTraining Loss:  135.51904910714285\n",
      "\tEpoch 89 complete! \tTraining Loss:  135.19384988839286\n",
      "\tEpoch 90 complete! \tTraining Loss:  134.89259542410716\n",
      "\tEpoch 91 complete! \tTraining Loss:  134.53521875\n",
      "\tEpoch 92 complete! \tTraining Loss:  134.31061160714285\n",
      "\tEpoch 93 complete! \tTraining Loss:  133.9710200892857\n",
      "\tEpoch 94 complete! \tTraining Loss:  133.65850558035714\n",
      "\tEpoch 95 complete! \tTraining Loss:  133.48227790178572\n",
      "\tEpoch 96 complete! \tTraining Loss:  133.08048660714286\n",
      "\tEpoch 97 complete! \tTraining Loss:  132.83661997767857\n",
      "\tEpoch 98 complete! \tTraining Loss:  132.59576171875\n",
      "\tEpoch 99 complete! \tTraining Loss:  132.31966183035715\n",
      "\tEpoch 100 complete! \tTraining Loss:  132.014765625\n",
      "\tEpoch 101 complete! \tTraining Loss:  131.83577957589284\n",
      "\tEpoch 102 complete! \tTraining Loss:  131.6039955357143\n",
      "\tEpoch 103 complete! \tTraining Loss:  131.33705636160715\n",
      "\tEpoch 104 complete! \tTraining Loss:  130.99955970982143\n",
      "\tEpoch 105 complete! \tTraining Loss:  130.7862734375\n",
      "\tEpoch 106 complete! \tTraining Loss:  130.60166629464285\n",
      "\tEpoch 107 complete! \tTraining Loss:  130.35036886160714\n",
      "\tEpoch 108 complete! \tTraining Loss:  130.19641517857144\n",
      "\tEpoch 109 complete! \tTraining Loss:  129.87202901785713\n",
      "\tEpoch 110 complete! \tTraining Loss:  129.82380915178572\n",
      "\tEpoch 111 complete! \tTraining Loss:  129.52484151785714\n",
      "\tEpoch 112 complete! \tTraining Loss:  129.35747935267858\n",
      "\tEpoch 113 complete! \tTraining Loss:  129.10727511160715\n",
      "\tEpoch 114 complete! \tTraining Loss:  128.9749375\n",
      "\tEpoch 115 complete! \tTraining Loss:  128.65138671875\n",
      "\tEpoch 116 complete! \tTraining Loss:  128.56685267857142\n",
      "\tEpoch 117 complete! \tTraining Loss:  128.31807254464286\n",
      "\tEpoch 118 complete! \tTraining Loss:  128.23823549107144\n",
      "\tEpoch 119 complete! \tTraining Loss:  127.99160323660715\n",
      "\tEpoch 120 complete! \tTraining Loss:  127.79339341517857\n",
      "\tEpoch 121 complete! \tTraining Loss:  127.68451339285714\n",
      "\tEpoch 122 complete! \tTraining Loss:  127.55755691964286\n",
      "\tEpoch 123 complete! \tTraining Loss:  127.34391964285714\n",
      "\tEpoch 124 complete! \tTraining Loss:  127.13440513392857\n",
      "\tEpoch 125 complete! \tTraining Loss:  126.9339140625\n",
      "\tEpoch 126 complete! \tTraining Loss:  126.79705970982143\n",
      "\tEpoch 127 complete! \tTraining Loss:  126.62634263392857\n",
      "\tEpoch 128 complete! \tTraining Loss:  126.51906082589285\n",
      "\tEpoch 129 complete! \tTraining Loss:  126.41843080357143\n",
      "\tEpoch 130 complete! \tTraining Loss:  126.27808258928572\n",
      "\tEpoch 131 complete! \tTraining Loss:  126.15597488839286\n",
      "\tEpoch 132 complete! \tTraining Loss:  126.00255357142858\n",
      "\tEpoch 133 complete! \tTraining Loss:  125.94647377232143\n",
      "\tEpoch 134 complete! \tTraining Loss:  125.75206529017858\n",
      "\tEpoch 135 complete! \tTraining Loss:  125.55704185267857\n",
      "\tEpoch 136 complete! \tTraining Loss:  125.531265625\n",
      "\tEpoch 137 complete! \tTraining Loss:  125.27668303571429\n",
      "\tEpoch 138 complete! \tTraining Loss:  125.2301328125\n",
      "\tEpoch 139 complete! \tTraining Loss:  125.07202678571429\n",
      "\tEpoch 140 complete! \tTraining Loss:  124.99269252232143\n",
      "\tEpoch 141 complete! \tTraining Loss:  124.78546763392858\n",
      "\tEpoch 142 complete! \tTraining Loss:  124.71582421875\n",
      "\tEpoch 143 complete! \tTraining Loss:  124.54963002232142\n",
      "\tEpoch 144 complete! \tTraining Loss:  124.47076116071429\n",
      "\tEpoch 145 complete! \tTraining Loss:  124.36665401785714\n",
      "\tEpoch 146 complete! \tTraining Loss:  124.21758203125\n",
      "\tEpoch 147 complete! \tTraining Loss:  124.21938616071428\n",
      "\tEpoch 148 complete! \tTraining Loss:  124.10151450892857\n",
      "\tEpoch 149 complete! \tTraining Loss:  123.91322600446429\n",
      "\tEpoch 150 complete! \tTraining Loss:  123.79436328125\n",
      "\tEpoch 151 complete! \tTraining Loss:  123.74874665178571\n",
      "\tEpoch 152 complete! \tTraining Loss:  123.54019419642857\n",
      "\tEpoch 153 complete! \tTraining Loss:  123.4578671875\n",
      "\tEpoch 154 complete! \tTraining Loss:  123.42931752232143\n",
      "\tEpoch 155 complete! \tTraining Loss:  123.36401450892858\n",
      "\tEpoch 156 complete! \tTraining Loss:  123.22296763392858\n",
      "\tEpoch 157 complete! \tTraining Loss:  123.0729765625\n",
      "\tEpoch 158 complete! \tTraining Loss:  123.04822712053571\n",
      "\tEpoch 159 complete! \tTraining Loss:  122.9389609375\n",
      "\tEpoch 160 complete! \tTraining Loss:  122.88972377232143\n",
      "\tEpoch 161 complete! \tTraining Loss:  122.78671651785714\n",
      "\tEpoch 162 complete! \tTraining Loss:  122.68002622767857\n",
      "\tEpoch 163 complete! \tTraining Loss:  122.55676506696429\n",
      "\tEpoch 164 complete! \tTraining Loss:  122.5258203125\n",
      "\tEpoch 165 complete! \tTraining Loss:  122.38843080357142\n",
      "\tEpoch 166 complete! \tTraining Loss:  122.30523270089286\n",
      "\tEpoch 167 complete! \tTraining Loss:  122.0712890625\n",
      "\tEpoch 168 complete! \tTraining Loss:  122.04911774553571\n",
      "\tEpoch 169 complete! \tTraining Loss:  122.03657645089285\n",
      "\tEpoch 170 complete! \tTraining Loss:  121.96645647321428\n",
      "\tEpoch 171 complete! \tTraining Loss:  121.86634486607143\n",
      "\tEpoch 172 complete! \tTraining Loss:  121.80709821428572\n",
      "\tEpoch 173 complete! \tTraining Loss:  121.68030915178572\n",
      "\tEpoch 174 complete! \tTraining Loss:  121.61737388392856\n",
      "\tEpoch 175 complete! \tTraining Loss:  121.50625167410715\n",
      "\tEpoch 176 complete! \tTraining Loss:  121.36124720982143\n",
      "\tEpoch 177 complete! \tTraining Loss:  121.29836272321428\n",
      "\tEpoch 178 complete! \tTraining Loss:  121.21705915178572\n",
      "\tEpoch 179 complete! \tTraining Loss:  121.22318582589286\n",
      "\tEpoch 180 complete! \tTraining Loss:  120.99942075892857\n",
      "\tEpoch 181 complete! \tTraining Loss:  121.01606026785714\n",
      "\tEpoch 182 complete! \tTraining Loss:  120.91339564732142\n",
      "\tEpoch 183 complete! \tTraining Loss:  120.92346986607143\n",
      "\tEpoch 184 complete! \tTraining Loss:  120.83310770089285\n",
      "\tEpoch 185 complete! \tTraining Loss:  120.69984933035714\n",
      "\tEpoch 186 complete! \tTraining Loss:  120.59275948660714\n",
      "\tEpoch 187 complete! \tTraining Loss:  120.60782645089286\n",
      "\tEpoch 188 complete! \tTraining Loss:  120.54476171875\n",
      "\tEpoch 189 complete! \tTraining Loss:  120.37021763392858\n",
      "\tEpoch 190 complete! \tTraining Loss:  120.29572433035715\n",
      "\tEpoch 191 complete! \tTraining Loss:  120.22367689732143\n",
      "\tEpoch 192 complete! \tTraining Loss:  120.18430078125\n",
      "\tEpoch 193 complete! \tTraining Loss:  120.25468582589286\n",
      "\tEpoch 194 complete! \tTraining Loss:  120.07455189732143\n",
      "\tEpoch 195 complete! \tTraining Loss:  119.97619029017856\n",
      "\tEpoch 196 complete! \tTraining Loss:  119.93545814732143\n",
      "\tEpoch 197 complete! \tTraining Loss:  119.84815680803571\n",
      "\tEpoch 198 complete! \tTraining Loss:  119.77092299107143\n",
      "\tEpoch 199 complete! \tTraining Loss:  119.69569419642858\n",
      "\tEpoch 200 complete! \tTraining Loss:  119.60519029017857\n",
      "\tEpoch 201 complete! \tTraining Loss:  119.57913225446428\n",
      "\tEpoch 202 complete! \tTraining Loss:  119.57097098214285\n",
      "\tEpoch 203 complete! \tTraining Loss:  119.39454017857143\n",
      "\tEpoch 204 complete! \tTraining Loss:  119.37813337053572\n",
      "\tEpoch 205 complete! \tTraining Loss:  119.31478125\n",
      "\tEpoch 206 complete! \tTraining Loss:  119.28841238839286\n",
      "\tEpoch 207 complete! \tTraining Loss:  119.226875\n",
      "\tEpoch 208 complete! \tTraining Loss:  119.14194252232143\n",
      "\tEpoch 209 complete! \tTraining Loss:  119.15355970982142\n",
      "\tEpoch 210 complete! \tTraining Loss:  119.00661495535714\n",
      "\tEpoch 211 complete! \tTraining Loss:  119.04816796875\n",
      "\tEpoch 212 complete! \tTraining Loss:  118.94006026785715\n",
      "\tEpoch 213 complete! \tTraining Loss:  118.91736551339285\n",
      "\tEpoch 214 complete! \tTraining Loss:  118.8845546875\n",
      "\tEpoch 215 complete! \tTraining Loss:  118.85864620535715\n",
      "\tEpoch 216 complete! \tTraining Loss:  118.68719754464286\n",
      "\tEpoch 217 complete! \tTraining Loss:  118.64131082589286\n",
      "\tEpoch 218 complete! \tTraining Loss:  118.62520368303572\n",
      "\tEpoch 219 complete! \tTraining Loss:  118.5465\n",
      "\tEpoch 220 complete! \tTraining Loss:  118.54994140625\n",
      "\tEpoch 221 complete! \tTraining Loss:  118.42722823660715\n",
      "\tEpoch 222 complete! \tTraining Loss:  118.41293303571429\n",
      "\tEpoch 223 complete! \tTraining Loss:  118.32629073660715\n",
      "\tEpoch 224 complete! \tTraining Loss:  118.29449553571429\n",
      "\tEpoch 225 complete! \tTraining Loss:  118.28194754464286\n",
      "\tEpoch 226 complete! \tTraining Loss:  118.20832645089286\n",
      "\tEpoch 227 complete! \tTraining Loss:  118.18343973214286\n",
      "\tEpoch 228 complete! \tTraining Loss:  118.19731138392856\n",
      "\tEpoch 229 complete! \tTraining Loss:  118.01174441964285\n",
      "\tEpoch 230 complete! \tTraining Loss:  117.98554631696429\n",
      "\tEpoch 231 complete! \tTraining Loss:  117.94866071428571\n",
      "\tEpoch 232 complete! \tTraining Loss:  118.01082868303571\n",
      "\tEpoch 233 complete! \tTraining Loss:  117.94008426339286\n",
      "\tEpoch 234 complete! \tTraining Loss:  117.80579631696429\n",
      "\tEpoch 235 complete! \tTraining Loss:  117.60374609375\n",
      "\tEpoch 236 complete! \tTraining Loss:  117.70073158482143\n",
      "\tEpoch 237 complete! \tTraining Loss:  117.67485100446429\n",
      "\tEpoch 238 complete! \tTraining Loss:  117.61477455357142\n",
      "\tEpoch 239 complete! \tTraining Loss:  117.56759765625\n",
      "\tEpoch 240 complete! \tTraining Loss:  117.50010491071428\n",
      "\tEpoch 241 complete! \tTraining Loss:  117.456875\n",
      "\tEpoch 242 complete! \tTraining Loss:  117.46739118303572\n",
      "\tEpoch 243 complete! \tTraining Loss:  117.39548716517857\n",
      "\tEpoch 244 complete! \tTraining Loss:  117.41604129464285\n",
      "\tEpoch 245 complete! \tTraining Loss:  117.36285881696429\n",
      "\tEpoch 246 complete! \tTraining Loss:  117.29892466517857\n",
      "\tEpoch 247 complete! \tTraining Loss:  117.261125\n",
      "\tEpoch 248 complete! \tTraining Loss:  117.32719977678572\n",
      "\tEpoch 249 complete! \tTraining Loss:  117.09841741071429\n",
      "\tEpoch 250 complete! \tTraining Loss:  117.10399720982143\n",
      "\tEpoch 251 complete! \tTraining Loss:  117.00153459821429\n",
      "\tEpoch 252 complete! \tTraining Loss:  116.99898381696428\n",
      "\tEpoch 253 complete! \tTraining Loss:  116.97966517857142\n",
      "\tEpoch 254 complete! \tTraining Loss:  116.86970033482143\n",
      "\tEpoch 255 complete! \tTraining Loss:  116.82031361607143\n",
      "\tEpoch 256 complete! \tTraining Loss:  116.79662611607142\n",
      "\tEpoch 257 complete! \tTraining Loss:  116.86995703125\n",
      "\tEpoch 258 complete! \tTraining Loss:  116.82188839285715\n",
      "\tEpoch 259 complete! \tTraining Loss:  116.78837444196428\n",
      "\tEpoch 260 complete! \tTraining Loss:  116.74725613839286\n",
      "\tEpoch 261 complete! \tTraining Loss:  116.64458928571429\n",
      "\tEpoch 262 complete! \tTraining Loss:  116.58058761160714\n",
      "\tEpoch 263 complete! \tTraining Loss:  116.59468136160714\n",
      "\tEpoch 264 complete! \tTraining Loss:  116.57708816964286\n",
      "\tEpoch 265 complete! \tTraining Loss:  116.54497042410715\n",
      "\tEpoch 266 complete! \tTraining Loss:  116.50059933035715\n",
      "\tEpoch 267 complete! \tTraining Loss:  116.4606953125\n",
      "\tEpoch 268 complete! \tTraining Loss:  116.37535658482143\n",
      "\tEpoch 269 complete! \tTraining Loss:  116.35547377232143\n",
      "\tEpoch 270 complete! \tTraining Loss:  116.33238783482143\n",
      "\tEpoch 271 complete! \tTraining Loss:  116.34411997767857\n",
      "\tEpoch 272 complete! \tTraining Loss:  116.26158147321429\n",
      "\tEpoch 273 complete! \tTraining Loss:  116.21465736607144\n",
      "\tEpoch 274 complete! \tTraining Loss:  116.21054408482142\n",
      "\tEpoch 275 complete! \tTraining Loss:  116.14076116071429\n",
      "\tEpoch 276 complete! \tTraining Loss:  116.15465011160714\n",
      "\tEpoch 277 complete! \tTraining Loss:  116.13496763392857\n",
      "\tEpoch 278 complete! \tTraining Loss:  116.02406640625\n",
      "\tEpoch 279 complete! \tTraining Loss:  116.01969084821428\n",
      "\tEpoch 280 complete! \tTraining Loss:  115.95405022321428\n",
      "\tEpoch 281 complete! \tTraining Loss:  115.90260267857143\n",
      "\tEpoch 282 complete! \tTraining Loss:  116.03001283482143\n",
      "\tEpoch 283 complete! \tTraining Loss:  115.87466685267857\n",
      "\tEpoch 284 complete! \tTraining Loss:  115.88373883928571\n",
      "\tEpoch 285 complete! \tTraining Loss:  115.82112332589286\n",
      "\tEpoch 286 complete! \tTraining Loss:  115.8187109375\n",
      "\tEpoch 287 complete! \tTraining Loss:  115.78895145089285\n",
      "\tEpoch 288 complete! \tTraining Loss:  115.75821819196429\n",
      "\tEpoch 289 complete! \tTraining Loss:  115.61345758928572\n",
      "\tEpoch 290 complete! \tTraining Loss:  115.72407756696428\n",
      "\tEpoch 291 complete! \tTraining Loss:  115.50376171875\n",
      "\tEpoch 292 complete! \tTraining Loss:  115.58848660714285\n",
      "\tEpoch 293 complete! \tTraining Loss:  115.61749832589285\n",
      "\tEpoch 294 complete! \tTraining Loss:  115.51448102678572\n",
      "\tEpoch 295 complete! \tTraining Loss:  115.51866294642858\n",
      "\tEpoch 296 complete! \tTraining Loss:  115.4916953125\n",
      "\tEpoch 297 complete! \tTraining Loss:  115.43159263392857\n",
      "\tEpoch 298 complete! \tTraining Loss:  115.45685825892858\n",
      "\tEpoch 299 complete! \tTraining Loss:  115.30788225446429\n",
      "\tEpoch 300 complete! \tTraining Loss:  115.30079520089286\n",
      "torch.Size([7000, 784])\n",
      "1\n",
      "\tEpoch 1 complete! \tTraining Loss:  364.1561127232143\n",
      "\tEpoch 2 complete! \tTraining Loss:  252.49208705357142\n",
      "\tEpoch 3 complete! \tTraining Loss:  233.605109375\n",
      "\tEpoch 4 complete! \tTraining Loss:  220.19374665178572\n",
      "\tEpoch 5 complete! \tTraining Loss:  208.18973102678572\n",
      "\tEpoch 6 complete! \tTraining Loss:  197.44029352678572\n",
      "\tEpoch 7 complete! \tTraining Loss:  187.33269754464285\n",
      "\tEpoch 8 complete! \tTraining Loss:  179.70782142857144\n",
      "\tEpoch 9 complete! \tTraining Loss:  174.30539285714286\n",
      "\tEpoch 10 complete! \tTraining Loss:  170.2611997767857\n",
      "\tEpoch 11 complete! \tTraining Loss:  166.88032700892856\n",
      "\tEpoch 12 complete! \tTraining Loss:  163.80549330357144\n",
      "\tEpoch 13 complete! \tTraining Loss:  160.93074776785716\n",
      "\tEpoch 14 complete! \tTraining Loss:  158.614625\n",
      "\tEpoch 15 complete! \tTraining Loss:  156.5142734375\n",
      "\tEpoch 16 complete! \tTraining Loss:  154.7277845982143\n",
      "\tEpoch 17 complete! \tTraining Loss:  153.16938058035714\n",
      "\tEpoch 18 complete! \tTraining Loss:  151.5995044642857\n",
      "\tEpoch 19 complete! \tTraining Loss:  150.297890625\n",
      "\tEpoch 20 complete! \tTraining Loss:  148.96373772321428\n",
      "\tEpoch 21 complete! \tTraining Loss:  147.80108035714287\n",
      "\tEpoch 22 complete! \tTraining Loss:  146.76467299107142\n",
      "\tEpoch 23 complete! \tTraining Loss:  145.61358370535714\n",
      "\tEpoch 24 complete! \tTraining Loss:  144.62677455357144\n",
      "\tEpoch 25 complete! \tTraining Loss:  143.76968191964286\n",
      "\tEpoch 26 complete! \tTraining Loss:  142.95945535714284\n",
      "\tEpoch 27 complete! \tTraining Loss:  142.21667075892856\n",
      "\tEpoch 28 complete! \tTraining Loss:  141.51782589285713\n",
      "\tEpoch 29 complete! \tTraining Loss:  140.75584933035714\n",
      "\tEpoch 30 complete! \tTraining Loss:  140.09629185267858\n",
      "\tEpoch 31 complete! \tTraining Loss:  139.52578013392858\n",
      "\tEpoch 32 complete! \tTraining Loss:  138.90150502232143\n",
      "\tEpoch 33 complete! \tTraining Loss:  138.39297544642858\n",
      "\tEpoch 34 complete! \tTraining Loss:  137.90001953125\n",
      "\tEpoch 35 complete! \tTraining Loss:  137.3346294642857\n",
      "\tEpoch 36 complete! \tTraining Loss:  136.77535044642858\n",
      "\tEpoch 37 complete! \tTraining Loss:  136.39008705357142\n",
      "\tEpoch 38 complete! \tTraining Loss:  135.99199051339286\n",
      "\tEpoch 39 complete! \tTraining Loss:  135.54574330357144\n",
      "\tEpoch 40 complete! \tTraining Loss:  135.02586886160714\n",
      "\tEpoch 41 complete! \tTraining Loss:  134.6752890625\n",
      "\tEpoch 42 complete! \tTraining Loss:  134.35649441964287\n",
      "\tEpoch 43 complete! \tTraining Loss:  133.9886595982143\n",
      "\tEpoch 44 complete! \tTraining Loss:  133.57949776785713\n",
      "\tEpoch 45 complete! \tTraining Loss:  133.28241517857143\n",
      "\tEpoch 46 complete! \tTraining Loss:  132.82880859375\n",
      "\tEpoch 47 complete! \tTraining Loss:  132.58046205357144\n",
      "\tEpoch 48 complete! \tTraining Loss:  132.31039620535714\n",
      "\tEpoch 49 complete! \tTraining Loss:  131.96097433035715\n",
      "\tEpoch 50 complete! \tTraining Loss:  131.71571205357142\n",
      "\tEpoch 51 complete! \tTraining Loss:  131.45043191964285\n",
      "\tEpoch 52 complete! \tTraining Loss:  131.13409040178573\n",
      "\tEpoch 53 complete! \tTraining Loss:  130.86274051339285\n",
      "\tEpoch 54 complete! \tTraining Loss:  130.55334598214284\n",
      "\tEpoch 55 complete! \tTraining Loss:  130.30450613839287\n",
      "\tEpoch 56 complete! \tTraining Loss:  130.06715904017858\n",
      "\tEpoch 57 complete! \tTraining Loss:  129.86253738839287\n",
      "\tEpoch 58 complete! \tTraining Loss:  129.66434654017857\n",
      "\tEpoch 59 complete! \tTraining Loss:  129.33586216517858\n",
      "\tEpoch 60 complete! \tTraining Loss:  129.16902957589286\n",
      "\tEpoch 61 complete! \tTraining Loss:  128.91497042410714\n",
      "\tEpoch 62 complete! \tTraining Loss:  128.70729296875\n",
      "\tEpoch 63 complete! \tTraining Loss:  128.53723660714286\n",
      "\tEpoch 64 complete! \tTraining Loss:  128.31625334821427\n",
      "\tEpoch 65 complete! \tTraining Loss:  128.1073197544643\n",
      "\tEpoch 66 complete! \tTraining Loss:  127.90241908482143\n",
      "\tEpoch 67 complete! \tTraining Loss:  127.72323660714285\n",
      "\tEpoch 68 complete! \tTraining Loss:  127.46139285714285\n",
      "\tEpoch 69 complete! \tTraining Loss:  127.36504631696428\n",
      "\tEpoch 70 complete! \tTraining Loss:  127.07917689732143\n",
      "\tEpoch 71 complete! \tTraining Loss:  127.02371986607143\n",
      "\tEpoch 72 complete! \tTraining Loss:  126.77894587053571\n",
      "\tEpoch 73 complete! \tTraining Loss:  126.64478180803572\n",
      "\tEpoch 74 complete! \tTraining Loss:  126.50861551339285\n",
      "\tEpoch 75 complete! \tTraining Loss:  126.32419140625\n",
      "\tEpoch 76 complete! \tTraining Loss:  126.13677176339286\n",
      "\tEpoch 77 complete! \tTraining Loss:  126.05222712053572\n",
      "\tEpoch 78 complete! \tTraining Loss:  125.86408370535715\n",
      "\tEpoch 79 complete! \tTraining Loss:  125.77321875\n",
      "\tEpoch 80 complete! \tTraining Loss:  125.51738671875\n",
      "\tEpoch 81 complete! \tTraining Loss:  125.48338727678572\n",
      "\tEpoch 82 complete! \tTraining Loss:  125.25040345982143\n",
      "\tEpoch 83 complete! \tTraining Loss:  125.12452845982143\n",
      "\tEpoch 84 complete! \tTraining Loss:  124.97657198660714\n",
      "\tEpoch 85 complete! \tTraining Loss:  124.76317522321429\n",
      "\tEpoch 86 complete! \tTraining Loss:  124.75812555803572\n",
      "\tEpoch 87 complete! \tTraining Loss:  124.55167131696429\n",
      "\tEpoch 88 complete! \tTraining Loss:  124.35833258928571\n",
      "\tEpoch 89 complete! \tTraining Loss:  124.30309765625\n",
      "\tEpoch 90 complete! \tTraining Loss:  124.15508761160714\n",
      "\tEpoch 91 complete! \tTraining Loss:  124.17500948660714\n",
      "\tEpoch 92 complete! \tTraining Loss:  123.90826953125\n",
      "\tEpoch 93 complete! \tTraining Loss:  123.86299274553572\n",
      "\tEpoch 94 complete! \tTraining Loss:  123.72903794642858\n",
      "\tEpoch 95 complete! \tTraining Loss:  123.59348214285714\n",
      "\tEpoch 96 complete! \tTraining Loss:  123.43615569196429\n",
      "\tEpoch 97 complete! \tTraining Loss:  123.32762834821429\n",
      "\tEpoch 98 complete! \tTraining Loss:  123.32040736607142\n",
      "\tEpoch 99 complete! \tTraining Loss:  123.05042801339286\n",
      "\tEpoch 100 complete! \tTraining Loss:  122.97550223214286\n",
      "\tEpoch 101 complete! \tTraining Loss:  122.97234263392858\n",
      "\tEpoch 102 complete! \tTraining Loss:  122.77120647321429\n",
      "\tEpoch 103 complete! \tTraining Loss:  122.66620535714286\n",
      "\tEpoch 104 complete! \tTraining Loss:  122.59946540178571\n",
      "\tEpoch 105 complete! \tTraining Loss:  122.46044475446429\n",
      "\tEpoch 106 complete! \tTraining Loss:  122.43104241071428\n",
      "\tEpoch 107 complete! \tTraining Loss:  122.36168805803571\n",
      "\tEpoch 108 complete! \tTraining Loss:  122.20240625\n",
      "\tEpoch 109 complete! \tTraining Loss:  122.00531361607143\n",
      "\tEpoch 110 complete! \tTraining Loss:  122.08123716517858\n",
      "\tEpoch 111 complete! \tTraining Loss:  121.88880580357143\n",
      "\tEpoch 112 complete! \tTraining Loss:  121.85427120535714\n",
      "\tEpoch 113 complete! \tTraining Loss:  121.76719252232142\n",
      "\tEpoch 114 complete! \tTraining Loss:  121.62468359375\n",
      "\tEpoch 115 complete! \tTraining Loss:  121.64687890625\n",
      "\tEpoch 116 complete! \tTraining Loss:  121.52293805803572\n",
      "\tEpoch 117 complete! \tTraining Loss:  121.43822098214285\n",
      "\tEpoch 118 complete! \tTraining Loss:  121.29962276785714\n",
      "\tEpoch 119 complete! \tTraining Loss:  121.21962388392858\n",
      "\tEpoch 120 complete! \tTraining Loss:  121.16155524553571\n",
      "\tEpoch 121 complete! \tTraining Loss:  121.06844642857143\n",
      "\tEpoch 122 complete! \tTraining Loss:  121.04280691964286\n",
      "\tEpoch 123 complete! \tTraining Loss:  120.84850111607143\n",
      "\tEpoch 124 complete! \tTraining Loss:  120.73962890625\n",
      "\tEpoch 125 complete! \tTraining Loss:  120.77681696428571\n",
      "\tEpoch 126 complete! \tTraining Loss:  120.73018638392857\n",
      "\tEpoch 127 complete! \tTraining Loss:  120.56731584821429\n",
      "\tEpoch 128 complete! \tTraining Loss:  120.51469642857143\n",
      "\tEpoch 129 complete! \tTraining Loss:  120.54658872767857\n",
      "\tEpoch 130 complete! \tTraining Loss:  120.37451004464286\n",
      "\tEpoch 131 complete! \tTraining Loss:  120.22513895089286\n",
      "\tEpoch 132 complete! \tTraining Loss:  120.17330189732142\n",
      "\tEpoch 133 complete! \tTraining Loss:  120.14201618303571\n",
      "\tEpoch 134 complete! \tTraining Loss:  120.03022265625\n",
      "\tEpoch 135 complete! \tTraining Loss:  120.01770033482143\n",
      "\tEpoch 136 complete! \tTraining Loss:  119.94411495535714\n",
      "\tEpoch 137 complete! \tTraining Loss:  119.83229408482143\n",
      "\tEpoch 138 complete! \tTraining Loss:  119.83488560267857\n",
      "\tEpoch 139 complete! \tTraining Loss:  119.67124107142857\n",
      "\tEpoch 140 complete! \tTraining Loss:  119.61611216517858\n",
      "\tEpoch 141 complete! \tTraining Loss:  119.66072544642857\n",
      "\tEpoch 142 complete! \tTraining Loss:  119.46837611607143\n",
      "\tEpoch 143 complete! \tTraining Loss:  119.42128013392858\n",
      "\tEpoch 144 complete! \tTraining Loss:  119.33320368303572\n",
      "\tEpoch 145 complete! \tTraining Loss:  119.34587779017858\n",
      "\tEpoch 146 complete! \tTraining Loss:  119.22114732142857\n",
      "\tEpoch 147 complete! \tTraining Loss:  119.21676004464285\n",
      "\tEpoch 148 complete! \tTraining Loss:  119.27121484375\n",
      "\tEpoch 149 complete! \tTraining Loss:  119.13618694196428\n",
      "\tEpoch 150 complete! \tTraining Loss:  119.05872767857143\n",
      "\tEpoch 151 complete! \tTraining Loss:  119.00486774553572\n",
      "\tEpoch 152 complete! \tTraining Loss:  118.90040904017857\n",
      "\tEpoch 153 complete! \tTraining Loss:  118.84942745535714\n",
      "\tEpoch 154 complete! \tTraining Loss:  118.68120591517857\n",
      "\tEpoch 155 complete! \tTraining Loss:  118.72900446428571\n",
      "\tEpoch 156 complete! \tTraining Loss:  118.63445368303572\n",
      "\tEpoch 157 complete! \tTraining Loss:  118.62158482142857\n",
      "\tEpoch 158 complete! \tTraining Loss:  118.59903515625\n",
      "\tEpoch 159 complete! \tTraining Loss:  118.5096328125\n",
      "\tEpoch 160 complete! \tTraining Loss:  118.51187611607143\n",
      "\tEpoch 161 complete! \tTraining Loss:  118.41509207589286\n",
      "\tEpoch 162 complete! \tTraining Loss:  118.31606808035714\n",
      "\tEpoch 163 complete! \tTraining Loss:  118.24522600446429\n",
      "\tEpoch 164 complete! \tTraining Loss:  118.15738895089285\n",
      "\tEpoch 165 complete! \tTraining Loss:  118.168140625\n",
      "\tEpoch 166 complete! \tTraining Loss:  118.08756584821428\n",
      "\tEpoch 167 complete! \tTraining Loss:  118.10921651785715\n",
      "\tEpoch 168 complete! \tTraining Loss:  118.09121372767858\n",
      "\tEpoch 169 complete! \tTraining Loss:  117.94489453125\n",
      "\tEpoch 170 complete! \tTraining Loss:  117.81592745535714\n",
      "\tEpoch 171 complete! \tTraining Loss:  117.80381361607142\n",
      "\tEpoch 172 complete! \tTraining Loss:  117.81775669642857\n",
      "\tEpoch 173 complete! \tTraining Loss:  117.77145870535715\n",
      "\tEpoch 174 complete! \tTraining Loss:  117.64926506696429\n",
      "\tEpoch 175 complete! \tTraining Loss:  117.64795145089286\n",
      "\tEpoch 176 complete! \tTraining Loss:  117.5352421875\n",
      "\tEpoch 177 complete! \tTraining Loss:  117.55963504464286\n",
      "\tEpoch 178 complete! \tTraining Loss:  117.51417243303571\n",
      "\tEpoch 179 complete! \tTraining Loss:  117.4691171875\n",
      "\tEpoch 180 complete! \tTraining Loss:  117.29604185267857\n",
      "\tEpoch 181 complete! \tTraining Loss:  117.33776953125\n",
      "\tEpoch 182 complete! \tTraining Loss:  117.25310379464285\n",
      "\tEpoch 183 complete! \tTraining Loss:  117.25997767857143\n",
      "\tEpoch 184 complete! \tTraining Loss:  117.13480524553572\n",
      "\tEpoch 185 complete! \tTraining Loss:  117.18433370535715\n",
      "\tEpoch 186 complete! \tTraining Loss:  117.11661886160714\n",
      "\tEpoch 187 complete! \tTraining Loss:  117.17675446428571\n",
      "\tEpoch 188 complete! \tTraining Loss:  116.997828125\n",
      "\tEpoch 189 complete! \tTraining Loss:  116.95707142857142\n",
      "\tEpoch 190 complete! \tTraining Loss:  116.97553292410714\n",
      "\tEpoch 191 complete! \tTraining Loss:  116.93108482142857\n",
      "\tEpoch 192 complete! \tTraining Loss:  116.79733872767856\n",
      "\tEpoch 193 complete! \tTraining Loss:  116.85977734375\n",
      "\tEpoch 194 complete! \tTraining Loss:  116.82081975446428\n",
      "\tEpoch 195 complete! \tTraining Loss:  116.75729073660715\n",
      "\tEpoch 196 complete! \tTraining Loss:  116.7030234375\n",
      "\tEpoch 197 complete! \tTraining Loss:  116.63069754464286\n",
      "\tEpoch 198 complete! \tTraining Loss:  116.63919587053572\n",
      "\tEpoch 199 complete! \tTraining Loss:  116.54212165178572\n",
      "\tEpoch 200 complete! \tTraining Loss:  116.47394196428571\n",
      "\tEpoch 201 complete! \tTraining Loss:  116.40650334821429\n",
      "\tEpoch 202 complete! \tTraining Loss:  116.46182533482143\n",
      "\tEpoch 203 complete! \tTraining Loss:  116.37470033482143\n",
      "\tEpoch 204 complete! \tTraining Loss:  116.2953828125\n",
      "\tEpoch 205 complete! \tTraining Loss:  116.31436104910715\n",
      "\tEpoch 206 complete! \tTraining Loss:  116.30078683035714\n",
      "\tEpoch 207 complete! \tTraining Loss:  116.19911997767858\n",
      "\tEpoch 208 complete! \tTraining Loss:  116.17318805803572\n",
      "\tEpoch 209 complete! \tTraining Loss:  116.15518303571429\n",
      "\tEpoch 210 complete! \tTraining Loss:  116.08781975446429\n",
      "\tEpoch 211 complete! \tTraining Loss:  116.09176227678572\n",
      "\tEpoch 212 complete! \tTraining Loss:  115.9461328125\n",
      "\tEpoch 213 complete! \tTraining Loss:  116.02987779017857\n",
      "\tEpoch 214 complete! \tTraining Loss:  115.94634877232143\n",
      "\tEpoch 215 complete! \tTraining Loss:  115.8893515625\n",
      "\tEpoch 216 complete! \tTraining Loss:  115.80952455357144\n",
      "\tEpoch 217 complete! \tTraining Loss:  115.84981082589286\n",
      "\tEpoch 218 complete! \tTraining Loss:  115.76720591517856\n",
      "\tEpoch 219 complete! \tTraining Loss:  115.73975613839286\n",
      "\tEpoch 220 complete! \tTraining Loss:  115.7401953125\n",
      "\tEpoch 221 complete! \tTraining Loss:  115.71495368303572\n",
      "\tEpoch 222 complete! \tTraining Loss:  115.63161774553572\n",
      "\tEpoch 223 complete! \tTraining Loss:  115.64369642857143\n",
      "\tEpoch 224 complete! \tTraining Loss:  115.52060714285714\n",
      "\tEpoch 225 complete! \tTraining Loss:  115.54979129464286\n",
      "\tEpoch 226 complete! \tTraining Loss:  115.53635491071428\n",
      "\tEpoch 227 complete! \tTraining Loss:  115.44344252232143\n",
      "\tEpoch 228 complete! \tTraining Loss:  115.51266741071429\n",
      "\tEpoch 229 complete! \tTraining Loss:  115.45466015625\n",
      "\tEpoch 230 complete! \tTraining Loss:  115.47087053571428\n",
      "\tEpoch 231 complete! \tTraining Loss:  115.39102734375\n",
      "\tEpoch 232 complete! \tTraining Loss:  115.29101674107143\n",
      "\tEpoch 233 complete! \tTraining Loss:  115.28030524553571\n",
      "\tEpoch 234 complete! \tTraining Loss:  115.20125446428571\n",
      "\tEpoch 235 complete! \tTraining Loss:  115.23949051339285\n",
      "\tEpoch 236 complete! \tTraining Loss:  115.22384933035714\n",
      "\tEpoch 237 complete! \tTraining Loss:  115.22604017857142\n",
      "\tEpoch 238 complete! \tTraining Loss:  115.07673046875\n",
      "\tEpoch 239 complete! \tTraining Loss:  115.04995758928571\n",
      "\tEpoch 240 complete! \tTraining Loss:  115.04542522321428\n",
      "\tEpoch 241 complete! \tTraining Loss:  115.04226283482143\n",
      "\tEpoch 242 complete! \tTraining Loss:  114.94324832589285\n",
      "\tEpoch 243 complete! \tTraining Loss:  115.02400055803571\n",
      "\tEpoch 244 complete! \tTraining Loss:  114.95043136160714\n",
      "\tEpoch 245 complete! \tTraining Loss:  114.92699497767858\n",
      "\tEpoch 246 complete! \tTraining Loss:  114.9113125\n",
      "\tEpoch 247 complete! \tTraining Loss:  114.87649553571428\n",
      "\tEpoch 248 complete! \tTraining Loss:  114.81988616071429\n",
      "\tEpoch 249 complete! \tTraining Loss:  114.69698493303571\n",
      "\tEpoch 250 complete! \tTraining Loss:  114.79803236607142\n",
      "\tEpoch 251 complete! \tTraining Loss:  114.64383035714286\n",
      "\tEpoch 252 complete! \tTraining Loss:  114.57451841517857\n",
      "\tEpoch 253 complete! \tTraining Loss:  114.59073716517857\n",
      "\tEpoch 254 complete! \tTraining Loss:  114.5559921875\n",
      "\tEpoch 255 complete! \tTraining Loss:  114.52894419642857\n",
      "\tEpoch 256 complete! \tTraining Loss:  114.56243526785714\n",
      "\tEpoch 257 complete! \tTraining Loss:  114.51828404017857\n",
      "\tEpoch 258 complete! \tTraining Loss:  114.40710379464285\n",
      "\tEpoch 259 complete! \tTraining Loss:  114.40559040178572\n",
      "\tEpoch 260 complete! \tTraining Loss:  114.42925279017857\n",
      "\tEpoch 261 complete! \tTraining Loss:  114.41962165178572\n",
      "\tEpoch 262 complete! \tTraining Loss:  114.35048493303572\n",
      "\tEpoch 263 complete! \tTraining Loss:  114.34359263392857\n",
      "\tEpoch 264 complete! \tTraining Loss:  114.23616183035715\n",
      "\tEpoch 265 complete! \tTraining Loss:  114.29884430803571\n",
      "\tEpoch 266 complete! \tTraining Loss:  114.24220256696428\n",
      "\tEpoch 267 complete! \tTraining Loss:  114.20430524553572\n",
      "\tEpoch 268 complete! \tTraining Loss:  114.21849609375\n",
      "\tEpoch 269 complete! \tTraining Loss:  114.10937444196429\n",
      "\tEpoch 270 complete! \tTraining Loss:  114.09710602678571\n",
      "\tEpoch 271 complete! \tTraining Loss:  114.06410602678571\n",
      "\tEpoch 272 complete! \tTraining Loss:  114.12905915178571\n",
      "\tEpoch 273 complete! \tTraining Loss:  114.12022209821428\n",
      "\tEpoch 274 complete! \tTraining Loss:  114.03411830357143\n",
      "\tEpoch 275 complete! \tTraining Loss:  114.02338337053571\n",
      "\tEpoch 276 complete! \tTraining Loss:  114.04419363839285\n",
      "\tEpoch 277 complete! \tTraining Loss:  113.96536941964285\n",
      "\tEpoch 278 complete! \tTraining Loss:  113.92753738839286\n",
      "\tEpoch 279 complete! \tTraining Loss:  113.85070368303572\n",
      "\tEpoch 280 complete! \tTraining Loss:  113.85388113839285\n",
      "\tEpoch 281 complete! \tTraining Loss:  113.89360602678572\n",
      "\tEpoch 282 complete! \tTraining Loss:  113.81780189732143\n",
      "\tEpoch 283 complete! \tTraining Loss:  113.76736439732143\n",
      "\tEpoch 284 complete! \tTraining Loss:  113.73840011160715\n",
      "\tEpoch 285 complete! \tTraining Loss:  113.72671484375\n",
      "\tEpoch 286 complete! \tTraining Loss:  113.82311383928571\n",
      "\tEpoch 287 complete! \tTraining Loss:  113.71077232142858\n",
      "\tEpoch 288 complete! \tTraining Loss:  113.67797098214285\n",
      "\tEpoch 289 complete! \tTraining Loss:  113.67132087053571\n",
      "\tEpoch 290 complete! \tTraining Loss:  113.59861439732143\n",
      "\tEpoch 291 complete! \tTraining Loss:  113.56939397321429\n",
      "\tEpoch 292 complete! \tTraining Loss:  113.61973270089285\n",
      "\tEpoch 293 complete! \tTraining Loss:  113.61012276785715\n",
      "\tEpoch 294 complete! \tTraining Loss:  113.51215792410714\n",
      "\tEpoch 295 complete! \tTraining Loss:  113.51931752232143\n",
      "\tEpoch 296 complete! \tTraining Loss:  113.44504185267857\n",
      "\tEpoch 297 complete! \tTraining Loss:  113.38641908482143\n",
      "\tEpoch 298 complete! \tTraining Loss:  113.36301841517857\n",
      "\tEpoch 299 complete! \tTraining Loss:  113.36416852678572\n",
      "\tEpoch 300 complete! \tTraining Loss:  113.31924832589286\n",
      "torch.Size([7000, 784])\n",
      "2\n",
      "\tEpoch 1 complete! \tTraining Loss:  371.5059921875\n",
      "\tEpoch 2 complete! \tTraining Loss:  252.38726450892858\n",
      "\tEpoch 3 complete! \tTraining Loss:  230.42341741071428\n",
      "\tEpoch 4 complete! \tTraining Loss:  212.65158482142857\n",
      "\tEpoch 5 complete! \tTraining Loss:  199.30999888392856\n",
      "\tEpoch 6 complete! \tTraining Loss:  188.51818638392857\n",
      "\tEpoch 7 complete! \tTraining Loss:  180.15342857142858\n",
      "\tEpoch 8 complete! \tTraining Loss:  174.13825669642858\n",
      "\tEpoch 9 complete! \tTraining Loss:  169.45330691964287\n",
      "\tEpoch 10 complete! \tTraining Loss:  165.78973549107144\n",
      "\tEpoch 11 complete! \tTraining Loss:  162.704\n",
      "\tEpoch 12 complete! \tTraining Loss:  159.99766183035715\n",
      "\tEpoch 13 complete! \tTraining Loss:  157.62001674107142\n",
      "\tEpoch 14 complete! \tTraining Loss:  155.7598080357143\n",
      "\tEpoch 15 complete! \tTraining Loss:  153.99159486607144\n",
      "\tEpoch 16 complete! \tTraining Loss:  152.4329486607143\n",
      "\tEpoch 17 complete! \tTraining Loss:  150.90945424107142\n",
      "\tEpoch 18 complete! \tTraining Loss:  149.65636383928572\n",
      "\tEpoch 19 complete! \tTraining Loss:  148.46694308035714\n",
      "\tEpoch 20 complete! \tTraining Loss:  147.26190513392856\n",
      "\tEpoch 21 complete! \tTraining Loss:  146.32346651785716\n",
      "\tEpoch 22 complete! \tTraining Loss:  145.24795535714287\n",
      "\tEpoch 23 complete! \tTraining Loss:  144.31095200892858\n",
      "\tEpoch 24 complete! \tTraining Loss:  143.47176450892857\n",
      "\tEpoch 25 complete! \tTraining Loss:  142.69128348214286\n",
      "\tEpoch 26 complete! \tTraining Loss:  142.0468080357143\n",
      "\tEpoch 27 complete! \tTraining Loss:  141.38350223214286\n",
      "\tEpoch 28 complete! \tTraining Loss:  140.70920926339286\n",
      "\tEpoch 29 complete! \tTraining Loss:  140.03217131696428\n",
      "\tEpoch 30 complete! \tTraining Loss:  139.49109598214287\n",
      "\tEpoch 31 complete! \tTraining Loss:  138.93387053571428\n",
      "\tEpoch 32 complete! \tTraining Loss:  138.38263225446428\n",
      "\tEpoch 33 complete! \tTraining Loss:  137.95351395089287\n",
      "\tEpoch 34 complete! \tTraining Loss:  137.40876785714286\n",
      "\tEpoch 35 complete! \tTraining Loss:  136.93472991071428\n",
      "\tEpoch 36 complete! \tTraining Loss:  136.53089732142857\n",
      "\tEpoch 37 complete! \tTraining Loss:  136.06415736607144\n",
      "\tEpoch 38 complete! \tTraining Loss:  135.74656082589286\n",
      "\tEpoch 39 complete! \tTraining Loss:  135.278453125\n",
      "\tEpoch 40 complete! \tTraining Loss:  134.84028627232144\n",
      "\tEpoch 41 complete! \tTraining Loss:  134.57179129464285\n",
      "\tEpoch 42 complete! \tTraining Loss:  134.19845033482142\n",
      "\tEpoch 43 complete! \tTraining Loss:  133.88829296875\n",
      "\tEpoch 44 complete! \tTraining Loss:  133.55710100446427\n",
      "\tEpoch 45 complete! \tTraining Loss:  133.22136160714285\n",
      "\tEpoch 46 complete! \tTraining Loss:  132.72472321428572\n",
      "\tEpoch 47 complete! \tTraining Loss:  132.5329888392857\n",
      "\tEpoch 48 complete! \tTraining Loss:  132.3019609375\n",
      "\tEpoch 49 complete! \tTraining Loss:  132.02420033482142\n",
      "\tEpoch 50 complete! \tTraining Loss:  131.75151395089287\n",
      "\tEpoch 51 complete! \tTraining Loss:  131.3600345982143\n",
      "\tEpoch 52 complete! \tTraining Loss:  131.13504073660715\n",
      "\tEpoch 53 complete! \tTraining Loss:  130.89285602678572\n",
      "\tEpoch 54 complete! \tTraining Loss:  130.65041741071428\n",
      "\tEpoch 55 complete! \tTraining Loss:  130.3503705357143\n",
      "\tEpoch 56 complete! \tTraining Loss:  130.1838677455357\n",
      "\tEpoch 57 complete! \tTraining Loss:  129.92590011160715\n",
      "\tEpoch 58 complete! \tTraining Loss:  129.64342745535714\n",
      "\tEpoch 59 complete! \tTraining Loss:  129.57227734375\n",
      "\tEpoch 60 complete! \tTraining Loss:  129.18270368303573\n",
      "\tEpoch 61 complete! \tTraining Loss:  129.0805970982143\n",
      "\tEpoch 62 complete! \tTraining Loss:  128.81675837053572\n",
      "\tEpoch 63 complete! \tTraining Loss:  128.60972377232142\n",
      "\tEpoch 64 complete! \tTraining Loss:  128.46910714285715\n",
      "\tEpoch 65 complete! \tTraining Loss:  128.22617522321428\n",
      "\tEpoch 66 complete! \tTraining Loss:  128.06919308035714\n",
      "\tEpoch 67 complete! \tTraining Loss:  127.88093024553571\n",
      "\tEpoch 68 complete! \tTraining Loss:  127.72988671875\n",
      "\tEpoch 69 complete! \tTraining Loss:  127.55398883928571\n",
      "\tEpoch 70 complete! \tTraining Loss:  127.38488002232143\n",
      "\tEpoch 71 complete! \tTraining Loss:  127.26481473214285\n",
      "\tEpoch 72 complete! \tTraining Loss:  127.09544252232143\n",
      "\tEpoch 73 complete! \tTraining Loss:  126.75562276785715\n",
      "\tEpoch 74 complete! \tTraining Loss:  126.77659933035714\n",
      "\tEpoch 75 complete! \tTraining Loss:  126.48457868303572\n",
      "\tEpoch 76 complete! \tTraining Loss:  126.42641796875\n",
      "\tEpoch 77 complete! \tTraining Loss:  126.33962946428572\n",
      "\tEpoch 78 complete! \tTraining Loss:  126.17515234375\n",
      "\tEpoch 79 complete! \tTraining Loss:  125.98114676339286\n",
      "\tEpoch 80 complete! \tTraining Loss:  125.87657979910715\n",
      "\tEpoch 81 complete! \tTraining Loss:  125.74702622767857\n",
      "\tEpoch 82 complete! \tTraining Loss:  125.62971930803572\n",
      "\tEpoch 83 complete! \tTraining Loss:  125.47362053571429\n",
      "\tEpoch 84 complete! \tTraining Loss:  125.32814397321428\n",
      "\tEpoch 85 complete! \tTraining Loss:  125.18281305803572\n",
      "\tEpoch 86 complete! \tTraining Loss:  125.03563783482143\n",
      "\tEpoch 87 complete! \tTraining Loss:  124.97776004464286\n",
      "\tEpoch 88 complete! \tTraining Loss:  124.84705412946428\n",
      "\tEpoch 89 complete! \tTraining Loss:  124.74178627232143\n",
      "\tEpoch 90 complete! \tTraining Loss:  124.61275558035715\n",
      "\tEpoch 91 complete! \tTraining Loss:  124.56944084821428\n",
      "\tEpoch 92 complete! \tTraining Loss:  124.44170535714285\n",
      "\tEpoch 93 complete! \tTraining Loss:  124.19796651785714\n",
      "\tEpoch 94 complete! \tTraining Loss:  124.19442131696428\n",
      "\tEpoch 95 complete! \tTraining Loss:  124.09613727678571\n",
      "\tEpoch 96 complete! \tTraining Loss:  123.97322209821428\n",
      "\tEpoch 97 complete! \tTraining Loss:  123.91540680803571\n",
      "\tEpoch 98 complete! \tTraining Loss:  123.73656584821428\n",
      "\tEpoch 99 complete! \tTraining Loss:  123.67594587053571\n",
      "\tEpoch 100 complete! \tTraining Loss:  123.546265625\n",
      "\tEpoch 101 complete! \tTraining Loss:  123.51348325892857\n",
      "\tEpoch 102 complete! \tTraining Loss:  123.41865401785714\n",
      "\tEpoch 103 complete! \tTraining Loss:  123.26553738839286\n",
      "\tEpoch 104 complete! \tTraining Loss:  123.20862444196429\n",
      "\tEpoch 105 complete! \tTraining Loss:  123.05571205357143\n",
      "\tEpoch 106 complete! \tTraining Loss:  123.02953348214285\n",
      "\tEpoch 107 complete! \tTraining Loss:  122.87528459821428\n",
      "\tEpoch 108 complete! \tTraining Loss:  122.80899720982143\n",
      "\tEpoch 109 complete! \tTraining Loss:  122.80259933035714\n",
      "\tEpoch 110 complete! \tTraining Loss:  122.69429743303571\n",
      "\tEpoch 111 complete! \tTraining Loss:  122.51093247767857\n",
      "\tEpoch 112 complete! \tTraining Loss:  122.48433872767858\n",
      "\tEpoch 113 complete! \tTraining Loss:  122.33830803571429\n",
      "\tEpoch 114 complete! \tTraining Loss:  122.32084319196429\n",
      "\tEpoch 115 complete! \tTraining Loss:  122.1640859375\n",
      "\tEpoch 116 complete! \tTraining Loss:  122.18601283482143\n",
      "\tEpoch 117 complete! \tTraining Loss:  122.04704854910715\n",
      "\tEpoch 118 complete! \tTraining Loss:  122.02303794642857\n",
      "\tEpoch 119 complete! \tTraining Loss:  121.97827008928572\n",
      "\tEpoch 120 complete! \tTraining Loss:  121.84206417410714\n",
      "\tEpoch 121 complete! \tTraining Loss:  121.69836439732143\n",
      "\tEpoch 122 complete! \tTraining Loss:  121.58246986607143\n",
      "\tEpoch 123 complete! \tTraining Loss:  121.56201060267857\n",
      "\tEpoch 124 complete! \tTraining Loss:  121.52774832589286\n",
      "\tEpoch 125 complete! \tTraining Loss:  121.440109375\n",
      "\tEpoch 126 complete! \tTraining Loss:  121.39032589285715\n",
      "\tEpoch 127 complete! \tTraining Loss:  121.27448102678571\n",
      "\tEpoch 128 complete! \tTraining Loss:  121.25414564732142\n",
      "\tEpoch 129 complete! \tTraining Loss:  121.23905524553571\n",
      "\tEpoch 130 complete! \tTraining Loss:  121.05670479910714\n",
      "\tEpoch 131 complete! \tTraining Loss:  120.95742020089286\n",
      "\tEpoch 132 complete! \tTraining Loss:  120.88340122767858\n",
      "\tEpoch 133 complete! \tTraining Loss:  120.90494810267857\n",
      "\tEpoch 134 complete! \tTraining Loss:  120.77751785714285\n",
      "\tEpoch 135 complete! \tTraining Loss:  120.74172377232144\n",
      "\tEpoch 136 complete! \tTraining Loss:  120.68575558035714\n",
      "\tEpoch 137 complete! \tTraining Loss:  120.68033314732143\n",
      "\tEpoch 138 complete! \tTraining Loss:  120.59809709821428\n",
      "\tEpoch 139 complete! \tTraining Loss:  120.46179743303571\n",
      "\tEpoch 140 complete! \tTraining Loss:  120.437375\n",
      "\tEpoch 141 complete! \tTraining Loss:  120.37609933035715\n",
      "\tEpoch 142 complete! \tTraining Loss:  120.23652790178572\n",
      "\tEpoch 143 complete! \tTraining Loss:  120.34197209821428\n",
      "\tEpoch 144 complete! \tTraining Loss:  120.15971930803572\n",
      "\tEpoch 145 complete! \tTraining Loss:  120.10079854910714\n",
      "\tEpoch 146 complete! \tTraining Loss:  120.02080301339285\n",
      "\tEpoch 147 complete! \tTraining Loss:  120.10413337053572\n",
      "\tEpoch 148 complete! \tTraining Loss:  119.93231417410715\n",
      "\tEpoch 149 complete! \tTraining Loss:  119.88354966517858\n",
      "\tEpoch 150 complete! \tTraining Loss:  119.81525613839285\n",
      "\tEpoch 151 complete! \tTraining Loss:  119.69445535714286\n",
      "\tEpoch 152 complete! \tTraining Loss:  119.72676060267857\n",
      "\tEpoch 153 complete! \tTraining Loss:  119.66208426339286\n",
      "\tEpoch 154 complete! \tTraining Loss:  119.61677678571428\n",
      "\tEpoch 155 complete! \tTraining Loss:  119.5734296875\n",
      "\tEpoch 156 complete! \tTraining Loss:  119.39299776785714\n",
      "\tEpoch 157 complete! \tTraining Loss:  119.42330747767858\n",
      "\tEpoch 158 complete! \tTraining Loss:  119.33631361607142\n",
      "\tEpoch 159 complete! \tTraining Loss:  119.314484375\n",
      "\tEpoch 160 complete! \tTraining Loss:  119.25334988839286\n",
      "\tEpoch 161 complete! \tTraining Loss:  119.10759654017858\n",
      "\tEpoch 162 complete! \tTraining Loss:  119.07803627232143\n",
      "\tEpoch 163 complete! \tTraining Loss:  119.14429966517856\n",
      "\tEpoch 164 complete! \tTraining Loss:  119.04356473214285\n",
      "\tEpoch 165 complete! \tTraining Loss:  118.97799665178572\n",
      "\tEpoch 166 complete! \tTraining Loss:  118.88963616071429\n",
      "\tEpoch 167 complete! \tTraining Loss:  118.90776450892857\n",
      "\tEpoch 168 complete! \tTraining Loss:  118.79909040178572\n",
      "\tEpoch 169 complete! \tTraining Loss:  118.83080915178572\n",
      "\tEpoch 170 complete! \tTraining Loss:  118.66585491071429\n",
      "\tEpoch 171 complete! \tTraining Loss:  118.70385546875\n",
      "\tEpoch 172 complete! \tTraining Loss:  118.67565178571428\n",
      "\tEpoch 173 complete! \tTraining Loss:  118.59616015625\n",
      "\tEpoch 174 complete! \tTraining Loss:  118.555890625\n",
      "\tEpoch 175 complete! \tTraining Loss:  118.47729129464285\n",
      "\tEpoch 176 complete! \tTraining Loss:  118.53481584821428\n",
      "\tEpoch 177 complete! \tTraining Loss:  118.39111272321429\n",
      "\tEpoch 178 complete! \tTraining Loss:  118.30741741071428\n",
      "\tEpoch 179 complete! \tTraining Loss:  118.29300725446429\n",
      "\tEpoch 180 complete! \tTraining Loss:  118.2783359375\n",
      "\tEpoch 181 complete! \tTraining Loss:  118.23635379464285\n",
      "\tEpoch 182 complete! \tTraining Loss:  118.20849888392857\n",
      "\tEpoch 183 complete! \tTraining Loss:  118.15987388392857\n",
      "\tEpoch 184 complete! \tTraining Loss:  118.17658426339285\n",
      "\tEpoch 185 complete! \tTraining Loss:  118.05825334821428\n",
      "\tEpoch 186 complete! \tTraining Loss:  118.03085881696428\n",
      "\tEpoch 187 complete! \tTraining Loss:  117.97373939732142\n",
      "\tEpoch 188 complete! \tTraining Loss:  117.91934988839286\n",
      "\tEpoch 189 complete! \tTraining Loss:  117.83324107142857\n",
      "\tEpoch 190 complete! \tTraining Loss:  117.81912444196429\n",
      "\tEpoch 191 complete! \tTraining Loss:  117.79926060267857\n",
      "\tEpoch 192 complete! \tTraining Loss:  117.73866964285715\n",
      "\tEpoch 193 complete! \tTraining Loss:  117.67086439732142\n",
      "\tEpoch 194 complete! \tTraining Loss:  117.66166127232142\n",
      "\tEpoch 195 complete! \tTraining Loss:  117.65448883928572\n",
      "\tEpoch 196 complete! \tTraining Loss:  117.57086049107143\n",
      "\tEpoch 197 complete! \tTraining Loss:  117.52813895089285\n",
      "\tEpoch 198 complete! \tTraining Loss:  117.555015625\n",
      "\tEpoch 199 complete! \tTraining Loss:  117.45328627232144\n",
      "\tEpoch 200 complete! \tTraining Loss:  117.38513058035714\n",
      "\tEpoch 201 complete! \tTraining Loss:  117.37636328125\n",
      "\tEpoch 202 complete! \tTraining Loss:  117.31602622767858\n",
      "\tEpoch 203 complete! \tTraining Loss:  117.30667299107142\n",
      "\tEpoch 204 complete! \tTraining Loss:  117.29759430803571\n",
      "\tEpoch 205 complete! \tTraining Loss:  117.31549776785714\n",
      "\tEpoch 206 complete! \tTraining Loss:  117.13831752232143\n",
      "\tEpoch 207 complete! \tTraining Loss:  117.14124107142857\n",
      "\tEpoch 208 complete! \tTraining Loss:  117.11065513392857\n",
      "\tEpoch 209 complete! \tTraining Loss:  117.02740959821429\n",
      "\tEpoch 210 complete! \tTraining Loss:  117.10279910714286\n",
      "\tEpoch 211 complete! \tTraining Loss:  116.93759040178571\n",
      "\tEpoch 212 complete! \tTraining Loss:  116.89388950892857\n",
      "\tEpoch 213 complete! \tTraining Loss:  116.91814508928572\n",
      "\tEpoch 214 complete! \tTraining Loss:  116.86381919642857\n",
      "\tEpoch 215 complete! \tTraining Loss:  116.89301785714285\n",
      "\tEpoch 216 complete! \tTraining Loss:  116.85413783482143\n",
      "\tEpoch 217 complete! \tTraining Loss:  116.7511796875\n",
      "\tEpoch 218 complete! \tTraining Loss:  116.73573325892858\n",
      "\tEpoch 219 complete! \tTraining Loss:  116.66830133928572\n",
      "\tEpoch 220 complete! \tTraining Loss:  116.67082421875\n",
      "\tEpoch 221 complete! \tTraining Loss:  116.6030703125\n",
      "\tEpoch 222 complete! \tTraining Loss:  116.53768359375\n",
      "\tEpoch 223 complete! \tTraining Loss:  116.45742578125\n",
      "\tEpoch 224 complete! \tTraining Loss:  116.4220859375\n",
      "\tEpoch 225 complete! \tTraining Loss:  116.32504631696429\n",
      "\tEpoch 226 complete! \tTraining Loss:  116.42693247767858\n",
      "\tEpoch 227 complete! \tTraining Loss:  116.26482087053571\n",
      "\tEpoch 228 complete! \tTraining Loss:  116.35118191964285\n",
      "\tEpoch 229 complete! \tTraining Loss:  116.27613616071429\n",
      "\tEpoch 230 complete! \tTraining Loss:  116.30421875\n",
      "\tEpoch 231 complete! \tTraining Loss:  116.17520926339286\n",
      "\tEpoch 232 complete! \tTraining Loss:  116.19436383928571\n",
      "\tEpoch 233 complete! \tTraining Loss:  116.12909709821429\n",
      "\tEpoch 234 complete! \tTraining Loss:  116.26940401785714\n",
      "\tEpoch 235 complete! \tTraining Loss:  116.16003125\n",
      "\tEpoch 236 complete! \tTraining Loss:  116.08381473214286\n",
      "\tEpoch 237 complete! \tTraining Loss:  116.03958537946428\n",
      "\tEpoch 238 complete! \tTraining Loss:  116.03014453125\n",
      "\tEpoch 239 complete! \tTraining Loss:  116.01473381696428\n",
      "\tEpoch 240 complete! \tTraining Loss:  116.01840680803572\n",
      "\tEpoch 241 complete! \tTraining Loss:  115.95817020089285\n",
      "\tEpoch 242 complete! \tTraining Loss:  115.91568415178571\n",
      "\tEpoch 243 complete! \tTraining Loss:  115.80185825892858\n",
      "\tEpoch 244 complete! \tTraining Loss:  115.88523660714286\n",
      "\tEpoch 245 complete! \tTraining Loss:  115.79563392857143\n",
      "\tEpoch 246 complete! \tTraining Loss:  115.72210435267857\n",
      "\tEpoch 247 complete! \tTraining Loss:  115.69028069196429\n",
      "\tEpoch 248 complete! \tTraining Loss:  115.70244921875\n",
      "\tEpoch 249 complete! \tTraining Loss:  115.71222712053572\n",
      "\tEpoch 250 complete! \tTraining Loss:  115.59934207589286\n",
      "\tEpoch 251 complete! \tTraining Loss:  115.51419698660715\n",
      "\tEpoch 252 complete! \tTraining Loss:  115.53983147321429\n",
      "\tEpoch 253 complete! \tTraining Loss:  115.53919252232143\n",
      "\tEpoch 254 complete! \tTraining Loss:  115.55082254464286\n",
      "\tEpoch 255 complete! \tTraining Loss:  115.38476395089286\n",
      "\tEpoch 256 complete! \tTraining Loss:  115.418546875\n",
      "\tEpoch 257 complete! \tTraining Loss:  115.4502265625\n",
      "\tEpoch 258 complete! \tTraining Loss:  115.39402957589286\n",
      "\tEpoch 259 complete! \tTraining Loss:  115.41136662946428\n",
      "\tEpoch 260 complete! \tTraining Loss:  115.32822935267858\n",
      "\tEpoch 261 complete! \tTraining Loss:  115.32489229910715\n",
      "\tEpoch 262 complete! \tTraining Loss:  115.28421316964285\n",
      "\tEpoch 263 complete! \tTraining Loss:  115.33005636160715\n",
      "\tEpoch 264 complete! \tTraining Loss:  115.22165569196429\n",
      "\tEpoch 265 complete! \tTraining Loss:  115.21339229910714\n",
      "\tEpoch 266 complete! \tTraining Loss:  115.15666238839286\n",
      "\tEpoch 267 complete! \tTraining Loss:  115.17495535714286\n",
      "\tEpoch 268 complete! \tTraining Loss:  115.06402566964286\n",
      "\tEpoch 269 complete! \tTraining Loss:  115.10681361607143\n",
      "\tEpoch 270 complete! \tTraining Loss:  115.08274107142857\n",
      "\tEpoch 271 complete! \tTraining Loss:  114.94024609375\n",
      "\tEpoch 272 complete! \tTraining Loss:  114.9745703125\n",
      "\tEpoch 273 complete! \tTraining Loss:  114.98570424107143\n",
      "\tEpoch 274 complete! \tTraining Loss:  114.92504966517858\n",
      "\tEpoch 275 complete! \tTraining Loss:  114.99302566964286\n",
      "\tEpoch 276 complete! \tTraining Loss:  114.9005390625\n",
      "\tEpoch 277 complete! \tTraining Loss:  114.88594029017857\n",
      "\tEpoch 278 complete! \tTraining Loss:  114.86130859375\n",
      "\tEpoch 279 complete! \tTraining Loss:  114.80122600446428\n",
      "\tEpoch 280 complete! \tTraining Loss:  114.71784151785714\n",
      "\tEpoch 281 complete! \tTraining Loss:  114.80045814732142\n",
      "\tEpoch 282 complete! \tTraining Loss:  114.64965457589285\n",
      "\tEpoch 283 complete! \tTraining Loss:  114.69617020089285\n",
      "\tEpoch 284 complete! \tTraining Loss:  114.68953850446428\n",
      "\tEpoch 285 complete! \tTraining Loss:  114.61702845982143\n",
      "\tEpoch 286 complete! \tTraining Loss:  114.59962276785714\n",
      "\tEpoch 287 complete! \tTraining Loss:  114.56457254464286\n",
      "\tEpoch 288 complete! \tTraining Loss:  114.60469587053572\n",
      "\tEpoch 289 complete! \tTraining Loss:  114.50770479910715\n",
      "\tEpoch 290 complete! \tTraining Loss:  114.58633091517858\n",
      "\tEpoch 291 complete! \tTraining Loss:  114.44906696428572\n",
      "\tEpoch 292 complete! \tTraining Loss:  114.44834375\n",
      "\tEpoch 293 complete! \tTraining Loss:  114.48491573660715\n",
      "\tEpoch 294 complete! \tTraining Loss:  114.44881919642857\n",
      "\tEpoch 295 complete! \tTraining Loss:  114.41940290178572\n",
      "\tEpoch 296 complete! \tTraining Loss:  114.34307477678571\n",
      "\tEpoch 297 complete! \tTraining Loss:  114.37918080357143\n",
      "\tEpoch 298 complete! \tTraining Loss:  114.31167243303571\n",
      "\tEpoch 299 complete! \tTraining Loss:  114.27037723214286\n",
      "\tEpoch 300 complete! \tTraining Loss:  114.23957979910715\n",
      "torch.Size([7000, 784])\n",
      "3\n",
      "\tEpoch 1 complete! \tTraining Loss:  373.5739520089286\n",
      "\tEpoch 2 complete! \tTraining Loss:  251.76027232142857\n",
      "\tEpoch 3 complete! \tTraining Loss:  227.72448214285714\n",
      "\tEpoch 4 complete! \tTraining Loss:  207.2277310267857\n",
      "\tEpoch 5 complete! \tTraining Loss:  191.52291517857142\n",
      "\tEpoch 6 complete! \tTraining Loss:  181.38632589285714\n",
      "\tEpoch 7 complete! \tTraining Loss:  174.44123549107144\n",
      "\tEpoch 8 complete! \tTraining Loss:  168.8800703125\n",
      "\tEpoch 9 complete! \tTraining Loss:  164.26204910714284\n",
      "\tEpoch 10 complete! \tTraining Loss:  160.47271875\n",
      "\tEpoch 11 complete! \tTraining Loss:  157.39136049107142\n",
      "\tEpoch 12 complete! \tTraining Loss:  155.03890848214286\n",
      "\tEpoch 13 complete! \tTraining Loss:  152.96031138392857\n",
      "\tEpoch 14 complete! \tTraining Loss:  151.06402790178572\n",
      "\tEpoch 15 complete! \tTraining Loss:  149.4659375\n",
      "\tEpoch 16 complete! \tTraining Loss:  148.01607589285715\n",
      "\tEpoch 17 complete! \tTraining Loss:  146.67586160714285\n",
      "\tEpoch 18 complete! \tTraining Loss:  145.42873995535714\n",
      "\tEpoch 19 complete! \tTraining Loss:  144.2004017857143\n",
      "\tEpoch 20 complete! \tTraining Loss:  143.17034598214286\n",
      "\tEpoch 21 complete! \tTraining Loss:  142.1224263392857\n",
      "\tEpoch 22 complete! \tTraining Loss:  141.2512885044643\n",
      "\tEpoch 23 complete! \tTraining Loss:  140.32781082589287\n",
      "\tEpoch 24 complete! \tTraining Loss:  139.57381584821428\n",
      "\tEpoch 25 complete! \tTraining Loss:  138.83685714285716\n",
      "\tEpoch 26 complete! \tTraining Loss:  138.13188895089286\n",
      "\tEpoch 27 complete! \tTraining Loss:  137.5035747767857\n",
      "\tEpoch 28 complete! \tTraining Loss:  136.8206958705357\n",
      "\tEpoch 29 complete! \tTraining Loss:  136.18675055803573\n",
      "\tEpoch 30 complete! \tTraining Loss:  135.65658426339286\n",
      "\tEpoch 31 complete! \tTraining Loss:  135.13998549107143\n",
      "\tEpoch 32 complete! \tTraining Loss:  134.54086997767857\n",
      "\tEpoch 33 complete! \tTraining Loss:  134.2259174107143\n",
      "\tEpoch 34 complete! \tTraining Loss:  133.63696540178572\n",
      "\tEpoch 35 complete! \tTraining Loss:  133.15649776785713\n",
      "\tEpoch 36 complete! \tTraining Loss:  132.77101785714285\n",
      "\tEpoch 37 complete! \tTraining Loss:  132.3927260044643\n",
      "\tEpoch 38 complete! \tTraining Loss:  131.98806863839286\n",
      "\tEpoch 39 complete! \tTraining Loss:  131.61352176339287\n",
      "\tEpoch 40 complete! \tTraining Loss:  131.28282645089286\n",
      "\tEpoch 41 complete! \tTraining Loss:  130.86605747767857\n",
      "\tEpoch 42 complete! \tTraining Loss:  130.50070479910715\n",
      "\tEpoch 43 complete! \tTraining Loss:  130.19622823660714\n",
      "\tEpoch 44 complete! \tTraining Loss:  129.86645926339287\n",
      "\tEpoch 45 complete! \tTraining Loss:  129.6042271205357\n",
      "\tEpoch 46 complete! \tTraining Loss:  129.31373995535714\n",
      "\tEpoch 47 complete! \tTraining Loss:  129.03652008928572\n",
      "\tEpoch 48 complete! \tTraining Loss:  128.83799665178572\n",
      "\tEpoch 49 complete! \tTraining Loss:  128.5031796875\n",
      "\tEpoch 50 complete! \tTraining Loss:  128.19070591517857\n",
      "\tEpoch 51 complete! \tTraining Loss:  127.95228459821429\n",
      "\tEpoch 52 complete! \tTraining Loss:  127.73394196428572\n",
      "\tEpoch 53 complete! \tTraining Loss:  127.52475055803572\n",
      "\tEpoch 54 complete! \tTraining Loss:  127.30402008928571\n",
      "\tEpoch 55 complete! \tTraining Loss:  127.04944308035714\n",
      "\tEpoch 56 complete! \tTraining Loss:  126.93734542410715\n",
      "\tEpoch 57 complete! \tTraining Loss:  126.62680691964286\n",
      "\tEpoch 58 complete! \tTraining Loss:  126.46404408482142\n",
      "\tEpoch 59 complete! \tTraining Loss:  126.25918805803572\n",
      "\tEpoch 60 complete! \tTraining Loss:  126.08753180803572\n",
      "\tEpoch 61 complete! \tTraining Loss:  125.83125111607143\n",
      "\tEpoch 62 complete! \tTraining Loss:  125.76826171875\n",
      "\tEpoch 63 complete! \tTraining Loss:  125.43973214285714\n",
      "\tEpoch 64 complete! \tTraining Loss:  125.3902265625\n",
      "\tEpoch 65 complete! \tTraining Loss:  125.24942857142857\n",
      "\tEpoch 66 complete! \tTraining Loss:  125.03010212053572\n",
      "\tEpoch 67 complete! \tTraining Loss:  124.857828125\n",
      "\tEpoch 68 complete! \tTraining Loss:  124.70360993303571\n",
      "\tEpoch 69 complete! \tTraining Loss:  124.58158035714285\n",
      "\tEpoch 70 complete! \tTraining Loss:  124.4404453125\n",
      "\tEpoch 71 complete! \tTraining Loss:  124.23236997767857\n",
      "\tEpoch 72 complete! \tTraining Loss:  124.16757589285714\n",
      "\tEpoch 73 complete! \tTraining Loss:  123.95997991071428\n",
      "\tEpoch 74 complete! \tTraining Loss:  123.8421953125\n",
      "\tEpoch 75 complete! \tTraining Loss:  123.74294196428572\n",
      "\tEpoch 76 complete! \tTraining Loss:  123.66333705357142\n",
      "\tEpoch 77 complete! \tTraining Loss:  123.38501450892858\n",
      "\tEpoch 78 complete! \tTraining Loss:  123.29496261160715\n",
      "\tEpoch 79 complete! \tTraining Loss:  123.18448716517857\n",
      "\tEpoch 80 complete! \tTraining Loss:  123.12939397321429\n",
      "\tEpoch 81 complete! \tTraining Loss:  122.96966852678571\n",
      "\tEpoch 82 complete! \tTraining Loss:  122.81799051339286\n",
      "\tEpoch 83 complete! \tTraining Loss:  122.77411830357143\n",
      "\tEpoch 84 complete! \tTraining Loss:  122.62668080357143\n",
      "\tEpoch 85 complete! \tTraining Loss:  122.49924441964286\n",
      "\tEpoch 86 complete! \tTraining Loss:  122.40066071428572\n",
      "\tEpoch 87 complete! \tTraining Loss:  122.22008872767857\n",
      "\tEpoch 88 complete! \tTraining Loss:  122.1805546875\n",
      "\tEpoch 89 complete! \tTraining Loss:  122.06002399553572\n",
      "\tEpoch 90 complete! \tTraining Loss:  121.93615457589286\n",
      "\tEpoch 91 complete! \tTraining Loss:  121.83390625\n",
      "\tEpoch 92 complete! \tTraining Loss:  121.80322265625\n",
      "\tEpoch 93 complete! \tTraining Loss:  121.63857254464286\n",
      "\tEpoch 94 complete! \tTraining Loss:  121.57595647321429\n",
      "\tEpoch 95 complete! \tTraining Loss:  121.47703627232143\n",
      "\tEpoch 96 complete! \tTraining Loss:  121.36409988839286\n",
      "\tEpoch 97 complete! \tTraining Loss:  121.32003850446428\n",
      "\tEpoch 98 complete! \tTraining Loss:  121.12718247767857\n",
      "\tEpoch 99 complete! \tTraining Loss:  121.12736328125\n",
      "\tEpoch 100 complete! \tTraining Loss:  121.01702790178571\n",
      "\tEpoch 101 complete! \tTraining Loss:  121.00422488839286\n",
      "\tEpoch 102 complete! \tTraining Loss:  120.88297209821428\n",
      "\tEpoch 103 complete! \tTraining Loss:  120.78838950892857\n",
      "\tEpoch 104 complete! \tTraining Loss:  120.75894810267857\n",
      "\tEpoch 105 complete! \tTraining Loss:  120.57190959821429\n",
      "\tEpoch 106 complete! \tTraining Loss:  120.58358147321428\n",
      "\tEpoch 107 complete! \tTraining Loss:  120.43786439732143\n",
      "\tEpoch 108 complete! \tTraining Loss:  120.36213058035715\n",
      "\tEpoch 109 complete! \tTraining Loss:  120.28138448660714\n",
      "\tEpoch 110 complete! \tTraining Loss:  120.22416573660715\n",
      "\tEpoch 111 complete! \tTraining Loss:  120.11240011160714\n",
      "\tEpoch 112 complete! \tTraining Loss:  119.96448828125\n",
      "\tEpoch 113 complete! \tTraining Loss:  120.00683928571429\n",
      "\tEpoch 114 complete! \tTraining Loss:  119.91558370535714\n",
      "\tEpoch 115 complete! \tTraining Loss:  119.82952455357143\n",
      "\tEpoch 116 complete! \tTraining Loss:  119.76312779017857\n",
      "\tEpoch 117 complete! \tTraining Loss:  119.68610770089286\n",
      "\tEpoch 118 complete! \tTraining Loss:  119.54616350446429\n",
      "\tEpoch 119 complete! \tTraining Loss:  119.53083203125\n",
      "\tEpoch 120 complete! \tTraining Loss:  119.54226004464286\n",
      "\tEpoch 121 complete! \tTraining Loss:  119.37843191964286\n",
      "\tEpoch 122 complete! \tTraining Loss:  119.29461941964286\n",
      "\tEpoch 123 complete! \tTraining Loss:  119.28735267857142\n",
      "\tEpoch 124 complete! \tTraining Loss:  119.20557477678571\n",
      "\tEpoch 125 complete! \tTraining Loss:  119.03973995535715\n",
      "\tEpoch 126 complete! \tTraining Loss:  119.06430747767857\n",
      "\tEpoch 127 complete! \tTraining Loss:  118.97427622767857\n",
      "\tEpoch 128 complete! \tTraining Loss:  118.95812555803572\n",
      "\tEpoch 129 complete! \tTraining Loss:  118.85795256696429\n",
      "\tEpoch 130 complete! \tTraining Loss:  118.797875\n",
      "\tEpoch 131 complete! \tTraining Loss:  118.74665345982143\n",
      "\tEpoch 132 complete! \tTraining Loss:  118.64651897321428\n",
      "\tEpoch 133 complete! \tTraining Loss:  118.55041741071429\n",
      "\tEpoch 134 complete! \tTraining Loss:  118.60357421875\n",
      "\tEpoch 135 complete! \tTraining Loss:  118.5220546875\n",
      "\tEpoch 136 complete! \tTraining Loss:  118.49148493303572\n",
      "\tEpoch 137 complete! \tTraining Loss:  118.39076339285714\n",
      "\tEpoch 138 complete! \tTraining Loss:  118.49066071428571\n",
      "\tEpoch 139 complete! \tTraining Loss:  118.26699386160715\n",
      "\tEpoch 140 complete! \tTraining Loss:  118.1598671875\n",
      "\tEpoch 141 complete! \tTraining Loss:  118.16958761160714\n",
      "\tEpoch 142 complete! \tTraining Loss:  118.09396986607143\n",
      "\tEpoch 143 complete! \tTraining Loss:  118.03963002232143\n",
      "\tEpoch 144 complete! \tTraining Loss:  118.06033314732143\n",
      "\tEpoch 145 complete! \tTraining Loss:  117.94308984375\n",
      "\tEpoch 146 complete! \tTraining Loss:  117.86104910714286\n",
      "\tEpoch 147 complete! \tTraining Loss:  117.87838895089286\n",
      "\tEpoch 148 complete! \tTraining Loss:  117.72198549107142\n",
      "\tEpoch 149 complete! \tTraining Loss:  117.72645703125\n",
      "\tEpoch 150 complete! \tTraining Loss:  117.62802790178571\n",
      "\tEpoch 151 complete! \tTraining Loss:  117.56092522321428\n",
      "\tEpoch 152 complete! \tTraining Loss:  117.46848046875\n",
      "\tEpoch 153 complete! \tTraining Loss:  117.50171428571429\n",
      "\tEpoch 154 complete! \tTraining Loss:  117.48755803571429\n",
      "\tEpoch 155 complete! \tTraining Loss:  117.35720256696429\n",
      "\tEpoch 156 complete! \tTraining Loss:  117.38810546875\n",
      "\tEpoch 157 complete! \tTraining Loss:  117.31100055803572\n",
      "\tEpoch 158 complete! \tTraining Loss:  117.24636886160714\n",
      "\tEpoch 159 complete! \tTraining Loss:  117.21178348214286\n",
      "\tEpoch 160 complete! \tTraining Loss:  117.1600625\n",
      "\tEpoch 161 complete! \tTraining Loss:  117.11183258928571\n",
      "\tEpoch 162 complete! \tTraining Loss:  117.1158828125\n",
      "\tEpoch 163 complete! \tTraining Loss:  117.02472433035715\n",
      "\tEpoch 164 complete! \tTraining Loss:  116.96747321428572\n",
      "\tEpoch 165 complete! \tTraining Loss:  116.95229743303571\n",
      "\tEpoch 166 complete! \tTraining Loss:  116.89512276785715\n",
      "\tEpoch 167 complete! \tTraining Loss:  116.76337444196429\n",
      "\tEpoch 168 complete! \tTraining Loss:  116.76440345982142\n",
      "\tEpoch 169 complete! \tTraining Loss:  116.71563113839285\n",
      "\tEpoch 170 complete! \tTraining Loss:  116.73319754464286\n",
      "\tEpoch 171 complete! \tTraining Loss:  116.71273995535714\n",
      "\tEpoch 172 complete! \tTraining Loss:  116.7105234375\n",
      "\tEpoch 173 complete! \tTraining Loss:  116.58866685267857\n",
      "\tEpoch 174 complete! \tTraining Loss:  116.57210993303572\n",
      "\tEpoch 175 complete! \tTraining Loss:  116.48358203125\n",
      "\tEpoch 176 complete! \tTraining Loss:  116.46163616071429\n",
      "\tEpoch 177 complete! \tTraining Loss:  116.33778292410715\n",
      "\tEpoch 178 complete! \tTraining Loss:  116.37712109375\n",
      "\tEpoch 179 complete! \tTraining Loss:  116.2805\n",
      "\tEpoch 180 complete! \tTraining Loss:  116.30475\n",
      "\tEpoch 181 complete! \tTraining Loss:  116.23688950892857\n",
      "\tEpoch 182 complete! \tTraining Loss:  116.16165011160714\n",
      "\tEpoch 183 complete! \tTraining Loss:  116.1371640625\n",
      "\tEpoch 184 complete! \tTraining Loss:  116.09617857142857\n",
      "\tEpoch 185 complete! \tTraining Loss:  116.18010658482143\n",
      "\tEpoch 186 complete! \tTraining Loss:  116.08801283482143\n",
      "\tEpoch 187 complete! \tTraining Loss:  115.99154017857143\n",
      "\tEpoch 188 complete! \tTraining Loss:  115.90798828125\n",
      "\tEpoch 189 complete! \tTraining Loss:  115.86188225446429\n",
      "\tEpoch 190 complete! \tTraining Loss:  115.88278292410715\n",
      "\tEpoch 191 complete! \tTraining Loss:  115.88403125\n",
      "\tEpoch 192 complete! \tTraining Loss:  115.76153683035714\n",
      "\tEpoch 193 complete! \tTraining Loss:  115.82485267857143\n",
      "\tEpoch 194 complete! \tTraining Loss:  115.73896484375\n",
      "\tEpoch 195 complete! \tTraining Loss:  115.67261941964286\n",
      "\tEpoch 196 complete! \tTraining Loss:  115.67191517857142\n",
      "\tEpoch 197 complete! \tTraining Loss:  115.58451618303572\n",
      "\tEpoch 198 complete! \tTraining Loss:  115.55311830357142\n",
      "\tEpoch 199 complete! \tTraining Loss:  115.4849609375\n",
      "\tEpoch 200 complete! \tTraining Loss:  115.50161272321428\n",
      "\tEpoch 201 complete! \tTraining Loss:  115.44901953125\n",
      "\tEpoch 202 complete! \tTraining Loss:  115.41562332589285\n",
      "\tEpoch 203 complete! \tTraining Loss:  115.39273828125\n",
      "\tEpoch 204 complete! \tTraining Loss:  115.47410602678572\n",
      "\tEpoch 205 complete! \tTraining Loss:  115.38182254464286\n",
      "\tEpoch 206 complete! \tTraining Loss:  115.31131975446428\n",
      "\tEpoch 207 complete! \tTraining Loss:  115.22801841517857\n",
      "\tEpoch 208 complete! \tTraining Loss:  115.205328125\n",
      "\tEpoch 209 complete! \tTraining Loss:  115.1519765625\n",
      "\tEpoch 210 complete! \tTraining Loss:  115.14769140625\n",
      "\tEpoch 211 complete! \tTraining Loss:  115.16908705357143\n",
      "\tEpoch 212 complete! \tTraining Loss:  115.08518247767857\n",
      "\tEpoch 213 complete! \tTraining Loss:  115.00763058035714\n",
      "\tEpoch 214 complete! \tTraining Loss:  115.01375725446428\n",
      "\tEpoch 215 complete! \tTraining Loss:  114.93373828125\n",
      "\tEpoch 216 complete! \tTraining Loss:  114.89721930803572\n",
      "\tEpoch 217 complete! \tTraining Loss:  114.94480022321429\n",
      "\tEpoch 218 complete! \tTraining Loss:  114.84384709821428\n",
      "\tEpoch 219 complete! \tTraining Loss:  114.79506919642857\n",
      "\tEpoch 220 complete! \tTraining Loss:  114.78153571428571\n",
      "\tEpoch 221 complete! \tTraining Loss:  114.78170814732142\n",
      "\tEpoch 222 complete! \tTraining Loss:  114.66244866071429\n",
      "\tEpoch 223 complete! \tTraining Loss:  114.76822098214285\n",
      "\tEpoch 224 complete! \tTraining Loss:  114.69828850446429\n",
      "\tEpoch 225 complete! \tTraining Loss:  114.63716183035714\n",
      "\tEpoch 226 complete! \tTraining Loss:  114.61963950892857\n",
      "\tEpoch 227 complete! \tTraining Loss:  114.65522209821428\n",
      "\tEpoch 228 complete! \tTraining Loss:  114.47682142857143\n",
      "\tEpoch 229 complete! \tTraining Loss:  114.55027176339286\n",
      "\tEpoch 230 complete! \tTraining Loss:  114.47322209821428\n",
      "\tEpoch 231 complete! \tTraining Loss:  114.49913504464286\n",
      "\tEpoch 232 complete! \tTraining Loss:  114.45122488839286\n",
      "\tEpoch 233 complete! \tTraining Loss:  114.31124162946429\n",
      "\tEpoch 234 complete! \tTraining Loss:  114.4564921875\n",
      "\tEpoch 235 complete! \tTraining Loss:  114.36544587053571\n",
      "\tEpoch 236 complete! \tTraining Loss:  114.26166127232143\n",
      "\tEpoch 237 complete! \tTraining Loss:  114.3301953125\n",
      "\tEpoch 238 complete! \tTraining Loss:  114.29166852678571\n",
      "\tEpoch 239 complete! \tTraining Loss:  114.23763225446429\n",
      "\tEpoch 240 complete! \tTraining Loss:  114.28798325892858\n",
      "\tEpoch 241 complete! \tTraining Loss:  114.14622879464285\n",
      "\tEpoch 242 complete! \tTraining Loss:  114.10055357142858\n",
      "\tEpoch 243 complete! \tTraining Loss:  114.11705301339286\n",
      "\tEpoch 244 complete! \tTraining Loss:  114.10131863839285\n",
      "\tEpoch 245 complete! \tTraining Loss:  114.05365457589286\n",
      "\tEpoch 246 complete! \tTraining Loss:  114.07479185267857\n",
      "\tEpoch 247 complete! \tTraining Loss:  113.97878180803572\n",
      "\tEpoch 248 complete! \tTraining Loss:  113.96385323660714\n",
      "\tEpoch 249 complete! \tTraining Loss:  113.85974609375\n",
      "\tEpoch 250 complete! \tTraining Loss:  113.84885044642857\n",
      "\tEpoch 251 complete! \tTraining Loss:  113.91120368303571\n",
      "\tEpoch 252 complete! \tTraining Loss:  113.85333872767858\n",
      "\tEpoch 253 complete! \tTraining Loss:  113.83959095982142\n",
      "\tEpoch 254 complete! \tTraining Loss:  113.78573772321428\n",
      "\tEpoch 255 complete! \tTraining Loss:  113.64913392857143\n",
      "\tEpoch 256 complete! \tTraining Loss:  113.67863448660714\n",
      "\tEpoch 257 complete! \tTraining Loss:  113.65763950892857\n",
      "\tEpoch 258 complete! \tTraining Loss:  113.59327678571428\n",
      "\tEpoch 259 complete! \tTraining Loss:  113.66821819196429\n",
      "\tEpoch 260 complete! \tTraining Loss:  113.63556863839285\n",
      "\tEpoch 261 complete! \tTraining Loss:  113.6416171875\n",
      "\tEpoch 262 complete! \tTraining Loss:  113.52601897321429\n",
      "\tEpoch 263 complete! \tTraining Loss:  113.61787332589286\n",
      "\tEpoch 264 complete! \tTraining Loss:  113.57818526785714\n",
      "\tEpoch 265 complete! \tTraining Loss:  113.55654017857142\n",
      "\tEpoch 266 complete! \tTraining Loss:  113.51565736607142\n",
      "\tEpoch 267 complete! \tTraining Loss:  113.43586160714285\n",
      "\tEpoch 268 complete! \tTraining Loss:  113.48106640625\n",
      "\tEpoch 269 complete! \tTraining Loss:  113.39315513392857\n",
      "\tEpoch 270 complete! \tTraining Loss:  113.38611607142857\n",
      "\tEpoch 271 complete! \tTraining Loss:  113.30429129464285\n",
      "\tEpoch 272 complete! \tTraining Loss:  113.36984040178571\n",
      "\tEpoch 273 complete! \tTraining Loss:  113.30459877232143\n",
      "\tEpoch 274 complete! \tTraining Loss:  113.23952232142857\n",
      "\tEpoch 275 complete! \tTraining Loss:  113.14317745535715\n",
      "\tEpoch 276 complete! \tTraining Loss:  113.19775111607143\n",
      "\tEpoch 277 complete! \tTraining Loss:  113.20298381696429\n",
      "\tEpoch 278 complete! \tTraining Loss:  113.25881529017857\n",
      "\tEpoch 279 complete! \tTraining Loss:  113.2048125\n",
      "\tEpoch 280 complete! \tTraining Loss:  113.12169252232142\n",
      "\tEpoch 281 complete! \tTraining Loss:  113.14325390625\n",
      "\tEpoch 282 complete! \tTraining Loss:  113.08727455357143\n",
      "\tEpoch 283 complete! \tTraining Loss:  113.08234207589285\n",
      "\tEpoch 284 complete! \tTraining Loss:  113.00586830357143\n",
      "\tEpoch 285 complete! \tTraining Loss:  112.91828292410715\n",
      "\tEpoch 286 complete! \tTraining Loss:  112.96503180803572\n",
      "\tEpoch 287 complete! \tTraining Loss:  112.86865792410714\n",
      "\tEpoch 288 complete! \tTraining Loss:  112.90300279017858\n",
      "\tEpoch 289 complete! \tTraining Loss:  112.90927064732143\n",
      "\tEpoch 290 complete! \tTraining Loss:  112.99858426339286\n",
      "\tEpoch 291 complete! \tTraining Loss:  112.83548604910715\n",
      "\tEpoch 292 complete! \tTraining Loss:  112.88086495535714\n",
      "\tEpoch 293 complete! \tTraining Loss:  112.84283649553572\n",
      "\tEpoch 294 complete! \tTraining Loss:  112.82059095982143\n",
      "\tEpoch 295 complete! \tTraining Loss:  112.83716964285715\n",
      "\tEpoch 296 complete! \tTraining Loss:  112.79899386160714\n",
      "\tEpoch 297 complete! \tTraining Loss:  112.72098270089286\n",
      "\tEpoch 298 complete! \tTraining Loss:  112.71381863839285\n",
      "\tEpoch 299 complete! \tTraining Loss:  112.67193024553572\n",
      "\tEpoch 300 complete! \tTraining Loss:  112.66463225446428\n",
      "torch.Size([7000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Training the experts \n",
    "\n",
    "task_id = 0\n",
    "# VAE Model Hyperparameters\n",
    "# batch_size = 100\n",
    "\n",
    "inputdim = dataStream.unlabeledData[0].shape[2]*dataStream.unlabeledData[0].shape[3]\n",
    "hiddendim1 = 650\n",
    "hiddendim2 = 300\n",
    "latent_dim = [64,64, 64,64,]\n",
    "tot_clust = 600      \n",
    "lr = [2e-4,2e-4,2e-4,2e-4,]\n",
    "task_clust_lbls = {}\n",
    "# epochs = 80\n",
    "epch = [300, 300 , 300, 300 ]\n",
    "s_encoder = mnist_vae.Encoder(input_dim =inputdim , hidden_dim1=hiddendim1, hidden_dim2=hiddendim2, latent_dim=latent_dim[task_id])\n",
    "s_decoder = mnist_vae.Decoder(latent_dim=latent_dim[task_id], hidden_dim1=hiddendim1,hidden_dim2=hiddendim2, output_dim=inputdim)\n",
    "s_task_model = mnist_vae.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "optimizer_VAE = Adam(s_task_model.parameters(), lr=lr[task_id])\n",
    "batchsize = 1000 #dataStream.batchSize\n",
    "\n",
    "# num1 = 20\n",
    "cnt = 0\n",
    "\n",
    "\n",
    "for task_id in range(4):\n",
    "\n",
    "  print(task_id)\n",
    "  data_loader, trn_data = trn_loader(task_id, batchsize)\n",
    "\n",
    "  s_task_model = vae_train(data_loader,s_task_model,optimizer_VAE ,epch[task_id],batchsize,inputdim)\n",
    "  torch.save(s_task_model.state_dict(),'s_task'+str(task_id)+'_model')\n",
    "\n",
    "\n",
    "  for i in range(1):\n",
    "    (x_lbl,y_lbl) = next(iter(data_loader))\n",
    "\n",
    "  x_lbl = x_lbl.view(batchsize,inputdim)\n",
    "  s_task_model.eval()\n",
    "  _,z_lbl,_ = s_task_model(x_lbl.to(DEVICE))\n",
    "  batch_ful = 7000\n",
    "  # dataStream.unlabeledData[0].shape[0]\n",
    "  train_loader_ful = DataLoader(dataset=trn_data, batch_size=batch_ful, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "  x_ful, y_ful, x_out, z_mean_vae = latent_data(train_loader_ful,s_task_model,inputdim,batch_ful)\n",
    "\n",
    "  X = z_mean_vae.cpu().detach().numpy()\n",
    "\n",
    "  clust_lbl =  torch.unique(y_lbl).detach().numpy()\n",
    "  n_clust = len(clust_lbl)\n",
    "\n",
    "  if task_id ==0 :\n",
    "    z_mn = {} \n",
    "    z_EVal = {} \n",
    "    z_EVec = {} \n",
    "  \n",
    "  init_cntroid = torch.zeros((n_clust, latent_dim[task_id]))\n",
    "\n",
    "  for num_clust in range(n_clust):\n",
    "    init_cntroid[num_clust,:] = torch.mean(z_lbl[y_lbl == clust_lbl[num_clust]],0)\n",
    "\n",
    "  init_cntroid = init_cntroid.detach().numpy()\n",
    "  #'k-means++'\n",
    "  kmeans = KMeans(n_clusters=tot_clust, random_state=3, init = 'k-means++' , n_init=\"auto\",max_iter=8000, tol=0.00002,  ).fit(z_lbl.cpu().detach().numpy())\n",
    "  clus_lbls = asgn_clust_labl(kmeans, y_lbl.detach().numpy())\n",
    "  task_clust_lbls[task_id] = clus_lbls\n",
    "\n",
    "  for i in range(n_clust):\n",
    "    zxmean =  torch.mean(z_lbl[y_lbl == clust_lbl[i]],0)\n",
    "    eVal, eVec = structure_data(z_lbl[y_lbl == clust_lbl[i]].T)\n",
    "    # if task_id == 0:\n",
    "    z_mn[task_id*n_clust+i] = zxmean\n",
    "    z_EVal[task_id*n_clust+i] = eVal\n",
    "    z_EVec[task_id*n_clust+i] = eVec\n",
    "\n",
    "  if task_id == 0:\n",
    "    # tmodel0 = task_model\n",
    "    filename = 'Spl0_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 1:\n",
    "    # tmodel1 = task_model\n",
    "    filename = 'Spl1_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 2:\n",
    "    # tmodel2 = task_model\n",
    "    filename = 'Spl2_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 3:\n",
    "    # tmodel3 = task_model\n",
    "    filename = 'Spl3_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1690883380263,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "oxxfzxSG1CRg"
   },
   "outputs": [],
   "source": [
    "tsk_count = 4\n",
    "arr_cnt = len(z_mn)\n",
    "\n",
    "sample_size = 125 # Sample size for the structured data\n",
    "\n",
    "# struct_samp_arr = torch.zeros((arr_cnt*sample_size,inputdim))\n",
    "struct_samp_arr = torch.zeros((arr_cnt*sample_size,dataStream.unlabeledData[0].shape[1],\n",
    "                               dataStream.unlabeledData[0].shape[2],dataStream.unlabeledData[0].shape[3]))\n",
    "tsk_arr = torch.zeros((arr_cnt*sample_size))\n",
    "arr = torch.zeros((arr_cnt*sample_size,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1690883386850,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "M3BVOLlr8Dd9"
   },
   "outputs": [],
   "source": [
    "### Structured data generation\n",
    "for n_cnt in range (len(z_mn)):\n",
    "  s_encoder = mnist_vae.Encoder(input_dim =inputdim , hidden_dim1=hiddendim1, hidden_dim2=hiddendim2,latent_dim=latent_dim[n_cnt//10])\n",
    "  s_decoder = mnist_vae.Decoder(latent_dim=latent_dim[n_cnt//10],hidden_dim1=hiddendim1,hidden_dim2=hiddendim2, output_dim=inputdim)\n",
    "  savd_model = mnist_vae.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "  # for c_cnt in range (2):\n",
    "  std_vec = torch.randn(sample_size, latent_dim[n_cnt//10]).to(DEVICE) #torch.empty(sample_size, latent_dim).normal_(mean=0,std=1.0) #\n",
    "  a_vec = torch.mul(std_vec,z_EVal[n_cnt])\n",
    "  a_vec = torch.matmul(z_EVec[n_cnt],a_vec.T).T+z_mn[n_cnt][None,:]\n",
    "  # cl1_vec_T1_0 = torch.matmul(std_vec.to(DEVICE),z_Ftrans[:,:,(n_cnt)])\n",
    "  # cl1_vec_T1_0 = torch.add(cl1_vec_T1_0,z_mn[:,n_cnt][None,:])\n",
    "  # print(n_cnt)\n",
    "  \n",
    "  savd_model.load_state_dict(torch.load('s_task'+str(n_cnt//10)+'_model'))\n",
    "  savd_model = savd_model.to(DEVICE)\n",
    "  struct_samp_arr[(sample_size)*(n_cnt):(sample_size)*(n_cnt+1),:,:,:] = savd_model.Decoder(a_vec).view(a_vec.shape[0],\n",
    "                                                                                                       1,28,28)\n",
    "  tsk_arr[n_cnt*sample_size:(n_cnt+1)*sample_size] = n_cnt//10\n",
    "  arr[(sample_size)*(n_cnt):(sample_size)*(n_cnt+1),:] = a_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1690883386850,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "oigVdTDZbujT",
    "outputId": "8398dfe4-56ca-40f2-a204-6e0fa7978227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_samp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1690883396708,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "Rs8iu38MfOBK",
    "outputId": "d47cb749-ad20-4e71-926b-2d89af9ddb09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f248daf9d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X = torch.reshape(struct_samp_arr,(5000,28,28))\n",
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X[228].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR13nq4wiAqx"
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1690883126841,
     "user": {
      "displayName": "Indu Avinash",
      "userId": "17431670914384041031"
     },
     "user_tz": -330
    },
    "id": "lCDtpHRFb6VM",
    "outputId": "5c9e1676-d102-4246-ad82-6d81fbf6b78f"
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_samp_arr = struct_samp_arr.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Expert = 0\n",
      "1088\n",
      "Selected Expert = 1\n",
      "1109\n",
      "Selected Expert = 2\n",
      "1100\n",
      "Selected Expert = 3\n",
      "1064\n",
      "Average accuracy= 0.8722\n",
      "Time taken = 305.52913880348206\n"
     ]
    }
   ],
   "source": [
    "c_vec = np.zeros((4,1))\n",
    "tr_sum = 0\n",
    "for task_id in range(0,4):\n",
    "  Xtest1 = dataStream.labeledData[task_id].to(DEVICE)\n",
    "  Xtest2 = Xtest1.view(Xtest1.shape[0],inputdim)\n",
    " \n",
    "  ytest1 = dataStream.labeledLabel[task_id].to(DEVICE)\n",
    "  s_encoder = mnist_vae.Encoder(input_dim =inputdim , hidden_dim1=hiddendim1, hidden_dim2=hiddendim2,latent_dim=latent_dim[task_id])\n",
    "  s_decoder = mnist_vae.Decoder(latent_dim=latent_dim[task_id],hidden_dim1=hiddendim1,hidden_dim2=hiddendim2, output_dim=inputdim)\n",
    "  savd_model = mnist_vae.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "  savd_model.load_state_dict(torch.load('s_task'+str(task_id)+'_model'))\n",
    "  savd_model.eval()\n",
    "  Xt_out,ZXtest1,_ = savd_model(Xtest2)\n",
    "  for i in range(4):\n",
    "    c_vec[i] = torch.sum(cos(struct_samp_arr[1250*i:1250*(i+1)].view(1250,784), Xt_out)).cpu().detach().numpy()\n",
    "\n",
    "  arg_flg = np.argmax(c_vec)\n",
    "  print('Selected Expert =', arg_flg)\n",
    "\n",
    "  filename = 'Spl'+str(arg_flg)+'_clust_model.sav'\n",
    "#   print(filename)\n",
    "  kmn_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "  tst1_out = kmn_model.predict(ZXtest1.cpu().detach().numpy())\n",
    "  pred_lbls = data_lbls(tst1_out, task_clust_lbls[arg_flg])\n",
    "\n",
    "  tr_sum += np.trace(confusion_matrix(ytest1.cpu().detach().numpy(), pred_lbls))\n",
    "  print(np.trace(confusion_matrix(ytest1.cpu().detach().numpy(), pred_lbls)))\n",
    "end = time.time()\n",
    "tr_sum /= 5000\n",
    "print('Average accuracy=',tr_sum,)\n",
    "print('Time taken =', (end-start), )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "19URY6KMeAp6wS0qS3fw5KTNApG6Mg_s8",
     "timestamp": 1690473671033
    },
    {
     "file_id": "1oML2h2gW3TJdMq9JaW9p2JrcN-n94vOA",
     "timestamp": 1685962414267
    }
   ]
  },
  "kernelspec": {
   "display_name": "stam_env",
   "language": "python",
   "name": "stam_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

