{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/svhn/extra_32x32.mat\n",
      "Using downloaded and verified file: ./data/svhn/test_32x32.mat\n",
      "Number of output:  10\n",
      "Number of labeled data:  5000\n",
      "Number of unlabeled data:  114289\n",
      "Number of unlabeled data batch:  114\n",
      "Number of task:  5\n",
      "Number of labeled data per task:  1000\n",
      "Number of unlabeled data per task:  13000\n",
      "Number of unlabeled data batch per task:  13\n",
      "Number of unlabeled data test:  5000\n",
      "0\n",
      "\tEpoch 1 complete! \tTraining Loss:  2114.8912723214285\n",
      "\tEpoch 2 complete! \tTraining Loss:  2041.3572723214286\n",
      "\tEpoch 3 complete! \tTraining Loss:  2033.3232633928571\n",
      "\tEpoch 4 complete! \tTraining Loss:  2027.055185267857\n",
      "\tEpoch 5 complete! \tTraining Loss:  2014.3034776785714\n",
      "\tEpoch 6 complete! \tTraining Loss:  2003.8388325892856\n",
      "\tEpoch 7 complete! \tTraining Loss:  1993.8169263392856\n",
      "\tEpoch 8 complete! \tTraining Loss:  1989.3333191964286\n",
      "\tEpoch 9 complete! \tTraining Loss:  1983.7081763392857\n",
      "\tEpoch 10 complete! \tTraining Loss:  1978.6323883928571\n",
      "\tEpoch 11 complete! \tTraining Loss:  1975.6894598214285\n",
      "\tEpoch 12 complete! \tTraining Loss:  1972.8350982142856\n",
      "\tEpoch 13 complete! \tTraining Loss:  1970.8250870535715\n",
      "\tEpoch 14 complete! \tTraining Loss:  1970.6184642857143\n",
      "\tEpoch 15 complete! \tTraining Loss:  1967.8589598214285\n",
      "\tEpoch 16 complete! \tTraining Loss:  1966.2065602678572\n",
      "\tEpoch 17 complete! \tTraining Loss:  1962.529205357143\n",
      "\tEpoch 18 complete! \tTraining Loss:  1962.377232142857\n",
      "\tEpoch 19 complete! \tTraining Loss:  1962.1269174107142\n",
      "\tEpoch 20 complete! \tTraining Loss:  1958.9908169642856\n",
      "\tEpoch 21 complete! \tTraining Loss:  1958.1062767857143\n",
      "\tEpoch 22 complete! \tTraining Loss:  1956.97925\n",
      "\tEpoch 23 complete! \tTraining Loss:  1959.6035066964287\n",
      "\tEpoch 24 complete! \tTraining Loss:  1957.2975267857144\n",
      "\tEpoch 25 complete! \tTraining Loss:  1954.9418035714286\n",
      "\tEpoch 26 complete! \tTraining Loss:  1955.237796875\n",
      "\tEpoch 27 complete! \tTraining Loss:  1954.8996964285714\n",
      "\tEpoch 28 complete! \tTraining Loss:  1953.390299107143\n",
      "\tEpoch 29 complete! \tTraining Loss:  1952.20340625\n",
      "\tEpoch 30 complete! \tTraining Loss:  1951.8744151785713\n",
      "\tEpoch 31 complete! \tTraining Loss:  1950.9961540178572\n",
      "\tEpoch 32 complete! \tTraining Loss:  1951.7635133928572\n",
      "\tEpoch 33 complete! \tTraining Loss:  1948.6701071428572\n",
      "\tEpoch 34 complete! \tTraining Loss:  1949.0561071428572\n",
      "\tEpoch 35 complete! \tTraining Loss:  1948.225825892857\n",
      "\tEpoch 36 complete! \tTraining Loss:  1948.3214799107143\n",
      "\tEpoch 37 complete! \tTraining Loss:  1947.655044642857\n",
      "\tEpoch 38 complete! \tTraining Loss:  1946.491439732143\n",
      "\tEpoch 39 complete! \tTraining Loss:  1946.746892857143\n",
      "\tEpoch 40 complete! \tTraining Loss:  1946.143310267857\n",
      "\tEpoch 41 complete! \tTraining Loss:  1945.625078125\n",
      "\tEpoch 42 complete! \tTraining Loss:  1944.609408482143\n",
      "\tEpoch 43 complete! \tTraining Loss:  1945.8095892857143\n",
      "\tEpoch 44 complete! \tTraining Loss:  1944.1769709821428\n",
      "\tEpoch 45 complete! \tTraining Loss:  1944.2424084821428\n",
      "\tEpoch 46 complete! \tTraining Loss:  1943.8812142857143\n",
      "\tEpoch 47 complete! \tTraining Loss:  1943.5395513392857\n",
      "\tEpoch 48 complete! \tTraining Loss:  1943.9880825892858\n",
      "\tEpoch 49 complete! \tTraining Loss:  1943.382890625\n",
      "\tEpoch 50 complete! \tTraining Loss:  1942.4586026785714\n",
      "\tEpoch 51 complete! \tTraining Loss:  1942.285236607143\n",
      "\tEpoch 52 complete! \tTraining Loss:  1942.2357098214286\n",
      "\tEpoch 53 complete! \tTraining Loss:  1942.1882633928572\n",
      "\tEpoch 54 complete! \tTraining Loss:  1942.698203125\n",
      "\tEpoch 55 complete! \tTraining Loss:  1940.9318950892857\n",
      "\tEpoch 56 complete! \tTraining Loss:  1942.5671919642857\n",
      "\tEpoch 57 complete! \tTraining Loss:  1941.2130892857142\n",
      "\tEpoch 58 complete! \tTraining Loss:  1940.9218035714287\n",
      "\tEpoch 59 complete! \tTraining Loss:  1940.6759308035714\n",
      "\tEpoch 60 complete! \tTraining Loss:  1940.92965625\n",
      "\tEpoch 61 complete! \tTraining Loss:  1939.9635446428572\n",
      "\tEpoch 62 complete! \tTraining Loss:  1941.1370892857142\n",
      "\tEpoch 63 complete! \tTraining Loss:  1939.8431517857143\n",
      "\tEpoch 64 complete! \tTraining Loss:  1939.98759375\n",
      "\tEpoch 65 complete! \tTraining Loss:  1940.2276160714287\n",
      "\tEpoch 66 complete! \tTraining Loss:  1939.8773705357144\n",
      "\tEpoch 67 complete! \tTraining Loss:  1939.6551049107143\n",
      "\tEpoch 68 complete! \tTraining Loss:  1939.8496852678572\n",
      "\tEpoch 69 complete! \tTraining Loss:  1939.8250401785715\n",
      "\tEpoch 70 complete! \tTraining Loss:  1939.2786964285715\n",
      "\tEpoch 71 complete! \tTraining Loss:  1939.5422991071428\n",
      "\tEpoch 72 complete! \tTraining Loss:  1938.8944441964286\n",
      "\tEpoch 73 complete! \tTraining Loss:  1939.5021450892857\n",
      "\tEpoch 74 complete! \tTraining Loss:  1939.0113772321429\n",
      "\tEpoch 75 complete! \tTraining Loss:  1938.6700803571428\n",
      "\tEpoch 76 complete! \tTraining Loss:  1938.2388683035715\n",
      "\tEpoch 77 complete! \tTraining Loss:  1937.9646696428572\n",
      "\tEpoch 78 complete! \tTraining Loss:  1938.6892299107142\n",
      "\tEpoch 79 complete! \tTraining Loss:  1938.120470982143\n",
      "\tEpoch 80 complete! \tTraining Loss:  1937.6561808035715\n",
      "\tEpoch 81 complete! \tTraining Loss:  1938.0930089285714\n",
      "\tEpoch 82 complete! \tTraining Loss:  1937.5498459821429\n",
      "\tEpoch 83 complete! \tTraining Loss:  1938.0717566964286\n",
      "\tEpoch 84 complete! \tTraining Loss:  1937.5080959821428\n",
      "\tEpoch 85 complete! \tTraining Loss:  1937.3424464285715\n",
      "\tEpoch 86 complete! \tTraining Loss:  1937.3588883928571\n",
      "\tEpoch 87 complete! \tTraining Loss:  1936.9302745535715\n",
      "\tEpoch 88 complete! \tTraining Loss:  1937.3263816964286\n",
      "\tEpoch 89 complete! \tTraining Loss:  1936.8244040178572\n",
      "\tEpoch 90 complete! \tTraining Loss:  1936.3703727678571\n",
      "\tEpoch 91 complete! \tTraining Loss:  1936.5059196428572\n",
      "\tEpoch 92 complete! \tTraining Loss:  1936.7336026785715\n",
      "\tEpoch 93 complete! \tTraining Loss:  1935.835109375\n",
      "\tEpoch 94 complete! \tTraining Loss:  1936.1771339285715\n",
      "\tEpoch 95 complete! \tTraining Loss:  1936.31121875\n",
      "\tEpoch 96 complete! \tTraining Loss:  1936.2003638392857\n",
      "\tEpoch 97 complete! \tTraining Loss:  1935.843361607143\n",
      "\tEpoch 98 complete! \tTraining Loss:  1935.5981741071428\n",
      "\tEpoch 99 complete! \tTraining Loss:  1936.7488258928572\n",
      "\tEpoch 100 complete! \tTraining Loss:  1935.3100982142857\n",
      "\tEpoch 101 complete! \tTraining Loss:  1935.76978125\n",
      "\tEpoch 102 complete! \tTraining Loss:  1935.133607142857\n",
      "\tEpoch 103 complete! \tTraining Loss:  1936.4594419642858\n",
      "\tEpoch 104 complete! \tTraining Loss:  1935.33146875\n",
      "\tEpoch 105 complete! \tTraining Loss:  1935.4894441964286\n",
      "\tEpoch 106 complete! \tTraining Loss:  1935.0082142857143\n",
      "\tEpoch 107 complete! \tTraining Loss:  1935.5190870535714\n",
      "\tEpoch 108 complete! \tTraining Loss:  1935.2582232142856\n",
      "\tEpoch 109 complete! \tTraining Loss:  1935.0363370535715\n",
      "\tEpoch 110 complete! \tTraining Loss:  1935.1865401785715\n",
      "\tEpoch 111 complete! \tTraining Loss:  1935.0735111607144\n",
      "\tEpoch 112 complete! \tTraining Loss:  1934.8575513392857\n",
      "\tEpoch 113 complete! \tTraining Loss:  1935.054482142857\n",
      "\tEpoch 114 complete! \tTraining Loss:  1934.65196875\n",
      "\tEpoch 115 complete! \tTraining Loss:  1934.6798660714285\n",
      "\tEpoch 116 complete! \tTraining Loss:  1934.9916294642858\n",
      "\tEpoch 117 complete! \tTraining Loss:  1934.643875\n",
      "\tEpoch 118 complete! \tTraining Loss:  1934.9593571428572\n",
      "\tEpoch 119 complete! \tTraining Loss:  1934.6841450892857\n",
      "\tEpoch 120 complete! \tTraining Loss:  1934.3424308035715\n",
      "\tEpoch 121 complete! \tTraining Loss:  1934.6328839285713\n",
      "\tEpoch 122 complete! \tTraining Loss:  1934.1695379464286\n",
      "\tEpoch 123 complete! \tTraining Loss:  1934.6724308035714\n",
      "\tEpoch 124 complete! \tTraining Loss:  1934.569046875\n",
      "\tEpoch 125 complete! \tTraining Loss:  1934.2978973214285\n",
      "\tEpoch 126 complete! \tTraining Loss:  1933.8275089285714\n",
      "\tEpoch 127 complete! \tTraining Loss:  1933.9581473214287\n",
      "\tEpoch 128 complete! \tTraining Loss:  1934.1317723214286\n",
      "\tEpoch 129 complete! \tTraining Loss:  1933.8281495535714\n",
      "\tEpoch 130 complete! \tTraining Loss:  1934.227060267857\n",
      "\tEpoch 131 complete! \tTraining Loss:  1933.658109375\n",
      "\tEpoch 132 complete! \tTraining Loss:  1934.0981941964285\n",
      "\tEpoch 133 complete! \tTraining Loss:  1933.9398258928572\n",
      "\tEpoch 134 complete! \tTraining Loss:  1933.7460491071429\n",
      "\tEpoch 135 complete! \tTraining Loss:  1933.737064732143\n",
      "\tEpoch 136 complete! \tTraining Loss:  1933.3666316964286\n",
      "\tEpoch 137 complete! \tTraining Loss:  1933.6034107142857\n",
      "\tEpoch 138 complete! \tTraining Loss:  1933.6878370535715\n",
      "\tEpoch 139 complete! \tTraining Loss:  1933.1557299107144\n",
      "\tEpoch 140 complete! \tTraining Loss:  1932.7763928571428\n",
      "\tEpoch 141 complete! \tTraining Loss:  1933.82621875\n",
      "\tEpoch 142 complete! \tTraining Loss:  1932.9468102678572\n",
      "\tEpoch 143 complete! \tTraining Loss:  1932.9207522321428\n",
      "\tEpoch 144 complete! \tTraining Loss:  1932.7780982142858\n",
      "\tEpoch 145 complete! \tTraining Loss:  1933.3521808035714\n",
      "\tEpoch 146 complete! \tTraining Loss:  1932.91225\n",
      "\tEpoch 147 complete! \tTraining Loss:  1932.9838950892856\n",
      "\tEpoch 148 complete! \tTraining Loss:  1933.023294642857\n",
      "\tEpoch 149 complete! \tTraining Loss:  1933.3160647321429\n",
      "\tEpoch 150 complete! \tTraining Loss:  1932.7978816964285\n",
      "\tEpoch 151 complete! \tTraining Loss:  1932.5924330357143\n",
      "\tEpoch 152 complete! \tTraining Loss:  1932.5039129464285\n",
      "\tEpoch 153 complete! \tTraining Loss:  1932.843138392857\n",
      "\tEpoch 154 complete! \tTraining Loss:  1932.6130558035713\n",
      "\tEpoch 155 complete! \tTraining Loss:  1932.641470982143\n",
      "\tEpoch 156 complete! \tTraining Loss:  1932.045515625\n",
      "\tEpoch 157 complete! \tTraining Loss:  1932.2028727678571\n",
      "\tEpoch 158 complete! \tTraining Loss:  1932.7053660714287\n",
      "\tEpoch 159 complete! \tTraining Loss:  1932.2\n",
      "\tEpoch 160 complete! \tTraining Loss:  1932.3574620535715\n",
      "\tEpoch 161 complete! \tTraining Loss:  1932.2480267857143\n",
      "\tEpoch 162 complete! \tTraining Loss:  1932.3229776785715\n",
      "\tEpoch 163 complete! \tTraining Loss:  1931.8924955357143\n",
      "\tEpoch 164 complete! \tTraining Loss:  1932.4920892857142\n",
      "\tEpoch 165 complete! \tTraining Loss:  1931.9787991071428\n",
      "\tEpoch 166 complete! \tTraining Loss:  1931.8219375\n",
      "\tEpoch 167 complete! \tTraining Loss:  1931.9240513392858\n",
      "\tEpoch 168 complete! \tTraining Loss:  1931.9609397321428\n",
      "\tEpoch 169 complete! \tTraining Loss:  1932.1020535714285\n",
      "\tEpoch 170 complete! \tTraining Loss:  1931.897546875\n",
      "\tEpoch 171 complete! \tTraining Loss:  1931.9083214285715\n",
      "\tEpoch 172 complete! \tTraining Loss:  1931.7776830357143\n",
      "\tEpoch 173 complete! \tTraining Loss:  1931.5982991071428\n",
      "\tEpoch 174 complete! \tTraining Loss:  1931.7654285714286\n",
      "\tEpoch 175 complete! \tTraining Loss:  1931.5370334821428\n",
      "\tEpoch 176 complete! \tTraining Loss:  1931.6299486607143\n",
      "\tEpoch 177 complete! \tTraining Loss:  1932.2535825892858\n",
      "\tEpoch 178 complete! \tTraining Loss:  1931.315955357143\n",
      "\tEpoch 179 complete! \tTraining Loss:  1931.76971875\n",
      "\tEpoch 180 complete! \tTraining Loss:  1931.331359375\n",
      "\tEpoch 181 complete! \tTraining Loss:  1931.4881138392857\n",
      "\tEpoch 182 complete! \tTraining Loss:  1931.6983973214285\n",
      "\tEpoch 183 complete! \tTraining Loss:  1931.587859375\n",
      "\tEpoch 184 complete! \tTraining Loss:  1931.4302745535715\n",
      "\tEpoch 185 complete! \tTraining Loss:  1931.5390892857142\n",
      "\tEpoch 186 complete! \tTraining Loss:  1931.4853928571429\n",
      "\tEpoch 187 complete! \tTraining Loss:  1931.5993258928572\n",
      "\tEpoch 188 complete! \tTraining Loss:  1930.7806495535715\n",
      "\tEpoch 189 complete! \tTraining Loss:  1931.4364799107143\n",
      "\tEpoch 190 complete! \tTraining Loss:  1931.2221138392856\n",
      "\tEpoch 191 complete! \tTraining Loss:  1931.4038950892857\n",
      "\tEpoch 192 complete! \tTraining Loss:  1930.9590290178571\n",
      "\tEpoch 193 complete! \tTraining Loss:  1931.4208816964285\n",
      "\tEpoch 194 complete! \tTraining Loss:  1931.0722075892857\n",
      "\tEpoch 195 complete! \tTraining Loss:  1930.8800424107144\n",
      "\tEpoch 196 complete! \tTraining Loss:  1930.9288727678572\n",
      "\tEpoch 197 complete! \tTraining Loss:  1931.1499754464285\n",
      "\tEpoch 198 complete! \tTraining Loss:  1931.4659419642858\n",
      "\tEpoch 199 complete! \tTraining Loss:  1930.8657589285715\n",
      "\tEpoch 200 complete! \tTraining Loss:  1931.0761294642857\n",
      "\tEpoch 201 complete! \tTraining Loss:  1931.4635669642857\n",
      "\tEpoch 202 complete! \tTraining Loss:  1930.8131830357142\n",
      "\tEpoch 203 complete! \tTraining Loss:  1930.6480379464285\n",
      "\tEpoch 204 complete! \tTraining Loss:  1930.9019263392856\n",
      "\tEpoch 205 complete! \tTraining Loss:  1931.216125\n",
      "\tEpoch 206 complete! \tTraining Loss:  1930.551580357143\n",
      "\tEpoch 207 complete! \tTraining Loss:  1930.469658482143\n",
      "\tEpoch 208 complete! \tTraining Loss:  1930.8385267857143\n",
      "\tEpoch 209 complete! \tTraining Loss:  1930.9311741071429\n",
      "\tEpoch 210 complete! \tTraining Loss:  1930.5450223214286\n",
      "\tEpoch 211 complete! \tTraining Loss:  1931.086267857143\n",
      "\tEpoch 212 complete! \tTraining Loss:  1930.7832566964287\n",
      "\tEpoch 213 complete! \tTraining Loss:  1930.9505357142857\n",
      "\tEpoch 214 complete! \tTraining Loss:  1930.5528928571428\n",
      "\tEpoch 215 complete! \tTraining Loss:  1930.6973370535713\n",
      "\tEpoch 216 complete! \tTraining Loss:  1930.5831629464285\n",
      "\tEpoch 217 complete! \tTraining Loss:  1930.7209910714287\n",
      "\tEpoch 218 complete! \tTraining Loss:  1930.5209888392858\n",
      "\tEpoch 219 complete! \tTraining Loss:  1930.467299107143\n",
      "\tEpoch 220 complete! \tTraining Loss:  1930.3284419642857\n",
      "\tEpoch 221 complete! \tTraining Loss:  1930.279388392857\n",
      "\tEpoch 222 complete! \tTraining Loss:  1930.2171607142857\n",
      "\tEpoch 223 complete! \tTraining Loss:  1930.3682678571429\n",
      "\tEpoch 224 complete! \tTraining Loss:  1930.465872767857\n",
      "\tEpoch 225 complete! \tTraining Loss:  1930.2001138392857\n",
      "\tEpoch 226 complete! \tTraining Loss:  1929.9659419642858\n",
      "\tEpoch 227 complete! \tTraining Loss:  1930.04\n",
      "\tEpoch 228 complete! \tTraining Loss:  1930.2914285714285\n",
      "\tEpoch 229 complete! \tTraining Loss:  1930.1273169642857\n",
      "\tEpoch 230 complete! \tTraining Loss:  1930.530857142857\n",
      "\tEpoch 231 complete! \tTraining Loss:  1930.3220669642858\n",
      "\tEpoch 232 complete! \tTraining Loss:  1930.1644151785715\n",
      "\tEpoch 233 complete! \tTraining Loss:  1930.4079620535715\n",
      "\tEpoch 234 complete! \tTraining Loss:  1930.416265625\n",
      "\tEpoch 235 complete! \tTraining Loss:  1930.2024241071429\n",
      "\tEpoch 236 complete! \tTraining Loss:  1930.0833861607143\n",
      "\tEpoch 237 complete! \tTraining Loss:  1930.0402455357143\n",
      "\tEpoch 238 complete! \tTraining Loss:  1930.0862321428572\n",
      "\tEpoch 239 complete! \tTraining Loss:  1930.0506875\n",
      "\tEpoch 240 complete! \tTraining Loss:  1930.0495290178571\n",
      "\tEpoch 241 complete! \tTraining Loss:  1930.8678973214285\n",
      "\tEpoch 242 complete! \tTraining Loss:  1930.045125\n",
      "\tEpoch 243 complete! \tTraining Loss:  1930.2030691964285\n",
      "\tEpoch 244 complete! \tTraining Loss:  1930.028375\n",
      "\tEpoch 245 complete! \tTraining Loss:  1930.3717879464286\n",
      "\tEpoch 246 complete! \tTraining Loss:  1930.0388035714286\n",
      "\tEpoch 247 complete! \tTraining Loss:  1929.9763816964285\n",
      "\tEpoch 248 complete! \tTraining Loss:  1930.18621875\n",
      "\tEpoch 249 complete! \tTraining Loss:  1929.8700558035714\n",
      "\tEpoch 250 complete! \tTraining Loss:  1929.5948683035715\n",
      "\tEpoch 251 complete! \tTraining Loss:  1929.59790625\n",
      "\tEpoch 252 complete! \tTraining Loss:  1929.9984330357142\n",
      "\tEpoch 253 complete! \tTraining Loss:  1929.55596875\n",
      "\tEpoch 254 complete! \tTraining Loss:  1929.886752232143\n",
      "\tEpoch 255 complete! \tTraining Loss:  1929.7657120535714\n",
      "\tEpoch 256 complete! \tTraining Loss:  1929.8670513392858\n",
      "\tEpoch 257 complete! \tTraining Loss:  1929.669546875\n",
      "\tEpoch 258 complete! \tTraining Loss:  1929.8129799107144\n",
      "\tEpoch 259 complete! \tTraining Loss:  1929.842529017857\n",
      "\tEpoch 260 complete! \tTraining Loss:  1929.7531517857142\n",
      "\tEpoch 261 complete! \tTraining Loss:  1929.6210959821428\n",
      "\tEpoch 262 complete! \tTraining Loss:  1929.657625\n",
      "\tEpoch 263 complete! \tTraining Loss:  1929.2805982142856\n",
      "\tEpoch 264 complete! \tTraining Loss:  1929.7171763392857\n",
      "\tEpoch 265 complete! \tTraining Loss:  1929.4991584821428\n",
      "\tEpoch 266 complete! \tTraining Loss:  1929.2068705357142\n",
      "\tEpoch 267 complete! \tTraining Loss:  1929.8617232142858\n",
      "\tEpoch 268 complete! \tTraining Loss:  1929.51134375\n",
      "\tEpoch 269 complete! \tTraining Loss:  1929.4959910714285\n",
      "\tEpoch 270 complete! \tTraining Loss:  1929.5919888392857\n",
      "\tEpoch 271 complete! \tTraining Loss:  1929.4574486607144\n",
      "\tEpoch 272 complete! \tTraining Loss:  1929.5561294642857\n",
      "\tEpoch 273 complete! \tTraining Loss:  1929.6491919642858\n",
      "\tEpoch 274 complete! \tTraining Loss:  1929.5248526785715\n",
      "\tEpoch 275 complete! \tTraining Loss:  1929.264825892857\n",
      "\tEpoch 276 complete! \tTraining Loss:  1929.3075580357142\n",
      "\tEpoch 277 complete! \tTraining Loss:  1929.3255825892857\n",
      "\tEpoch 278 complete! \tTraining Loss:  1929.424109375\n",
      "\tEpoch 279 complete! \tTraining Loss:  1929.6502924107142\n",
      "\tEpoch 280 complete! \tTraining Loss:  1929.684859375\n",
      "\tEpoch 281 complete! \tTraining Loss:  1929.646158482143\n",
      "\tEpoch 282 complete! \tTraining Loss:  1929.330765625\n",
      "\tEpoch 283 complete! \tTraining Loss:  1929.3358303571429\n",
      "\tEpoch 284 complete! \tTraining Loss:  1929.4236026785713\n",
      "\tEpoch 285 complete! \tTraining Loss:  1929.1860267857144\n",
      "\tEpoch 286 complete! \tTraining Loss:  1929.4702455357142\n",
      "\tEpoch 287 complete! \tTraining Loss:  1929.1032053571428\n",
      "\tEpoch 288 complete! \tTraining Loss:  1929.02025\n",
      "\tEpoch 289 complete! \tTraining Loss:  1929.6101964285715\n",
      "\tEpoch 290 complete! \tTraining Loss:  1929.31240625\n",
      "\tEpoch 291 complete! \tTraining Loss:  1929.225\n",
      "\tEpoch 292 complete! \tTraining Loss:  1929.5112611607142\n",
      "\tEpoch 293 complete! \tTraining Loss:  1929.3047410714287\n",
      "\tEpoch 294 complete! \tTraining Loss:  1929.1767232142856\n",
      "\tEpoch 295 complete! \tTraining Loss:  1929.2510379464286\n",
      "\tEpoch 296 complete! \tTraining Loss:  1929.0941808035714\n",
      "\tEpoch 297 complete! \tTraining Loss:  1929.177830357143\n",
      "\tEpoch 298 complete! \tTraining Loss:  1928.931875\n",
      "\tEpoch 299 complete! \tTraining Loss:  1929.434205357143\n",
      "\tEpoch 300 complete! \tTraining Loss:  1929.1388660714285\n",
      "\tEpoch 301 complete! \tTraining Loss:  1929.31825\n",
      "\tEpoch 302 complete! \tTraining Loss:  1929.1875223214286\n",
      "\tEpoch 303 complete! \tTraining Loss:  1929.0929419642857\n",
      "\tEpoch 304 complete! \tTraining Loss:  1929.4399174107143\n",
      "\tEpoch 305 complete! \tTraining Loss:  1928.6711696428572\n",
      "\tEpoch 306 complete! \tTraining Loss:  1928.862265625\n",
      "\tEpoch 307 complete! \tTraining Loss:  1928.9906785714286\n",
      "\tEpoch 308 complete! \tTraining Loss:  1928.8087053571428\n",
      "\tEpoch 309 complete! \tTraining Loss:  1929.1339910714287\n",
      "\tEpoch 310 complete! \tTraining Loss:  1929.1351517857142\n",
      "\tEpoch 311 complete! \tTraining Loss:  1928.843783482143\n",
      "\tEpoch 312 complete! \tTraining Loss:  1928.9387700892858\n",
      "\tEpoch 313 complete! \tTraining Loss:  1928.9095267857142\n",
      "\tEpoch 314 complete! \tTraining Loss:  1928.8601964285715\n",
      "\tEpoch 315 complete! \tTraining Loss:  1928.624453125\n",
      "\tEpoch 316 complete! \tTraining Loss:  1928.753171875\n",
      "\tEpoch 317 complete! \tTraining Loss:  1928.8030044642858\n",
      "\tEpoch 318 complete! \tTraining Loss:  1928.9805736607143\n",
      "\tEpoch 319 complete! \tTraining Loss:  1928.701857142857\n",
      "\tEpoch 320 complete! \tTraining Loss:  1928.4806049107142\n",
      "\tEpoch 321 complete! \tTraining Loss:  1928.8000200892857\n",
      "\tEpoch 322 complete! \tTraining Loss:  1928.8514263392858\n",
      "\tEpoch 323 complete! \tTraining Loss:  1929.1486919642857\n",
      "\tEpoch 324 complete! \tTraining Loss:  1928.7451160714286\n",
      "\tEpoch 325 complete! \tTraining Loss:  1928.7198303571429\n",
      "\tEpoch 326 complete! \tTraining Loss:  1928.6471339285715\n",
      "\tEpoch 327 complete! \tTraining Loss:  1928.7328303571428\n",
      "\tEpoch 328 complete! \tTraining Loss:  1928.544515625\n",
      "\tEpoch 329 complete! \tTraining Loss:  1928.6265825892858\n",
      "\tEpoch 330 complete! \tTraining Loss:  1928.9610133928572\n",
      "\tEpoch 331 complete! \tTraining Loss:  1928.564875\n",
      "\tEpoch 332 complete! \tTraining Loss:  1928.7701808035715\n",
      "\tEpoch 333 complete! \tTraining Loss:  1928.6696741071428\n",
      "\tEpoch 334 complete! \tTraining Loss:  1928.4090825892856\n",
      "\tEpoch 335 complete! \tTraining Loss:  1928.7859620535714\n",
      "\tEpoch 336 complete! \tTraining Loss:  1928.810203125\n",
      "\tEpoch 337 complete! \tTraining Loss:  1928.461734375\n",
      "\tEpoch 338 complete! \tTraining Loss:  1928.8733660714286\n",
      "\tEpoch 339 complete! \tTraining Loss:  1928.6220736607142\n",
      "\tEpoch 340 complete! \tTraining Loss:  1928.7985357142857\n",
      "\tEpoch 341 complete! \tTraining Loss:  1928.241560267857\n",
      "\tEpoch 342 complete! \tTraining Loss:  1928.7407299107142\n",
      "\tEpoch 343 complete! \tTraining Loss:  1928.4291428571428\n",
      "\tEpoch 344 complete! \tTraining Loss:  1928.4313816964286\n",
      "\tEpoch 345 complete! \tTraining Loss:  1928.306658482143\n",
      "\tEpoch 346 complete! \tTraining Loss:  1928.658419642857\n",
      "\tEpoch 347 complete! \tTraining Loss:  1928.3433348214285\n",
      "\tEpoch 348 complete! \tTraining Loss:  1928.4841160714286\n",
      "\tEpoch 349 complete! \tTraining Loss:  1928.6241919642857\n",
      "\tEpoch 350 complete! \tTraining Loss:  1928.4327633928572\n",
      "torch.Size([10000, 3072])\n",
      "1\n",
      "\tEpoch 1 complete! \tTraining Loss:  1943.7825679824562\n",
      "\tEpoch 2 complete! \tTraining Loss:  1941.8508530701754\n",
      "\tEpoch 3 complete! \tTraining Loss:  1941.4991951754387\n",
      "\tEpoch 4 complete! \tTraining Loss:  1940.4924934210526\n",
      "\tEpoch 5 complete! \tTraining Loss:  1940.3442719298246\n",
      "\tEpoch 6 complete! \tTraining Loss:  1940.0667390350877\n",
      "\tEpoch 7 complete! \tTraining Loss:  1939.7181798245615\n",
      "\tEpoch 8 complete! \tTraining Loss:  1939.6673004385964\n",
      "\tEpoch 9 complete! \tTraining Loss:  1939.2674342105263\n",
      "\tEpoch 10 complete! \tTraining Loss:  1939.1232587719298\n",
      "\tEpoch 11 complete! \tTraining Loss:  1939.0376074561404\n",
      "\tEpoch 12 complete! \tTraining Loss:  1939.2387872807017\n",
      "\tEpoch 13 complete! \tTraining Loss:  1938.5899605263157\n",
      "\tEpoch 14 complete! \tTraining Loss:  1938.557677631579\n",
      "\tEpoch 15 complete! \tTraining Loss:  1938.457600877193\n",
      "\tEpoch 16 complete! \tTraining Loss:  1938.1242872807018\n",
      "\tEpoch 17 complete! \tTraining Loss:  1938.1584912280703\n",
      "\tEpoch 18 complete! \tTraining Loss:  1938.1878223684212\n",
      "\tEpoch 19 complete! \tTraining Loss:  1938.352024122807\n",
      "\tEpoch 20 complete! \tTraining Loss:  1937.9358881578946\n",
      "\tEpoch 21 complete! \tTraining Loss:  1937.9042587719298\n",
      "\tEpoch 22 complete! \tTraining Loss:  1938.0157719298245\n",
      "\tEpoch 23 complete! \tTraining Loss:  1937.580072368421\n",
      "\tEpoch 24 complete! \tTraining Loss:  1937.7356842105264\n",
      "\tEpoch 25 complete! \tTraining Loss:  1937.4090043859649\n",
      "\tEpoch 26 complete! \tTraining Loss:  1937.3986074561403\n",
      "\tEpoch 27 complete! \tTraining Loss:  1937.3470592105264\n",
      "\tEpoch 28 complete! \tTraining Loss:  1937.5063157894738\n",
      "\tEpoch 29 complete! \tTraining Loss:  1937.5807324561404\n",
      "\tEpoch 30 complete! \tTraining Loss:  1937.4049671052633\n",
      "\tEpoch 31 complete! \tTraining Loss:  1937.2128399122807\n",
      "\tEpoch 32 complete! \tTraining Loss:  1937.066052631579\n",
      "\tEpoch 33 complete! \tTraining Loss:  1936.9431600877192\n",
      "\tEpoch 34 complete! \tTraining Loss:  1936.9763399122808\n",
      "\tEpoch 35 complete! \tTraining Loss:  1936.857701754386\n",
      "\tEpoch 36 complete! \tTraining Loss:  1936.9108179824561\n",
      "\tEpoch 37 complete! \tTraining Loss:  1936.710975877193\n",
      "\tEpoch 38 complete! \tTraining Loss:  1936.5763114035087\n",
      "\tEpoch 39 complete! \tTraining Loss:  1936.9125745614035\n",
      "\tEpoch 40 complete! \tTraining Loss:  1936.7439385964913\n",
      "\tEpoch 41 complete! \tTraining Loss:  1936.6776469298245\n",
      "\tEpoch 42 complete! \tTraining Loss:  1936.6069013157894\n",
      "\tEpoch 43 complete! \tTraining Loss:  1936.5585328947368\n",
      "\tEpoch 44 complete! \tTraining Loss:  1936.645980263158\n",
      "\tEpoch 45 complete! \tTraining Loss:  1936.6346184210527\n",
      "\tEpoch 46 complete! \tTraining Loss:  1936.3419780701754\n",
      "\tEpoch 47 complete! \tTraining Loss:  1936.1646315789474\n",
      "\tEpoch 48 complete! \tTraining Loss:  1936.2311710526317\n",
      "\tEpoch 49 complete! \tTraining Loss:  1936.028182017544\n",
      "\tEpoch 50 complete! \tTraining Loss:  1936.2229166666666\n",
      "\tEpoch 51 complete! \tTraining Loss:  1936.1207521929825\n",
      "\tEpoch 52 complete! \tTraining Loss:  1936.1243728070176\n",
      "\tEpoch 53 complete! \tTraining Loss:  1935.889379385965\n",
      "\tEpoch 54 complete! \tTraining Loss:  1935.9718925438597\n",
      "\tEpoch 55 complete! \tTraining Loss:  1936.068278508772\n",
      "\tEpoch 56 complete! \tTraining Loss:  1935.8649978070175\n",
      "\tEpoch 57 complete! \tTraining Loss:  1935.8147324561403\n",
      "\tEpoch 58 complete! \tTraining Loss:  1935.9062916666667\n",
      "\tEpoch 59 complete! \tTraining Loss:  1935.973278508772\n",
      "\tEpoch 60 complete! \tTraining Loss:  1936.049543859649\n",
      "\tEpoch 61 complete! \tTraining Loss:  1936.0793289473684\n",
      "\tEpoch 62 complete! \tTraining Loss:  1935.6387280701754\n",
      "\tEpoch 63 complete! \tTraining Loss:  1935.821322368421\n",
      "\tEpoch 64 complete! \tTraining Loss:  1935.358125\n",
      "\tEpoch 65 complete! \tTraining Loss:  1935.9448157894738\n",
      "\tEpoch 66 complete! \tTraining Loss:  1935.718754385965\n",
      "\tEpoch 67 complete! \tTraining Loss:  1935.780451754386\n",
      "\tEpoch 68 complete! \tTraining Loss:  1935.5515986842106\n",
      "\tEpoch 69 complete! \tTraining Loss:  1935.6206403508772\n",
      "\tEpoch 70 complete! \tTraining Loss:  1935.5624561403508\n",
      "\tEpoch 71 complete! \tTraining Loss:  1935.5889013157894\n",
      "\tEpoch 72 complete! \tTraining Loss:  1935.5854539473685\n",
      "\tEpoch 73 complete! \tTraining Loss:  1935.406370614035\n",
      "\tEpoch 74 complete! \tTraining Loss:  1935.3175065789474\n",
      "\tEpoch 75 complete! \tTraining Loss:  1935.2046381578948\n",
      "\tEpoch 76 complete! \tTraining Loss:  1935.3935416666666\n",
      "\tEpoch 77 complete! \tTraining Loss:  1935.5289627192983\n",
      "\tEpoch 78 complete! \tTraining Loss:  1935.3439473684211\n",
      "\tEpoch 79 complete! \tTraining Loss:  1935.2718859649124\n",
      "\tEpoch 80 complete! \tTraining Loss:  1935.2232521929825\n",
      "\tEpoch 81 complete! \tTraining Loss:  1935.482100877193\n",
      "\tEpoch 82 complete! \tTraining Loss:  1935.0562280701754\n",
      "\tEpoch 83 complete! \tTraining Loss:  1935.1495657894736\n",
      "\tEpoch 84 complete! \tTraining Loss:  1935.2387938596491\n",
      "\tEpoch 85 complete! \tTraining Loss:  1935.153120614035\n",
      "\tEpoch 86 complete! \tTraining Loss:  1935.147831140351\n",
      "\tEpoch 87 complete! \tTraining Loss:  1935.0844561403508\n",
      "\tEpoch 88 complete! \tTraining Loss:  1934.956230263158\n",
      "\tEpoch 89 complete! \tTraining Loss:  1935.02425877193\n",
      "\tEpoch 90 complete! \tTraining Loss:  1934.9534583333334\n",
      "\tEpoch 91 complete! \tTraining Loss:  1934.9283004385966\n",
      "\tEpoch 92 complete! \tTraining Loss:  1934.940085526316\n",
      "\tEpoch 93 complete! \tTraining Loss:  1935.112807017544\n",
      "\tEpoch 94 complete! \tTraining Loss:  1934.9193969298246\n",
      "\tEpoch 95 complete! \tTraining Loss:  1934.8133815789474\n",
      "\tEpoch 96 complete! \tTraining Loss:  1934.647\n",
      "\tEpoch 97 complete! \tTraining Loss:  1934.9082083333333\n",
      "\tEpoch 98 complete! \tTraining Loss:  1934.9268969298246\n",
      "\tEpoch 99 complete! \tTraining Loss:  1934.7276557017544\n",
      "\tEpoch 100 complete! \tTraining Loss:  1934.7015899122807\n",
      "\tEpoch 101 complete! \tTraining Loss:  1934.6548399122807\n",
      "\tEpoch 102 complete! \tTraining Loss:  1934.8082061403509\n",
      "\tEpoch 103 complete! \tTraining Loss:  1934.6028245614036\n",
      "\tEpoch 104 complete! \tTraining Loss:  1934.6900789473684\n",
      "\tEpoch 105 complete! \tTraining Loss:  1934.7712390350878\n",
      "\tEpoch 106 complete! \tTraining Loss:  1934.4769868421054\n",
      "\tEpoch 107 complete! \tTraining Loss:  1934.559640350877\n",
      "\tEpoch 108 complete! \tTraining Loss:  1934.8933530701754\n",
      "\tEpoch 109 complete! \tTraining Loss:  1934.5538223684212\n",
      "\tEpoch 110 complete! \tTraining Loss:  1934.553024122807\n",
      "\tEpoch 111 complete! \tTraining Loss:  1934.385125\n",
      "\tEpoch 112 complete! \tTraining Loss:  1934.7396907894738\n",
      "\tEpoch 113 complete! \tTraining Loss:  1934.0905592105264\n",
      "\tEpoch 114 complete! \tTraining Loss:  1934.2237565789474\n",
      "\tEpoch 115 complete! \tTraining Loss:  1934.510394736842\n",
      "\tEpoch 116 complete! \tTraining Loss:  1934.4609122807017\n",
      "\tEpoch 117 complete! \tTraining Loss:  1934.495379385965\n",
      "\tEpoch 118 complete! \tTraining Loss:  1934.4634254385965\n",
      "\tEpoch 119 complete! \tTraining Loss:  1934.6468618421052\n",
      "\tEpoch 120 complete! \tTraining Loss:  1934.117600877193\n",
      "\tEpoch 121 complete! \tTraining Loss:  1934.1155219298246\n",
      "\tEpoch 122 complete! \tTraining Loss:  1934.2341644736841\n",
      "\tEpoch 123 complete! \tTraining Loss:  1934.2464780701755\n",
      "\tEpoch 124 complete! \tTraining Loss:  1934.1865307017545\n",
      "\tEpoch 125 complete! \tTraining Loss:  1934.1380263157894\n",
      "\tEpoch 126 complete! \tTraining Loss:  1934.2744013157894\n",
      "\tEpoch 127 complete! \tTraining Loss:  1934.382076754386\n",
      "\tEpoch 128 complete! \tTraining Loss:  1934.0268135964911\n",
      "\tEpoch 129 complete! \tTraining Loss:  1934.2797280701755\n",
      "\tEpoch 130 complete! \tTraining Loss:  1934.0728530701754\n",
      "\tEpoch 131 complete! \tTraining Loss:  1934.1351600877192\n",
      "\tEpoch 132 complete! \tTraining Loss:  1934.3371315789473\n",
      "\tEpoch 133 complete! \tTraining Loss:  1934.4433859649123\n",
      "\tEpoch 134 complete! \tTraining Loss:  1934.0903442982456\n",
      "\tEpoch 135 complete! \tTraining Loss:  1934.3120263157896\n",
      "\tEpoch 136 complete! \tTraining Loss:  1933.885173245614\n",
      "\tEpoch 137 complete! \tTraining Loss:  1934.0310526315789\n",
      "\tEpoch 138 complete! \tTraining Loss:  1934.1034385964913\n",
      "\tEpoch 139 complete! \tTraining Loss:  1933.9734846491228\n",
      "\tEpoch 140 complete! \tTraining Loss:  1934.1423114035088\n",
      "\tEpoch 141 complete! \tTraining Loss:  1933.9080175438596\n",
      "\tEpoch 142 complete! \tTraining Loss:  1933.795346491228\n",
      "\tEpoch 143 complete! \tTraining Loss:  1933.9231710526317\n",
      "\tEpoch 144 complete! \tTraining Loss:  1933.871842105263\n",
      "\tEpoch 145 complete! \tTraining Loss:  1934.0619342105263\n",
      "\tEpoch 146 complete! \tTraining Loss:  1933.7200328947367\n",
      "\tEpoch 147 complete! \tTraining Loss:  1934.1471600877194\n",
      "\tEpoch 148 complete! \tTraining Loss:  1934.079956140351\n",
      "\tEpoch 149 complete! \tTraining Loss:  1933.7670614035087\n",
      "\tEpoch 150 complete! \tTraining Loss:  1933.8743486842104\n",
      "\tEpoch 151 complete! \tTraining Loss:  1933.7460548245615\n",
      "\tEpoch 152 complete! \tTraining Loss:  1933.6868925438596\n",
      "\tEpoch 153 complete! \tTraining Loss:  1934.0667850877194\n",
      "\tEpoch 154 complete! \tTraining Loss:  1933.6769956140351\n",
      "\tEpoch 155 complete! \tTraining Loss:  1933.805504385965\n",
      "\tEpoch 156 complete! \tTraining Loss:  1933.8390460526316\n",
      "\tEpoch 157 complete! \tTraining Loss:  1933.8318048245615\n",
      "\tEpoch 158 complete! \tTraining Loss:  1933.855274122807\n",
      "\tEpoch 159 complete! \tTraining Loss:  1933.4816140350877\n",
      "\tEpoch 160 complete! \tTraining Loss:  1933.7318157894738\n",
      "\tEpoch 161 complete! \tTraining Loss:  1933.7607828947368\n",
      "\tEpoch 162 complete! \tTraining Loss:  1933.6513377192982\n",
      "\tEpoch 163 complete! \tTraining Loss:  1933.7213771929823\n",
      "\tEpoch 164 complete! \tTraining Loss:  1933.7018355263158\n",
      "\tEpoch 165 complete! \tTraining Loss:  1933.6146798245613\n",
      "\tEpoch 166 complete! \tTraining Loss:  1933.5400219298247\n",
      "\tEpoch 167 complete! \tTraining Loss:  1933.712875\n",
      "\tEpoch 168 complete! \tTraining Loss:  1933.6036776315789\n",
      "\tEpoch 169 complete! \tTraining Loss:  1933.6060877192983\n",
      "\tEpoch 170 complete! \tTraining Loss:  1933.467201754386\n",
      "\tEpoch 171 complete! \tTraining Loss:  1933.5054188596491\n",
      "\tEpoch 172 complete! \tTraining Loss:  1933.6708859649123\n",
      "\tEpoch 173 complete! \tTraining Loss:  1933.761149122807\n",
      "\tEpoch 174 complete! \tTraining Loss:  1933.417467105263\n",
      "\tEpoch 175 complete! \tTraining Loss:  1933.5402478070175\n",
      "\tEpoch 176 complete! \tTraining Loss:  1933.4681074561404\n",
      "\tEpoch 177 complete! \tTraining Loss:  1933.3332894736843\n",
      "\tEpoch 178 complete! \tTraining Loss:  1933.6102565789474\n",
      "\tEpoch 179 complete! \tTraining Loss:  1933.5199846491228\n",
      "\tEpoch 180 complete! \tTraining Loss:  1933.4789144736842\n",
      "\tEpoch 181 complete! \tTraining Loss:  1933.5117324561404\n",
      "\tEpoch 182 complete! \tTraining Loss:  1933.1916885964913\n",
      "\tEpoch 183 complete! \tTraining Loss:  1933.567581140351\n",
      "\tEpoch 184 complete! \tTraining Loss:  1933.3906842105264\n",
      "\tEpoch 185 complete! \tTraining Loss:  1933.4664780701755\n",
      "\tEpoch 186 complete! \tTraining Loss:  1933.4466578947367\n",
      "\tEpoch 187 complete! \tTraining Loss:  1933.2943728070175\n",
      "\tEpoch 188 complete! \tTraining Loss:  1933.4551557017544\n",
      "\tEpoch 189 complete! \tTraining Loss:  1933.082754385965\n",
      "\tEpoch 190 complete! \tTraining Loss:  1933.3421842105263\n",
      "\tEpoch 191 complete! \tTraining Loss:  1933.3531513157895\n",
      "\tEpoch 192 complete! \tTraining Loss:  1933.3972653508772\n",
      "\tEpoch 193 complete! \tTraining Loss:  1933.2209100877194\n",
      "\tEpoch 194 complete! \tTraining Loss:  1933.236\n",
      "\tEpoch 195 complete! \tTraining Loss:  1933.3157960526316\n",
      "\tEpoch 196 complete! \tTraining Loss:  1933.173855263158\n",
      "\tEpoch 197 complete! \tTraining Loss:  1933.281971491228\n",
      "\tEpoch 198 complete! \tTraining Loss:  1933.187370614035\n",
      "\tEpoch 199 complete! \tTraining Loss:  1933.2424342105264\n",
      "\tEpoch 200 complete! \tTraining Loss:  1933.171721491228\n",
      "\tEpoch 201 complete! \tTraining Loss:  1933.1191381578947\n",
      "\tEpoch 202 complete! \tTraining Loss:  1933.2152609649122\n",
      "\tEpoch 203 complete! \tTraining Loss:  1932.9128881578947\n",
      "\tEpoch 204 complete! \tTraining Loss:  1933.1914692982457\n",
      "\tEpoch 205 complete! \tTraining Loss:  1933.1530153508772\n",
      "\tEpoch 206 complete! \tTraining Loss:  1933.1277785087718\n",
      "\tEpoch 207 complete! \tTraining Loss:  1933.2345438596492\n",
      "\tEpoch 208 complete! \tTraining Loss:  1933.0309539473685\n",
      "\tEpoch 209 complete! \tTraining Loss:  1933.1664342105264\n",
      "\tEpoch 210 complete! \tTraining Loss:  1933.0469057017544\n",
      "\tEpoch 211 complete! \tTraining Loss:  1933.1828048245613\n",
      "\tEpoch 212 complete! \tTraining Loss:  1933.1765328947367\n",
      "\tEpoch 213 complete! \tTraining Loss:  1933.254346491228\n",
      "\tEpoch 214 complete! \tTraining Loss:  1933.109576754386\n",
      "\tEpoch 215 complete! \tTraining Loss:  1933.0074057017544\n",
      "\tEpoch 216 complete! \tTraining Loss:  1932.9824802631579\n",
      "\tEpoch 217 complete! \tTraining Loss:  1933.0077434210527\n",
      "\tEpoch 218 complete! \tTraining Loss:  1932.7489583333333\n",
      "\tEpoch 219 complete! \tTraining Loss:  1932.8868070175438\n",
      "\tEpoch 220 complete! \tTraining Loss:  1933.0674210526315\n",
      "\tEpoch 221 complete! \tTraining Loss:  1933.0093793859648\n",
      "\tEpoch 222 complete! \tTraining Loss:  1933.005149122807\n",
      "\tEpoch 223 complete! \tTraining Loss:  1932.8434561403508\n",
      "\tEpoch 224 complete! \tTraining Loss:  1933.065548245614\n",
      "\tEpoch 225 complete! \tTraining Loss:  1932.8699692982457\n",
      "\tEpoch 226 complete! \tTraining Loss:  1932.9192280701754\n",
      "\tEpoch 227 complete! \tTraining Loss:  1932.895427631579\n",
      "\tEpoch 228 complete! \tTraining Loss:  1932.7321864035089\n",
      "\tEpoch 229 complete! \tTraining Loss:  1932.927918859649\n",
      "\tEpoch 230 complete! \tTraining Loss:  1932.7117697368421\n",
      "\tEpoch 231 complete! \tTraining Loss:  1933.0532675438596\n",
      "\tEpoch 232 complete! \tTraining Loss:  1932.8162807017543\n",
      "\tEpoch 233 complete! \tTraining Loss:  1932.883423245614\n",
      "\tEpoch 234 complete! \tTraining Loss:  1932.8519013157895\n",
      "\tEpoch 235 complete! \tTraining Loss:  1932.7351447368421\n",
      "\tEpoch 236 complete! \tTraining Loss:  1932.8508157894737\n",
      "\tEpoch 237 complete! \tTraining Loss:  1932.7162083333333\n",
      "\tEpoch 238 complete! \tTraining Loss:  1932.667346491228\n",
      "\tEpoch 239 complete! \tTraining Loss:  1932.8004671052631\n",
      "\tEpoch 240 complete! \tTraining Loss:  1933.0690372807017\n",
      "\tEpoch 241 complete! \tTraining Loss:  1932.6527478070175\n",
      "\tEpoch 242 complete! \tTraining Loss:  1932.8261929824562\n",
      "\tEpoch 243 complete! \tTraining Loss:  1932.6730899122806\n",
      "\tEpoch 244 complete! \tTraining Loss:  1932.5856447368421\n",
      "\tEpoch 245 complete! \tTraining Loss:  1932.742067982456\n",
      "\tEpoch 246 complete! \tTraining Loss:  1932.7569671052631\n",
      "\tEpoch 247 complete! \tTraining Loss:  1932.575697368421\n",
      "\tEpoch 248 complete! \tTraining Loss:  1932.7687324561402\n",
      "\tEpoch 249 complete! \tTraining Loss:  1932.8653333333334\n",
      "\tEpoch 250 complete! \tTraining Loss:  1932.885100877193\n",
      "\tEpoch 251 complete! \tTraining Loss:  1932.5226381578948\n",
      "\tEpoch 252 complete! \tTraining Loss:  1932.6117960526317\n",
      "\tEpoch 253 complete! \tTraining Loss:  1932.537971491228\n",
      "\tEpoch 254 complete! \tTraining Loss:  1932.5802872807017\n",
      "\tEpoch 255 complete! \tTraining Loss:  1932.4796074561405\n",
      "\tEpoch 256 complete! \tTraining Loss:  1932.3484824561403\n",
      "\tEpoch 257 complete! \tTraining Loss:  1932.75638377193\n",
      "\tEpoch 258 complete! \tTraining Loss:  1932.4364978070175\n",
      "\tEpoch 259 complete! \tTraining Loss:  1932.6350219298245\n",
      "\tEpoch 260 complete! \tTraining Loss:  1932.4220657894737\n",
      "\tEpoch 261 complete! \tTraining Loss:  1932.4929627192982\n",
      "\tEpoch 262 complete! \tTraining Loss:  1932.4295482456141\n",
      "\tEpoch 263 complete! \tTraining Loss:  1932.5174583333333\n",
      "\tEpoch 264 complete! \tTraining Loss:  1932.3061096491228\n",
      "\tEpoch 265 complete! \tTraining Loss:  1932.552899122807\n",
      "\tEpoch 266 complete! \tTraining Loss:  1932.7408048245613\n",
      "\tEpoch 267 complete! \tTraining Loss:  1932.4768399122806\n",
      "\tEpoch 268 complete! \tTraining Loss:  1932.4302412280701\n",
      "\tEpoch 269 complete! \tTraining Loss:  1932.2684934210527\n",
      "\tEpoch 270 complete! \tTraining Loss:  1932.5619495614035\n",
      "\tEpoch 271 complete! \tTraining Loss:  1932.2882521929826\n",
      "\tEpoch 272 complete! \tTraining Loss:  1932.6670921052632\n",
      "\tEpoch 273 complete! \tTraining Loss:  1932.2947149122806\n",
      "\tEpoch 274 complete! \tTraining Loss:  1932.4950548245613\n",
      "\tEpoch 275 complete! \tTraining Loss:  1932.3983596491228\n",
      "\tEpoch 276 complete! \tTraining Loss:  1932.437532894737\n",
      "\tEpoch 277 complete! \tTraining Loss:  1932.3545833333333\n",
      "\tEpoch 278 complete! \tTraining Loss:  1932.2059868421052\n",
      "\tEpoch 279 complete! \tTraining Loss:  1932.1450131578947\n",
      "\tEpoch 280 complete! \tTraining Loss:  1932.2637872807018\n",
      "\tEpoch 281 complete! \tTraining Loss:  1932.4838157894737\n",
      "\tEpoch 282 complete! \tTraining Loss:  1932.249484649123\n",
      "\tEpoch 283 complete! \tTraining Loss:  1932.0956228070174\n",
      "\tEpoch 284 complete! \tTraining Loss:  1932.5643728070174\n",
      "\tEpoch 285 complete! \tTraining Loss:  1932.2388070175439\n",
      "\tEpoch 286 complete! \tTraining Loss:  1932.1508618421053\n",
      "\tEpoch 287 complete! \tTraining Loss:  1932.2660153508773\n",
      "\tEpoch 288 complete! \tTraining Loss:  1932.1312631578946\n",
      "\tEpoch 289 complete! \tTraining Loss:  1932.3276754385965\n",
      "\tEpoch 290 complete! \tTraining Loss:  1932.2636951754387\n",
      "\tEpoch 291 complete! \tTraining Loss:  1932.0435745614036\n",
      "\tEpoch 292 complete! \tTraining Loss:  1932.0841578947368\n",
      "\tEpoch 293 complete! \tTraining Loss:  1932.500581140351\n",
      "\tEpoch 294 complete! \tTraining Loss:  1932.1594934210527\n",
      "\tEpoch 295 complete! \tTraining Loss:  1932.2259649122807\n",
      "\tEpoch 296 complete! \tTraining Loss:  1932.1904692982457\n",
      "\tEpoch 297 complete! \tTraining Loss:  1932.1152828947368\n",
      "\tEpoch 298 complete! \tTraining Loss:  1932.024572368421\n",
      "\tEpoch 299 complete! \tTraining Loss:  1932.0719539473685\n",
      "\tEpoch 300 complete! \tTraining Loss:  1932.3012850877194\n",
      "\tEpoch 301 complete! \tTraining Loss:  1932.1191074561405\n",
      "\tEpoch 302 complete! \tTraining Loss:  1932.0796403508773\n",
      "\tEpoch 303 complete! \tTraining Loss:  1932.201331140351\n",
      "\tEpoch 304 complete! \tTraining Loss:  1932.113265350877\n",
      "\tEpoch 305 complete! \tTraining Loss:  1932.0858026315789\n",
      "\tEpoch 306 complete! \tTraining Loss:  1932.1250657894736\n",
      "\tEpoch 307 complete! \tTraining Loss:  1932.2050307017544\n",
      "\tEpoch 308 complete! \tTraining Loss:  1931.9502368421054\n",
      "\tEpoch 309 complete! \tTraining Loss:  1931.8184210526315\n",
      "\tEpoch 310 complete! \tTraining Loss:  1932.01075\n",
      "\tEpoch 311 complete! \tTraining Loss:  1931.9736666666668\n",
      "\tEpoch 312 complete! \tTraining Loss:  1931.9179100877193\n",
      "\tEpoch 313 complete! \tTraining Loss:  1932.1743004385964\n",
      "\tEpoch 314 complete! \tTraining Loss:  1932.3478662280702\n",
      "\tEpoch 315 complete! \tTraining Loss:  1932.049254385965\n",
      "\tEpoch 316 complete! \tTraining Loss:  1931.848653508772\n",
      "\tEpoch 317 complete! \tTraining Loss:  1931.9336425438596\n",
      "\tEpoch 318 complete! \tTraining Loss:  1932.0951951754387\n",
      "\tEpoch 319 complete! \tTraining Loss:  1931.9605021929824\n",
      "\tEpoch 320 complete! \tTraining Loss:  1931.9441842105264\n",
      "\tEpoch 321 complete! \tTraining Loss:  1932.0001644736842\n",
      "\tEpoch 322 complete! \tTraining Loss:  1931.827245614035\n",
      "\tEpoch 323 complete! \tTraining Loss:  1931.9445548245615\n",
      "\tEpoch 324 complete! \tTraining Loss:  1932.1669342105263\n",
      "\tEpoch 325 complete! \tTraining Loss:  1932.1760657894738\n",
      "\tEpoch 326 complete! \tTraining Loss:  1931.823177631579\n",
      "\tEpoch 327 complete! \tTraining Loss:  1932.043471491228\n",
      "\tEpoch 328 complete! \tTraining Loss:  1931.9182894736841\n",
      "\tEpoch 329 complete! \tTraining Loss:  1931.9717763157894\n",
      "\tEpoch 330 complete! \tTraining Loss:  1931.7994210526315\n",
      "\tEpoch 331 complete! \tTraining Loss:  1931.8000657894736\n",
      "\tEpoch 332 complete! \tTraining Loss:  1931.768100877193\n",
      "\tEpoch 333 complete! \tTraining Loss:  1931.6792521929824\n",
      "\tEpoch 334 complete! \tTraining Loss:  1931.6794100877194\n",
      "\tEpoch 335 complete! \tTraining Loss:  1931.9076359649123\n",
      "\tEpoch 336 complete! \tTraining Loss:  1931.7471951754385\n",
      "\tEpoch 337 complete! \tTraining Loss:  1931.8550811403509\n",
      "\tEpoch 338 complete! \tTraining Loss:  1931.7886907894738\n",
      "\tEpoch 339 complete! \tTraining Loss:  1931.651717105263\n",
      "\tEpoch 340 complete! \tTraining Loss:  1931.825076754386\n",
      "\tEpoch 341 complete! \tTraining Loss:  1931.7010657894737\n",
      "\tEpoch 342 complete! \tTraining Loss:  1931.7426820175438\n",
      "\tEpoch 343 complete! \tTraining Loss:  1931.8322828947369\n",
      "\tEpoch 344 complete! \tTraining Loss:  1931.7059429824562\n",
      "\tEpoch 345 complete! \tTraining Loss:  1931.7746030701755\n",
      "\tEpoch 346 complete! \tTraining Loss:  1931.6892456140351\n",
      "\tEpoch 347 complete! \tTraining Loss:  1931.7343662280703\n",
      "\tEpoch 348 complete! \tTraining Loss:  1931.3951228070175\n",
      "\tEpoch 349 complete! \tTraining Loss:  1931.5694210526315\n",
      "\tEpoch 350 complete! \tTraining Loss:  1931.7372719298246\n",
      "torch.Size([10000, 3072])\n",
      "2\n",
      "\tEpoch 1 complete! \tTraining Loss:  1955.5767530487806\n",
      "\tEpoch 2 complete! \tTraining Loss:  1952.9425914634146\n",
      "\tEpoch 3 complete! \tTraining Loss:  1952.2691585365853\n",
      "\tEpoch 4 complete! \tTraining Loss:  1951.6352621951219\n",
      "\tEpoch 5 complete! \tTraining Loss:  1951.163768292683\n",
      "\tEpoch 6 complete! \tTraining Loss:  1950.815963414634\n",
      "\tEpoch 7 complete! \tTraining Loss:  1950.7924664634147\n",
      "\tEpoch 8 complete! \tTraining Loss:  1950.4186951219513\n",
      "\tEpoch 9 complete! \tTraining Loss:  1950.6594298780487\n",
      "\tEpoch 10 complete! \tTraining Loss:  1950.2492560975609\n",
      "\tEpoch 11 complete! \tTraining Loss:  1949.8081341463414\n",
      "\tEpoch 12 complete! \tTraining Loss:  1950.0426280487804\n",
      "\tEpoch 13 complete! \tTraining Loss:  1949.8304847560976\n",
      "\tEpoch 14 complete! \tTraining Loss:  1949.5958536585365\n",
      "\tEpoch 15 complete! \tTraining Loss:  1949.6600487804878\n",
      "\tEpoch 16 complete! \tTraining Loss:  1949.3435548780487\n",
      "\tEpoch 17 complete! \tTraining Loss:  1949.2127743902438\n",
      "\tEpoch 18 complete! \tTraining Loss:  1949.0801615853659\n",
      "\tEpoch 19 complete! \tTraining Loss:  1948.8868018292683\n",
      "\tEpoch 20 complete! \tTraining Loss:  1949.010006097561\n",
      "\tEpoch 21 complete! \tTraining Loss:  1949.0575548780487\n",
      "\tEpoch 22 complete! \tTraining Loss:  1948.7917469512195\n",
      "\tEpoch 23 complete! \tTraining Loss:  1948.7587530487806\n",
      "\tEpoch 24 complete! \tTraining Loss:  1948.4461646341463\n",
      "\tEpoch 25 complete! \tTraining Loss:  1948.9175152439025\n",
      "\tEpoch 26 complete! \tTraining Loss:  1948.4495152439024\n",
      "\tEpoch 27 complete! \tTraining Loss:  1948.258018292683\n",
      "\tEpoch 28 complete! \tTraining Loss:  1948.8052835365854\n",
      "\tEpoch 29 complete! \tTraining Loss:  1948.4107774390243\n",
      "\tEpoch 30 complete! \tTraining Loss:  1948.2266341463414\n",
      "\tEpoch 31 complete! \tTraining Loss:  1948.2568079268292\n",
      "\tEpoch 32 complete! \tTraining Loss:  1948.249792682927\n",
      "\tEpoch 33 complete! \tTraining Loss:  1948.2751920731707\n",
      "\tEpoch 34 complete! \tTraining Loss:  1948.1953567073172\n",
      "\tEpoch 35 complete! \tTraining Loss:  1947.7060853658536\n",
      "\tEpoch 36 complete! \tTraining Loss:  1947.879100609756\n",
      "\tEpoch 37 complete! \tTraining Loss:  1947.8780914634146\n",
      "\tEpoch 38 complete! \tTraining Loss:  1948.166280487805\n",
      "\tEpoch 39 complete! \tTraining Loss:  1947.8056981707316\n",
      "\tEpoch 40 complete! \tTraining Loss:  1947.8276676829269\n",
      "\tEpoch 41 complete! \tTraining Loss:  1948.0222378048782\n",
      "\tEpoch 42 complete! \tTraining Loss:  1947.7872652439025\n",
      "\tEpoch 43 complete! \tTraining Loss:  1947.9061067073171\n",
      "\tEpoch 44 complete! \tTraining Loss:  1947.5275030487805\n",
      "\tEpoch 45 complete! \tTraining Loss:  1947.520524390244\n",
      "\tEpoch 46 complete! \tTraining Loss:  1947.9509725609755\n",
      "\tEpoch 47 complete! \tTraining Loss:  1947.476387195122\n",
      "\tEpoch 48 complete! \tTraining Loss:  1947.6292743902438\n",
      "\tEpoch 49 complete! \tTraining Loss:  1947.2855396341463\n",
      "\tEpoch 50 complete! \tTraining Loss:  1947.5058048780488\n",
      "\tEpoch 51 complete! \tTraining Loss:  1947.5539329268292\n",
      "\tEpoch 52 complete! \tTraining Loss:  1947.4647835365854\n",
      "\tEpoch 53 complete! \tTraining Loss:  1947.1711097560976\n",
      "\tEpoch 54 complete! \tTraining Loss:  1947.5493780487805\n",
      "\tEpoch 55 complete! \tTraining Loss:  1947.136481707317\n",
      "\tEpoch 56 complete! \tTraining Loss:  1947.4463140243902\n",
      "\tEpoch 57 complete! \tTraining Loss:  1947.4169146341465\n",
      "\tEpoch 58 complete! \tTraining Loss:  1947.1186219512194\n",
      "\tEpoch 59 complete! \tTraining Loss:  1947.2486128048781\n",
      "\tEpoch 60 complete! \tTraining Loss:  1946.9139024390245\n",
      "\tEpoch 61 complete! \tTraining Loss:  1947.034625\n",
      "\tEpoch 62 complete! \tTraining Loss:  1946.9939054878048\n",
      "\tEpoch 63 complete! \tTraining Loss:  1947.1507713414635\n",
      "\tEpoch 64 complete! \tTraining Loss:  1946.9505640243901\n",
      "\tEpoch 65 complete! \tTraining Loss:  1947.058887195122\n",
      "\tEpoch 66 complete! \tTraining Loss:  1946.7912012195122\n",
      "\tEpoch 67 complete! \tTraining Loss:  1946.7731493902438\n",
      "\tEpoch 68 complete! \tTraining Loss:  1946.918292682927\n",
      "\tEpoch 69 complete! \tTraining Loss:  1947.1185792682927\n",
      "\tEpoch 70 complete! \tTraining Loss:  1946.9414725609756\n",
      "\tEpoch 71 complete! \tTraining Loss:  1946.7234603658537\n",
      "\tEpoch 72 complete! \tTraining Loss:  1946.9465335365853\n",
      "\tEpoch 73 complete! \tTraining Loss:  1946.9043109756099\n",
      "\tEpoch 74 complete! \tTraining Loss:  1946.9681341463415\n",
      "\tEpoch 75 complete! \tTraining Loss:  1946.6426737804877\n",
      "\tEpoch 76 complete! \tTraining Loss:  1946.8613841463414\n",
      "\tEpoch 77 complete! \tTraining Loss:  1946.9329420731708\n",
      "\tEpoch 78 complete! \tTraining Loss:  1946.6633414634146\n",
      "\tEpoch 79 complete! \tTraining Loss:  1946.7038719512195\n",
      "\tEpoch 80 complete! \tTraining Loss:  1946.601899390244\n",
      "\tEpoch 81 complete! \tTraining Loss:  1946.3832865853658\n",
      "\tEpoch 82 complete! \tTraining Loss:  1946.733006097561\n",
      "\tEpoch 83 complete! \tTraining Loss:  1946.5556646341463\n",
      "\tEpoch 84 complete! \tTraining Loss:  1946.6009695121952\n",
      "\tEpoch 85 complete! \tTraining Loss:  1946.5986829268293\n",
      "\tEpoch 86 complete! \tTraining Loss:  1946.5801798780487\n",
      "\tEpoch 87 complete! \tTraining Loss:  1946.4648475609756\n",
      "\tEpoch 88 complete! \tTraining Loss:  1946.5196676829269\n",
      "\tEpoch 89 complete! \tTraining Loss:  1946.3939390243902\n",
      "\tEpoch 90 complete! \tTraining Loss:  1946.2412713414635\n",
      "\tEpoch 91 complete! \tTraining Loss:  1946.5784268292682\n",
      "\tEpoch 92 complete! \tTraining Loss:  1946.2970640243902\n",
      "\tEpoch 93 complete! \tTraining Loss:  1946.791625\n",
      "\tEpoch 94 complete! \tTraining Loss:  1946.469631097561\n",
      "\tEpoch 95 complete! \tTraining Loss:  1946.4628048780487\n",
      "\tEpoch 96 complete! \tTraining Loss:  1946.400375\n",
      "\tEpoch 97 complete! \tTraining Loss:  1946.1499237804878\n",
      "\tEpoch 98 complete! \tTraining Loss:  1946.2173780487806\n",
      "\tEpoch 99 complete! \tTraining Loss:  1946.4337896341463\n",
      "\tEpoch 100 complete! \tTraining Loss:  1946.0952042682927\n",
      "\tEpoch 101 complete! \tTraining Loss:  1946.549042682927\n",
      "\tEpoch 102 complete! \tTraining Loss:  1946.1850396341463\n",
      "\tEpoch 103 complete! \tTraining Loss:  1946.0005365853658\n",
      "\tEpoch 104 complete! \tTraining Loss:  1946.2495914634146\n",
      "\tEpoch 105 complete! \tTraining Loss:  1946.4285396341463\n",
      "\tEpoch 106 complete! \tTraining Loss:  1946.2980243902439\n",
      "\tEpoch 107 complete! \tTraining Loss:  1946.0160701219513\n",
      "\tEpoch 108 complete! \tTraining Loss:  1945.9801280487804\n",
      "\tEpoch 109 complete! \tTraining Loss:  1946.3098048780487\n",
      "\tEpoch 110 complete! \tTraining Loss:  1945.593231707317\n",
      "\tEpoch 111 complete! \tTraining Loss:  1945.991451219512\n",
      "\tEpoch 112 complete! \tTraining Loss:  1945.7327103658536\n",
      "\tEpoch 113 complete! \tTraining Loss:  1945.9608079268294\n",
      "\tEpoch 114 complete! \tTraining Loss:  1946.076231707317\n",
      "\tEpoch 115 complete! \tTraining Loss:  1945.9112530487805\n",
      "\tEpoch 116 complete! \tTraining Loss:  1945.7292835365854\n",
      "\tEpoch 117 complete! \tTraining Loss:  1945.6728567073171\n",
      "\tEpoch 118 complete! \tTraining Loss:  1945.7963231707317\n",
      "\tEpoch 119 complete! \tTraining Loss:  1945.9100091463415\n",
      "\tEpoch 120 complete! \tTraining Loss:  1945.6034664634146\n",
      "\tEpoch 121 complete! \tTraining Loss:  1946.0335518292684\n",
      "\tEpoch 122 complete! \tTraining Loss:  1945.819506097561\n",
      "\tEpoch 123 complete! \tTraining Loss:  1946.2284054878048\n",
      "\tEpoch 124 complete! \tTraining Loss:  1945.763225609756\n",
      "\tEpoch 125 complete! \tTraining Loss:  1946.0057926829268\n",
      "\tEpoch 126 complete! \tTraining Loss:  1945.728018292683\n",
      "\tEpoch 127 complete! \tTraining Loss:  1945.929137195122\n",
      "\tEpoch 128 complete! \tTraining Loss:  1945.7289298780488\n",
      "\tEpoch 129 complete! \tTraining Loss:  1945.5730457317072\n",
      "\tEpoch 130 complete! \tTraining Loss:  1945.6690304878048\n",
      "\tEpoch 131 complete! \tTraining Loss:  1945.6233963414634\n",
      "\tEpoch 132 complete! \tTraining Loss:  1945.8090213414634\n",
      "\tEpoch 133 complete! \tTraining Loss:  1945.7131402439024\n",
      "\tEpoch 134 complete! \tTraining Loss:  1945.706655487805\n",
      "\tEpoch 135 complete! \tTraining Loss:  1945.4867317073172\n",
      "\tEpoch 136 complete! \tTraining Loss:  1945.6653201219513\n",
      "\tEpoch 137 complete! \tTraining Loss:  1945.5591402439024\n",
      "\tEpoch 138 complete! \tTraining Loss:  1945.431606707317\n",
      "\tEpoch 139 complete! \tTraining Loss:  1945.7615548780489\n",
      "\tEpoch 140 complete! \tTraining Loss:  1945.6559359756097\n",
      "\tEpoch 141 complete! \tTraining Loss:  1945.7001676829268\n",
      "\tEpoch 142 complete! \tTraining Loss:  1945.361012195122\n",
      "\tEpoch 143 complete! \tTraining Loss:  1945.5975\n",
      "\tEpoch 144 complete! \tTraining Loss:  1945.5405731707317\n",
      "\tEpoch 145 complete! \tTraining Loss:  1945.4267347560976\n",
      "\tEpoch 146 complete! \tTraining Loss:  1945.037725609756\n",
      "\tEpoch 147 complete! \tTraining Loss:  1945.5433567073171\n",
      "\tEpoch 148 complete! \tTraining Loss:  1945.5741890243903\n",
      "\tEpoch 149 complete! \tTraining Loss:  1945.711512195122\n",
      "\tEpoch 150 complete! \tTraining Loss:  1945.689243902439\n",
      "\tEpoch 151 complete! \tTraining Loss:  1945.3104237804878\n",
      "\tEpoch 152 complete! \tTraining Loss:  1945.362481707317\n",
      "\tEpoch 153 complete! \tTraining Loss:  1945.5065914634147\n",
      "\tEpoch 154 complete! \tTraining Loss:  1945.3641280487805\n",
      "\tEpoch 155 complete! \tTraining Loss:  1945.1681920731708\n",
      "\tEpoch 156 complete! \tTraining Loss:  1945.2740884146342\n",
      "\tEpoch 157 complete! \tTraining Loss:  1945.3940274390243\n",
      "\tEpoch 158 complete! \tTraining Loss:  1945.5240030487805\n",
      "\tEpoch 159 complete! \tTraining Loss:  1945.485143292683\n",
      "\tEpoch 160 complete! \tTraining Loss:  1945.2378932926829\n",
      "\tEpoch 161 complete! \tTraining Loss:  1945.3911158536584\n",
      "\tEpoch 162 complete! \tTraining Loss:  1945.4027164634147\n",
      "\tEpoch 163 complete! \tTraining Loss:  1945.006868902439\n",
      "\tEpoch 164 complete! \tTraining Loss:  1945.2121280487804\n",
      "\tEpoch 165 complete! \tTraining Loss:  1945.2157225609756\n",
      "\tEpoch 166 complete! \tTraining Loss:  1945.2466493902439\n",
      "\tEpoch 167 complete! \tTraining Loss:  1945.2778719512196\n",
      "\tEpoch 168 complete! \tTraining Loss:  1944.952600609756\n",
      "\tEpoch 169 complete! \tTraining Loss:  1945.065838414634\n",
      "\tEpoch 170 complete! \tTraining Loss:  1945.2082865853658\n",
      "\tEpoch 171 complete! \tTraining Loss:  1944.941393292683\n",
      "\tEpoch 172 complete! \tTraining Loss:  1944.9759085365854\n",
      "\tEpoch 173 complete! \tTraining Loss:  1945.3513170731708\n",
      "\tEpoch 174 complete! \tTraining Loss:  1945.096256097561\n",
      "\tEpoch 175 complete! \tTraining Loss:  1945.0205670731707\n",
      "\tEpoch 176 complete! \tTraining Loss:  1945.104548780488\n",
      "\tEpoch 177 complete! \tTraining Loss:  1945.1845823170731\n",
      "\tEpoch 178 complete! \tTraining Loss:  1944.7742286585367\n",
      "\tEpoch 179 complete! \tTraining Loss:  1944.923487804878\n",
      "\tEpoch 180 complete! \tTraining Loss:  1945.2760914634146\n",
      "\tEpoch 181 complete! \tTraining Loss:  1944.6550670731708\n",
      "\tEpoch 182 complete! \tTraining Loss:  1945.0164085365855\n",
      "\tEpoch 183 complete! \tTraining Loss:  1944.7836036585365\n",
      "\tEpoch 184 complete! \tTraining Loss:  1945.218012195122\n",
      "\tEpoch 185 complete! \tTraining Loss:  1944.9770213414633\n",
      "\tEpoch 186 complete! \tTraining Loss:  1944.864911585366\n",
      "\tEpoch 187 complete! \tTraining Loss:  1945.1537012195122\n",
      "\tEpoch 188 complete! \tTraining Loss:  1945.0025609756099\n",
      "\tEpoch 189 complete! \tTraining Loss:  1944.6866768292682\n",
      "\tEpoch 190 complete! \tTraining Loss:  1945.0796310975609\n",
      "\tEpoch 191 complete! \tTraining Loss:  1944.7221981707316\n",
      "\tEpoch 192 complete! \tTraining Loss:  1944.9943841463414\n",
      "\tEpoch 193 complete! \tTraining Loss:  1944.7341951219512\n",
      "\tEpoch 194 complete! \tTraining Loss:  1944.774417682927\n",
      "\tEpoch 195 complete! \tTraining Loss:  1944.6033597560975\n",
      "\tEpoch 196 complete! \tTraining Loss:  1944.900512195122\n",
      "\tEpoch 197 complete! \tTraining Loss:  1944.611542682927\n",
      "\tEpoch 198 complete! \tTraining Loss:  1944.808631097561\n",
      "\tEpoch 199 complete! \tTraining Loss:  1944.8436036585365\n",
      "\tEpoch 200 complete! \tTraining Loss:  1945.0140426829269\n",
      "\tEpoch 201 complete! \tTraining Loss:  1945.0236920731707\n",
      "\tEpoch 202 complete! \tTraining Loss:  1944.5200182926828\n",
      "\tEpoch 203 complete! \tTraining Loss:  1944.808762195122\n",
      "\tEpoch 204 complete! \tTraining Loss:  1944.7796798780487\n",
      "\tEpoch 205 complete! \tTraining Loss:  1944.5862347560976\n",
      "\tEpoch 206 complete! \tTraining Loss:  1944.7147073170731\n",
      "\tEpoch 207 complete! \tTraining Loss:  1945.0032347560975\n",
      "\tEpoch 208 complete! \tTraining Loss:  1944.8383231707317\n",
      "\tEpoch 209 complete! \tTraining Loss:  1944.798673780488\n",
      "\tEpoch 210 complete! \tTraining Loss:  1944.3133201219512\n",
      "\tEpoch 211 complete! \tTraining Loss:  1944.7832073170732\n",
      "\tEpoch 212 complete! \tTraining Loss:  1944.590137195122\n",
      "\tEpoch 213 complete! \tTraining Loss:  1944.4395274390245\n",
      "\tEpoch 214 complete! \tTraining Loss:  1944.4620548780488\n",
      "\tEpoch 215 complete! \tTraining Loss:  1944.857881097561\n",
      "\tEpoch 216 complete! \tTraining Loss:  1944.7296402439024\n",
      "\tEpoch 217 complete! \tTraining Loss:  1944.5972164634147\n",
      "\tEpoch 218 complete! \tTraining Loss:  1944.3925914634146\n",
      "\tEpoch 219 complete! \tTraining Loss:  1944.7419451219512\n",
      "\tEpoch 220 complete! \tTraining Loss:  1944.6181097560975\n",
      "\tEpoch 221 complete! \tTraining Loss:  1944.8486585365854\n",
      "\tEpoch 222 complete! \tTraining Loss:  1944.6300030487805\n",
      "\tEpoch 223 complete! \tTraining Loss:  1944.6452195121951\n",
      "\tEpoch 224 complete! \tTraining Loss:  1944.429149390244\n",
      "\tEpoch 225 complete! \tTraining Loss:  1944.2873170731707\n",
      "\tEpoch 226 complete! \tTraining Loss:  1944.774530487805\n",
      "\tEpoch 227 complete! \tTraining Loss:  1944.5497042682928\n",
      "\tEpoch 228 complete! \tTraining Loss:  1944.5810304878048\n",
      "\tEpoch 229 complete! \tTraining Loss:  1944.4634542682927\n",
      "\tEpoch 230 complete! \tTraining Loss:  1944.4899847560976\n",
      "\tEpoch 231 complete! \tTraining Loss:  1944.603143292683\n",
      "\tEpoch 232 complete! \tTraining Loss:  1944.4484573170732\n",
      "\tEpoch 233 complete! \tTraining Loss:  1944.5184054878048\n",
      "\tEpoch 234 complete! \tTraining Loss:  1944.492737804878\n",
      "\tEpoch 235 complete! \tTraining Loss:  1944.219993902439\n",
      "\tEpoch 236 complete! \tTraining Loss:  1944.3418445121952\n",
      "\tEpoch 237 complete! \tTraining Loss:  1944.669106707317\n",
      "\tEpoch 238 complete! \tTraining Loss:  1944.2514329268292\n",
      "\tEpoch 239 complete! \tTraining Loss:  1944.3324237804877\n",
      "\tEpoch 240 complete! \tTraining Loss:  1944.3194237804878\n",
      "\tEpoch 241 complete! \tTraining Loss:  1944.4929268292683\n",
      "\tEpoch 242 complete! \tTraining Loss:  1944.316637195122\n",
      "\tEpoch 243 complete! \tTraining Loss:  1944.0794481707317\n",
      "\tEpoch 244 complete! \tTraining Loss:  1944.2945243902439\n",
      "\tEpoch 245 complete! \tTraining Loss:  1944.4351280487806\n",
      "\tEpoch 246 complete! \tTraining Loss:  1944.3204664634147\n",
      "\tEpoch 247 complete! \tTraining Loss:  1944.294262195122\n",
      "\tEpoch 248 complete! \tTraining Loss:  1944.7482103658538\n",
      "\tEpoch 249 complete! \tTraining Loss:  1944.3378201219512\n",
      "\tEpoch 250 complete! \tTraining Loss:  1944.616625\n",
      "\tEpoch 251 complete! \tTraining Loss:  1944.3772408536586\n",
      "\tEpoch 252 complete! \tTraining Loss:  1944.3157682926828\n",
      "\tEpoch 253 complete! \tTraining Loss:  1944.1122042682928\n",
      "\tEpoch 254 complete! \tTraining Loss:  1944.4970457317074\n",
      "\tEpoch 255 complete! \tTraining Loss:  1944.1447012195122\n",
      "\tEpoch 256 complete! \tTraining Loss:  1944.4500396341464\n",
      "\tEpoch 257 complete! \tTraining Loss:  1944.3161829268292\n",
      "\tEpoch 258 complete! \tTraining Loss:  1943.9698323170733\n",
      "\tEpoch 259 complete! \tTraining Loss:  1944.0921829268293\n",
      "\tEpoch 260 complete! \tTraining Loss:  1944.5072804878048\n",
      "\tEpoch 261 complete! \tTraining Loss:  1944.3410487804879\n",
      "\tEpoch 262 complete! \tTraining Loss:  1944.2830091463416\n",
      "\tEpoch 263 complete! \tTraining Loss:  1944.3385975609756\n",
      "\tEpoch 264 complete! \tTraining Loss:  1944.0816006097562\n",
      "\tEpoch 265 complete! \tTraining Loss:  1944.2576585365853\n",
      "\tEpoch 266 complete! \tTraining Loss:  1944.1731067073172\n",
      "\tEpoch 267 complete! \tTraining Loss:  1944.003405487805\n",
      "\tEpoch 268 complete! \tTraining Loss:  1944.0385914634146\n",
      "\tEpoch 269 complete! \tTraining Loss:  1944.1317774390243\n",
      "\tEpoch 270 complete! \tTraining Loss:  1944.1405792682926\n",
      "\tEpoch 271 complete! \tTraining Loss:  1943.9468018292682\n",
      "\tEpoch 272 complete! \tTraining Loss:  1944.2441554878048\n",
      "\tEpoch 273 complete! \tTraining Loss:  1944.2370274390244\n",
      "\tEpoch 274 complete! \tTraining Loss:  1944.0954390243903\n",
      "\tEpoch 275 complete! \tTraining Loss:  1943.9464085365853\n",
      "\tEpoch 276 complete! \tTraining Loss:  1944.054542682927\n",
      "\tEpoch 277 complete! \tTraining Loss:  1944.2166036585365\n",
      "\tEpoch 278 complete! \tTraining Loss:  1943.9918384146342\n",
      "\tEpoch 279 complete! \tTraining Loss:  1944.0745213414634\n",
      "\tEpoch 280 complete! \tTraining Loss:  1944.1770823170732\n",
      "\tEpoch 281 complete! \tTraining Loss:  1944.126530487805\n",
      "\tEpoch 282 complete! \tTraining Loss:  1944.1522042682927\n",
      "\tEpoch 283 complete! \tTraining Loss:  1944.0781402439025\n",
      "\tEpoch 284 complete! \tTraining Loss:  1944.1516402439024\n",
      "\tEpoch 285 complete! \tTraining Loss:  1943.8134237804877\n",
      "\tEpoch 286 complete! \tTraining Loss:  1943.742457317073\n",
      "\tEpoch 287 complete! \tTraining Loss:  1944.060399390244\n",
      "\tEpoch 288 complete! \tTraining Loss:  1943.915112804878\n",
      "\tEpoch 289 complete! \tTraining Loss:  1943.7828658536584\n",
      "\tEpoch 290 complete! \tTraining Loss:  1944.0581341463414\n",
      "\tEpoch 291 complete! \tTraining Loss:  1944.118987804878\n",
      "\tEpoch 292 complete! \tTraining Loss:  1943.9119024390243\n",
      "\tEpoch 293 complete! \tTraining Loss:  1943.8111585365853\n",
      "\tEpoch 294 complete! \tTraining Loss:  1943.6025792682926\n",
      "\tEpoch 295 complete! \tTraining Loss:  1943.8870914634147\n",
      "\tEpoch 296 complete! \tTraining Loss:  1943.9799481707316\n",
      "\tEpoch 297 complete! \tTraining Loss:  1943.775112804878\n",
      "\tEpoch 298 complete! \tTraining Loss:  1943.685875\n",
      "\tEpoch 299 complete! \tTraining Loss:  1944.1740091463414\n",
      "\tEpoch 300 complete! \tTraining Loss:  1943.934332317073\n",
      "\tEpoch 301 complete! \tTraining Loss:  1943.6815548780487\n",
      "\tEpoch 302 complete! \tTraining Loss:  1943.6669481707318\n",
      "\tEpoch 303 complete! \tTraining Loss:  1943.9412835365854\n",
      "\tEpoch 304 complete! \tTraining Loss:  1943.8735823170732\n",
      "\tEpoch 305 complete! \tTraining Loss:  1943.6484024390245\n",
      "\tEpoch 306 complete! \tTraining Loss:  1943.4602774390244\n",
      "\tEpoch 307 complete! \tTraining Loss:  1943.6283932926829\n",
      "\tEpoch 308 complete! \tTraining Loss:  1943.718012195122\n",
      "\tEpoch 309 complete! \tTraining Loss:  1944.1021737804879\n",
      "\tEpoch 310 complete! \tTraining Loss:  1943.6844207317074\n",
      "\tEpoch 311 complete! \tTraining Loss:  1943.6755792682927\n",
      "\tEpoch 312 complete! \tTraining Loss:  1943.7365304878049\n",
      "\tEpoch 313 complete! \tTraining Loss:  1943.757661585366\n",
      "\tEpoch 314 complete! \tTraining Loss:  1943.6989268292682\n",
      "\tEpoch 315 complete! \tTraining Loss:  1943.4488109756098\n",
      "\tEpoch 316 complete! \tTraining Loss:  1943.7707103658536\n",
      "\tEpoch 317 complete! \tTraining Loss:  1943.7222347560976\n",
      "\tEpoch 318 complete! \tTraining Loss:  1943.8652134146341\n",
      "\tEpoch 319 complete! \tTraining Loss:  1943.6199603658536\n",
      "\tEpoch 320 complete! \tTraining Loss:  1943.5891798780488\n",
      "\tEpoch 321 complete! \tTraining Loss:  1943.8225152439024\n",
      "\tEpoch 322 complete! \tTraining Loss:  1943.8094085365854\n",
      "\tEpoch 323 complete! \tTraining Loss:  1943.4995701219511\n",
      "\tEpoch 324 complete! \tTraining Loss:  1943.5178780487804\n",
      "\tEpoch 325 complete! \tTraining Loss:  1943.7729146341464\n",
      "\tEpoch 326 complete! \tTraining Loss:  1943.8362530487805\n",
      "\tEpoch 327 complete! \tTraining Loss:  1943.5598384146342\n",
      "\tEpoch 328 complete! \tTraining Loss:  1943.5139786585366\n",
      "\tEpoch 329 complete! \tTraining Loss:  1943.4124390243903\n",
      "\tEpoch 330 complete! \tTraining Loss:  1943.6322926829268\n",
      "\tEpoch 331 complete! \tTraining Loss:  1943.5264298780487\n",
      "\tEpoch 332 complete! \tTraining Loss:  1943.3233841463414\n",
      "\tEpoch 333 complete! \tTraining Loss:  1943.5841737804878\n",
      "\tEpoch 334 complete! \tTraining Loss:  1943.6013262195122\n",
      "\tEpoch 335 complete! \tTraining Loss:  1943.4640213414634\n",
      "\tEpoch 336 complete! \tTraining Loss:  1943.2787652439024\n",
      "\tEpoch 337 complete! \tTraining Loss:  1943.6116280487804\n",
      "\tEpoch 338 complete! \tTraining Loss:  1943.3395487804878\n",
      "\tEpoch 339 complete! \tTraining Loss:  1943.8275274390244\n",
      "\tEpoch 340 complete! \tTraining Loss:  1943.6485792682927\n",
      "\tEpoch 341 complete! \tTraining Loss:  1943.4324085365854\n",
      "\tEpoch 342 complete! \tTraining Loss:  1943.460881097561\n",
      "\tEpoch 343 complete! \tTraining Loss:  1943.3212530487806\n",
      "\tEpoch 344 complete! \tTraining Loss:  1943.2392591463415\n",
      "\tEpoch 345 complete! \tTraining Loss:  1943.3862682926829\n",
      "\tEpoch 346 complete! \tTraining Loss:  1943.4134115853658\n",
      "\tEpoch 347 complete! \tTraining Loss:  1943.5316341463415\n",
      "\tEpoch 348 complete! \tTraining Loss:  1944.1321097560976\n",
      "\tEpoch 349 complete! \tTraining Loss:  1943.3863902439025\n",
      "\tEpoch 350 complete! \tTraining Loss:  1943.4261920731708\n",
      "torch.Size([10000, 3072])\n",
      "3\n",
      "\tEpoch 1 complete! \tTraining Loss:  1970.2362258064516\n",
      "\tEpoch 2 complete! \tTraining Loss:  1967.8457620967743\n",
      "\tEpoch 3 complete! \tTraining Loss:  1966.5725443548388\n",
      "\tEpoch 4 complete! \tTraining Loss:  1965.484258064516\n",
      "\tEpoch 5 complete! \tTraining Loss:  1965.2684556451613\n",
      "\tEpoch 6 complete! \tTraining Loss:  1965.0025887096774\n",
      "\tEpoch 7 complete! \tTraining Loss:  1964.7265846774194\n",
      "\tEpoch 8 complete! \tTraining Loss:  1964.5312661290322\n",
      "\tEpoch 9 complete! \tTraining Loss:  1964.3279193548387\n",
      "\tEpoch 10 complete! \tTraining Loss:  1964.1152338709678\n",
      "\tEpoch 11 complete! \tTraining Loss:  1963.8900483870968\n",
      "\tEpoch 12 complete! \tTraining Loss:  1962.8357338709677\n",
      "\tEpoch 13 complete! \tTraining Loss:  1963.4697419354839\n",
      "\tEpoch 14 complete! \tTraining Loss:  1963.1382217741937\n",
      "\tEpoch 15 complete! \tTraining Loss:  1963.3604314516128\n",
      "\tEpoch 16 complete! \tTraining Loss:  1962.7168911290323\n",
      "\tEpoch 17 complete! \tTraining Loss:  1962.2831169354838\n",
      "\tEpoch 18 complete! \tTraining Loss:  1962.6360201612904\n",
      "\tEpoch 19 complete! \tTraining Loss:  1962.0579193548388\n",
      "\tEpoch 20 complete! \tTraining Loss:  1961.978875\n",
      "\tEpoch 21 complete! \tTraining Loss:  1962.0411774193549\n",
      "\tEpoch 22 complete! \tTraining Loss:  1962.4137338709677\n",
      "\tEpoch 23 complete! \tTraining Loss:  1962.0107137096775\n",
      "\tEpoch 24 complete! \tTraining Loss:  1961.9106008064516\n",
      "\tEpoch 25 complete! \tTraining Loss:  1961.570560483871\n",
      "\tEpoch 26 complete! \tTraining Loss:  1961.860560483871\n",
      "\tEpoch 27 complete! \tTraining Loss:  1961.561741935484\n",
      "\tEpoch 28 complete! \tTraining Loss:  1961.8246451612904\n",
      "\tEpoch 29 complete! \tTraining Loss:  1961.5755806451614\n",
      "\tEpoch 30 complete! \tTraining Loss:  1961.3956451612903\n",
      "\tEpoch 31 complete! \tTraining Loss:  1960.8376975806452\n",
      "\tEpoch 32 complete! \tTraining Loss:  1961.699810483871\n",
      "\tEpoch 33 complete! \tTraining Loss:  1961.371754032258\n",
      "\tEpoch 34 complete! \tTraining Loss:  1961.5410524193549\n",
      "\tEpoch 35 complete! \tTraining Loss:  1961.3085161290323\n",
      "\tEpoch 36 complete! \tTraining Loss:  1961.2917258064517\n",
      "\tEpoch 37 complete! \tTraining Loss:  1961.1967580645162\n",
      "\tEpoch 38 complete! \tTraining Loss:  1961.1711491935484\n",
      "\tEpoch 39 complete! \tTraining Loss:  1961.0217419354838\n",
      "\tEpoch 40 complete! \tTraining Loss:  1961.0527016129033\n",
      "\tEpoch 41 complete! \tTraining Loss:  1961.0657701612904\n",
      "\tEpoch 42 complete! \tTraining Loss:  1960.881866935484\n",
      "\tEpoch 43 complete! \tTraining Loss:  1960.901697580645\n",
      "\tEpoch 44 complete! \tTraining Loss:  1961.1443629032258\n",
      "\tEpoch 45 complete! \tTraining Loss:  1960.540495967742\n",
      "\tEpoch 46 complete! \tTraining Loss:  1960.7954717741936\n",
      "\tEpoch 47 complete! \tTraining Loss:  1961.039008064516\n",
      "\tEpoch 48 complete! \tTraining Loss:  1960.423125\n",
      "\tEpoch 49 complete! \tTraining Loss:  1960.912129032258\n",
      "\tEpoch 50 complete! \tTraining Loss:  1960.4082217741936\n",
      "\tEpoch 51 complete! \tTraining Loss:  1960.606\n",
      "\tEpoch 52 complete! \tTraining Loss:  1960.5820766129032\n",
      "\tEpoch 53 complete! \tTraining Loss:  1960.9384637096773\n",
      "\tEpoch 54 complete! \tTraining Loss:  1960.2934758064516\n",
      "\tEpoch 55 complete! \tTraining Loss:  1960.3438508064517\n",
      "\tEpoch 56 complete! \tTraining Loss:  1960.1484596774194\n",
      "\tEpoch 57 complete! \tTraining Loss:  1960.0797661290324\n",
      "\tEpoch 58 complete! \tTraining Loss:  1959.6060766129033\n",
      "\tEpoch 59 complete! \tTraining Loss:  1960.2752177419354\n",
      "\tEpoch 60 complete! \tTraining Loss:  1960.7901532258065\n",
      "\tEpoch 61 complete! \tTraining Loss:  1959.5336975806451\n",
      "\tEpoch 62 complete! \tTraining Loss:  1959.5154072580644\n",
      "\tEpoch 63 complete! \tTraining Loss:  1960.1375241935484\n",
      "\tEpoch 64 complete! \tTraining Loss:  1959.8948991935483\n",
      "\tEpoch 65 complete! \tTraining Loss:  1959.3136653225806\n",
      "\tEpoch 66 complete! \tTraining Loss:  1960.0763346774193\n",
      "\tEpoch 67 complete! \tTraining Loss:  1959.6160403225806\n",
      "\tEpoch 68 complete! \tTraining Loss:  1959.7133427419355\n",
      "\tEpoch 69 complete! \tTraining Loss:  1959.8447056451612\n",
      "\tEpoch 70 complete! \tTraining Loss:  1959.8979112903226\n",
      "\tEpoch 71 complete! \tTraining Loss:  1959.4735120967741\n",
      "\tEpoch 72 complete! \tTraining Loss:  1959.93775\n",
      "\tEpoch 73 complete! \tTraining Loss:  1959.5913387096775\n",
      "\tEpoch 74 complete! \tTraining Loss:  1959.5428951612903\n",
      "\tEpoch 75 complete! \tTraining Loss:  1959.0195887096775\n",
      "\tEpoch 76 complete! \tTraining Loss:  1960.045564516129\n",
      "\tEpoch 77 complete! \tTraining Loss:  1959.7646411290323\n",
      "\tEpoch 78 complete! \tTraining Loss:  1959.674052419355\n",
      "\tEpoch 79 complete! \tTraining Loss:  1959.9658548387097\n",
      "\tEpoch 80 complete! \tTraining Loss:  1959.719254032258\n",
      "\tEpoch 81 complete! \tTraining Loss:  1959.6385322580645\n",
      "\tEpoch 82 complete! \tTraining Loss:  1959.847689516129\n",
      "\tEpoch 83 complete! \tTraining Loss:  1959.0227379032258\n",
      "\tEpoch 84 complete! \tTraining Loss:  1959.2044838709678\n",
      "\tEpoch 85 complete! \tTraining Loss:  1959.6292056451614\n",
      "\tEpoch 86 complete! \tTraining Loss:  1959.524125\n",
      "\tEpoch 87 complete! \tTraining Loss:  1959.0909596774193\n",
      "\tEpoch 88 complete! \tTraining Loss:  1959.527254032258\n",
      "\tEpoch 89 complete! \tTraining Loss:  1959.1951370967743\n",
      "\tEpoch 90 complete! \tTraining Loss:  1959.5204798387097\n",
      "\tEpoch 91 complete! \tTraining Loss:  1958.810189516129\n",
      "\tEpoch 92 complete! \tTraining Loss:  1959.5282016129033\n",
      "\tEpoch 93 complete! \tTraining Loss:  1959.2364435483871\n",
      "\tEpoch 94 complete! \tTraining Loss:  1959.277745967742\n",
      "\tEpoch 95 complete! \tTraining Loss:  1959.3112137096775\n",
      "\tEpoch 96 complete! \tTraining Loss:  1959.5489838709677\n",
      "\tEpoch 97 complete! \tTraining Loss:  1958.995741935484\n",
      "\tEpoch 98 complete! \tTraining Loss:  1959.217552419355\n",
      "\tEpoch 99 complete! \tTraining Loss:  1958.6666532258064\n",
      "\tEpoch 100 complete! \tTraining Loss:  1959.0109717741936\n",
      "\tEpoch 101 complete! \tTraining Loss:  1958.8265403225807\n",
      "\tEpoch 102 complete! \tTraining Loss:  1959.429375\n",
      "\tEpoch 103 complete! \tTraining Loss:  1959.0362862903226\n",
      "\tEpoch 104 complete! \tTraining Loss:  1958.998439516129\n",
      "\tEpoch 105 complete! \tTraining Loss:  1958.9733346774194\n",
      "\tEpoch 106 complete! \tTraining Loss:  1959.2372782258064\n",
      "\tEpoch 107 complete! \tTraining Loss:  1959.1241653225807\n",
      "\tEpoch 108 complete! \tTraining Loss:  1959.0657782258065\n",
      "\tEpoch 109 complete! \tTraining Loss:  1958.928991935484\n",
      "\tEpoch 110 complete! \tTraining Loss:  1958.7935887096774\n",
      "\tEpoch 111 complete! \tTraining Loss:  1958.655883064516\n",
      "\tEpoch 112 complete! \tTraining Loss:  1959.3931975806452\n",
      "\tEpoch 113 complete! \tTraining Loss:  1958.6647056451613\n",
      "\tEpoch 114 complete! \tTraining Loss:  1958.3479475806453\n",
      "\tEpoch 115 complete! \tTraining Loss:  1958.4486411290322\n",
      "\tEpoch 116 complete! \tTraining Loss:  1959.1807096774194\n",
      "\tEpoch 117 complete! \tTraining Loss:  1959.1582983870967\n",
      "\tEpoch 118 complete! \tTraining Loss:  1958.8056169354838\n",
      "\tEpoch 119 complete! \tTraining Loss:  1958.6860483870967\n",
      "\tEpoch 120 complete! \tTraining Loss:  1958.227508064516\n",
      "\tEpoch 121 complete! \tTraining Loss:  1958.5599193548387\n",
      "\tEpoch 122 complete! \tTraining Loss:  1958.3141572580646\n",
      "\tEpoch 123 complete! \tTraining Loss:  1958.6007620967741\n",
      "\tEpoch 124 complete! \tTraining Loss:  1959.077060483871\n",
      "\tEpoch 125 complete! \tTraining Loss:  1958.7201370967741\n",
      "\tEpoch 126 complete! \tTraining Loss:  1958.808995967742\n",
      "\tEpoch 127 complete! \tTraining Loss:  1958.670564516129\n",
      "\tEpoch 128 complete! \tTraining Loss:  1958.7032459677419\n",
      "\tEpoch 129 complete! \tTraining Loss:  1958.520262096774\n",
      "\tEpoch 130 complete! \tTraining Loss:  1958.7211612903225\n",
      "\tEpoch 131 complete! \tTraining Loss:  1958.2461733870969\n",
      "\tEpoch 132 complete! \tTraining Loss:  1958.8539153225806\n",
      "\tEpoch 133 complete! \tTraining Loss:  1958.2875846774193\n",
      "\tEpoch 134 complete! \tTraining Loss:  1958.1452459677419\n",
      "\tEpoch 135 complete! \tTraining Loss:  1958.5325967741935\n",
      "\tEpoch 136 complete! \tTraining Loss:  1958.74725\n",
      "\tEpoch 137 complete! \tTraining Loss:  1957.8735403225805\n",
      "\tEpoch 138 complete! \tTraining Loss:  1958.548314516129\n",
      "\tEpoch 139 complete! \tTraining Loss:  1958.459116935484\n",
      "\tEpoch 140 complete! \tTraining Loss:  1958.171004032258\n",
      "\tEpoch 141 complete! \tTraining Loss:  1958.5765443548387\n",
      "\tEpoch 142 complete! \tTraining Loss:  1958.550818548387\n",
      "\tEpoch 143 complete! \tTraining Loss:  1958.0601008064516\n",
      "\tEpoch 144 complete! \tTraining Loss:  1958.1447822580644\n",
      "\tEpoch 145 complete! \tTraining Loss:  1958.2467701612902\n",
      "\tEpoch 146 complete! \tTraining Loss:  1957.876366935484\n",
      "\tEpoch 147 complete! \tTraining Loss:  1958.1003306451612\n",
      "\tEpoch 148 complete! \tTraining Loss:  1958.0258588709678\n",
      "\tEpoch 149 complete! \tTraining Loss:  1959.0069032258064\n",
      "\tEpoch 150 complete! \tTraining Loss:  1958.1954233870968\n",
      "\tEpoch 151 complete! \tTraining Loss:  1957.520072580645\n",
      "\tEpoch 152 complete! \tTraining Loss:  1958.2193508064515\n",
      "\tEpoch 153 complete! \tTraining Loss:  1958.0867661290322\n",
      "\tEpoch 154 complete! \tTraining Loss:  1958.1902177419354\n",
      "\tEpoch 155 complete! \tTraining Loss:  1957.745133064516\n",
      "\tEpoch 156 complete! \tTraining Loss:  1957.7470725806452\n",
      "\tEpoch 157 complete! \tTraining Loss:  1957.9828629032259\n",
      "\tEpoch 158 complete! \tTraining Loss:  1958.0126854838709\n",
      "\tEpoch 159 complete! \tTraining Loss:  1958.2679556451612\n",
      "\tEpoch 160 complete! \tTraining Loss:  1957.7492741935484\n",
      "\tEpoch 161 complete! \tTraining Loss:  1957.8747701612904\n",
      "\tEpoch 162 complete! \tTraining Loss:  1957.993741935484\n",
      "\tEpoch 163 complete! \tTraining Loss:  1958.158814516129\n",
      "\tEpoch 164 complete! \tTraining Loss:  1957.3324274193549\n",
      "\tEpoch 165 complete! \tTraining Loss:  1957.832181451613\n",
      "\tEpoch 166 complete! \tTraining Loss:  1957.5145322580645\n",
      "\tEpoch 167 complete! \tTraining Loss:  1957.8824193548387\n",
      "\tEpoch 168 complete! \tTraining Loss:  1958.198508064516\n",
      "\tEpoch 169 complete! \tTraining Loss:  1958.1793870967742\n",
      "\tEpoch 170 complete! \tTraining Loss:  1957.7977540322581\n",
      "\tEpoch 171 complete! \tTraining Loss:  1957.802189516129\n",
      "\tEpoch 172 complete! \tTraining Loss:  1957.565560483871\n",
      "\tEpoch 173 complete! \tTraining Loss:  1957.7596491935483\n",
      "\tEpoch 174 complete! \tTraining Loss:  1957.2115\n",
      "\tEpoch 175 complete! \tTraining Loss:  1957.8873588709678\n",
      "\tEpoch 176 complete! \tTraining Loss:  1957.5819838709676\n",
      "\tEpoch 177 complete! \tTraining Loss:  1957.7654193548387\n",
      "\tEpoch 178 complete! \tTraining Loss:  1957.6316008064516\n",
      "\tEpoch 179 complete! \tTraining Loss:  1957.6288629032258\n",
      "\tEpoch 180 complete! \tTraining Loss:  1957.861629032258\n",
      "\tEpoch 181 complete! \tTraining Loss:  1958.0028911290322\n",
      "\tEpoch 182 complete! \tTraining Loss:  1957.9337701612903\n",
      "\tEpoch 183 complete! \tTraining Loss:  1957.099995967742\n",
      "\tEpoch 184 complete! \tTraining Loss:  1957.6676693548386\n",
      "\tEpoch 185 complete! \tTraining Loss:  1957.5897258064517\n",
      "\tEpoch 186 complete! \tTraining Loss:  1957.7192741935485\n",
      "\tEpoch 187 complete! \tTraining Loss:  1957.2034798387097\n",
      "\tEpoch 188 complete! \tTraining Loss:  1957.6836491935485\n",
      "\tEpoch 189 complete! \tTraining Loss:  1957.4546612903225\n",
      "\tEpoch 190 complete! \tTraining Loss:  1957.4705\n",
      "\tEpoch 191 complete! \tTraining Loss:  1957.3512661290322\n",
      "\tEpoch 192 complete! \tTraining Loss:  1957.3661088709678\n",
      "\tEpoch 193 complete! \tTraining Loss:  1957.2268991935484\n",
      "\tEpoch 194 complete! \tTraining Loss:  1957.3565120967742\n",
      "\tEpoch 195 complete! \tTraining Loss:  1957.7614153225807\n",
      "\tEpoch 196 complete! \tTraining Loss:  1957.2540846774193\n",
      "\tEpoch 197 complete! \tTraining Loss:  1957.7393306451613\n",
      "\tEpoch 198 complete! \tTraining Loss:  1957.1234233870969\n",
      "\tEpoch 199 complete! \tTraining Loss:  1957.0232217741936\n",
      "\tEpoch 200 complete! \tTraining Loss:  1957.4305120967742\n",
      "\tEpoch 201 complete! \tTraining Loss:  1957.4163346774194\n",
      "\tEpoch 202 complete! \tTraining Loss:  1957.7122177419355\n",
      "\tEpoch 203 complete! \tTraining Loss:  1957.7316169354838\n",
      "\tEpoch 204 complete! \tTraining Loss:  1957.6475161290323\n",
      "\tEpoch 205 complete! \tTraining Loss:  1957.771629032258\n",
      "\tEpoch 206 complete! \tTraining Loss:  1957.5148427419356\n",
      "\tEpoch 207 complete! \tTraining Loss:  1956.5989637096775\n",
      "\tEpoch 208 complete! \tTraining Loss:  1957.5329879032258\n",
      "\tEpoch 209 complete! \tTraining Loss:  1957.8272217741935\n",
      "\tEpoch 210 complete! \tTraining Loss:  1957.2026653225807\n",
      "\tEpoch 211 complete! \tTraining Loss:  1957.1660201612904\n",
      "\tEpoch 212 complete! \tTraining Loss:  1956.9610887096774\n",
      "\tEpoch 213 complete! \tTraining Loss:  1957.0364153225808\n",
      "\tEpoch 214 complete! \tTraining Loss:  1957.4782338709676\n",
      "\tEpoch 215 complete! \tTraining Loss:  1957.1928387096775\n",
      "\tEpoch 216 complete! \tTraining Loss:  1957.0260120967741\n",
      "\tEpoch 217 complete! \tTraining Loss:  1956.9493669354838\n",
      "\tEpoch 218 complete! \tTraining Loss:  1957.3666411290324\n",
      "\tEpoch 219 complete! \tTraining Loss:  1957.3724032258065\n",
      "\tEpoch 220 complete! \tTraining Loss:  1956.9232137096774\n",
      "\tEpoch 221 complete! \tTraining Loss:  1956.86075\n",
      "\tEpoch 222 complete! \tTraining Loss:  1957.1499758064517\n",
      "\tEpoch 223 complete! \tTraining Loss:  1957.5168991935484\n",
      "\tEpoch 224 complete! \tTraining Loss:  1957.4642741935484\n",
      "\tEpoch 225 complete! \tTraining Loss:  1957.0197217741936\n",
      "\tEpoch 226 complete! \tTraining Loss:  1956.8542056451613\n",
      "\tEpoch 227 complete! \tTraining Loss:  1956.984427419355\n",
      "\tEpoch 228 complete! \tTraining Loss:  1957.5764798387097\n",
      "\tEpoch 229 complete! \tTraining Loss:  1957.0159838709678\n",
      "\tEpoch 230 complete! \tTraining Loss:  1957.0310887096773\n",
      "\tEpoch 231 complete! \tTraining Loss:  1956.8567862903226\n",
      "\tEpoch 232 complete! \tTraining Loss:  1956.894987903226\n",
      "\tEpoch 233 complete! \tTraining Loss:  1957.7424879032258\n",
      "\tEpoch 234 complete! \tTraining Loss:  1956.7139758064516\n",
      "\tEpoch 235 complete! \tTraining Loss:  1956.5960201612904\n",
      "\tEpoch 236 complete! \tTraining Loss:  1957.2099758064517\n",
      "\tEpoch 237 complete! \tTraining Loss:  1956.8786411290323\n",
      "\tEpoch 238 complete! \tTraining Loss:  1957.0913064516128\n",
      "\tEpoch 239 complete! \tTraining Loss:  1957.438802419355\n",
      "\tEpoch 240 complete! \tTraining Loss:  1956.8052862903226\n",
      "\tEpoch 241 complete! \tTraining Loss:  1956.7223266129033\n",
      "\tEpoch 242 complete! \tTraining Loss:  1956.8827903225806\n",
      "\tEpoch 243 complete! \tTraining Loss:  1956.8704435483871\n",
      "\tEpoch 244 complete! \tTraining Loss:  1956.402814516129\n",
      "\tEpoch 245 complete! \tTraining Loss:  1957.1901693548386\n",
      "\tEpoch 246 complete! \tTraining Loss:  1956.7472258064515\n",
      "\tEpoch 247 complete! \tTraining Loss:  1956.5107701612903\n",
      "\tEpoch 248 complete! \tTraining Loss:  1956.9310806451613\n",
      "\tEpoch 249 complete! \tTraining Loss:  1956.7492217741935\n",
      "\tEpoch 250 complete! \tTraining Loss:  1956.9780967741935\n",
      "\tEpoch 251 complete! \tTraining Loss:  1956.8037459677419\n",
      "\tEpoch 252 complete! \tTraining Loss:  1956.9321008064517\n",
      "\tEpoch 253 complete! \tTraining Loss:  1956.9831169354838\n",
      "\tEpoch 254 complete! \tTraining Loss:  1956.810060483871\n",
      "\tEpoch 255 complete! \tTraining Loss:  1956.321491935484\n",
      "\tEpoch 256 complete! \tTraining Loss:  1956.3851451612902\n",
      "\tEpoch 257 complete! \tTraining Loss:  1956.634495967742\n",
      "\tEpoch 258 complete! \tTraining Loss:  1956.7069032258064\n",
      "\tEpoch 259 complete! \tTraining Loss:  1956.6307137096774\n",
      "\tEpoch 260 complete! \tTraining Loss:  1957.3183427419356\n",
      "\tEpoch 261 complete! \tTraining Loss:  1956.7517217741936\n",
      "\tEpoch 262 complete! \tTraining Loss:  1956.5587782258065\n",
      "\tEpoch 263 complete! \tTraining Loss:  1956.4674596774194\n",
      "\tEpoch 264 complete! \tTraining Loss:  1956.5613548387096\n",
      "\tEpoch 265 complete! \tTraining Loss:  1956.8819798387096\n",
      "\tEpoch 266 complete! \tTraining Loss:  1956.5106169354838\n",
      "\tEpoch 267 complete! \tTraining Loss:  1956.7525725806452\n",
      "\tEpoch 268 complete! \tTraining Loss:  1956.4914354838709\n",
      "\tEpoch 269 complete! \tTraining Loss:  1956.2102580645162\n",
      "\tEpoch 270 complete! \tTraining Loss:  1956.3857338709677\n",
      "\tEpoch 271 complete! \tTraining Loss:  1956.5252258064515\n",
      "\tEpoch 272 complete! \tTraining Loss:  1956.6587298387096\n",
      "\tEpoch 273 complete! \tTraining Loss:  1956.6406290322582\n",
      "\tEpoch 274 complete! \tTraining Loss:  1956.5940443548386\n",
      "\tEpoch 275 complete! \tTraining Loss:  1956.6507258064516\n",
      "\tEpoch 276 complete! \tTraining Loss:  1956.2809798387098\n",
      "\tEpoch 277 complete! \tTraining Loss:  1956.1421774193548\n",
      "\tEpoch 278 complete! \tTraining Loss:  1956.312310483871\n",
      "\tEpoch 279 complete! \tTraining Loss:  1956.0718266129031\n",
      "\tEpoch 280 complete! \tTraining Loss:  1956.3322661290322\n",
      "\tEpoch 281 complete! \tTraining Loss:  1956.4980887096774\n",
      "\tEpoch 282 complete! \tTraining Loss:  1956.9269475806452\n",
      "\tEpoch 283 complete! \tTraining Loss:  1956.45125\n",
      "\tEpoch 284 complete! \tTraining Loss:  1956.0275766129032\n",
      "\tEpoch 285 complete! \tTraining Loss:  1956.0977943548387\n",
      "\tEpoch 286 complete! \tTraining Loss:  1956.5091532258064\n",
      "\tEpoch 287 complete! \tTraining Loss:  1956.9276935483872\n",
      "\tEpoch 288 complete! \tTraining Loss:  1956.0919354838709\n",
      "\tEpoch 289 complete! \tTraining Loss:  1956.7507419354838\n",
      "\tEpoch 290 complete! \tTraining Loss:  1956.2144032258066\n",
      "\tEpoch 291 complete! \tTraining Loss:  1956.4201733870968\n",
      "\tEpoch 292 complete! \tTraining Loss:  1957.0064838709677\n",
      "\tEpoch 293 complete! \tTraining Loss:  1956.3192137096773\n",
      "\tEpoch 294 complete! \tTraining Loss:  1956.5038024193548\n",
      "\tEpoch 295 complete! \tTraining Loss:  1956.119629032258\n",
      "\tEpoch 296 complete! \tTraining Loss:  1956.577939516129\n",
      "\tEpoch 297 complete! \tTraining Loss:  1955.959310483871\n",
      "\tEpoch 298 complete! \tTraining Loss:  1956.2867419354839\n",
      "\tEpoch 299 complete! \tTraining Loss:  1955.9742862903227\n",
      "\tEpoch 300 complete! \tTraining Loss:  1956.2484677419354\n",
      "\tEpoch 301 complete! \tTraining Loss:  1956.9368991935485\n",
      "\tEpoch 302 complete! \tTraining Loss:  1956.1545362903225\n",
      "\tEpoch 303 complete! \tTraining Loss:  1956.0821693548387\n",
      "\tEpoch 304 complete! \tTraining Loss:  1955.9574838709677\n",
      "\tEpoch 305 complete! \tTraining Loss:  1956.5661975806452\n",
      "\tEpoch 306 complete! \tTraining Loss:  1956.9750120967742\n",
      "\tEpoch 307 complete! \tTraining Loss:  1956.3581612903226\n",
      "\tEpoch 308 complete! \tTraining Loss:  1956.211318548387\n",
      "\tEpoch 309 complete! \tTraining Loss:  1956.6038266129033\n",
      "\tEpoch 310 complete! \tTraining Loss:  1956.4922137096773\n",
      "\tEpoch 311 complete! \tTraining Loss:  1955.973125\n",
      "\tEpoch 312 complete! \tTraining Loss:  1956.130068548387\n",
      "\tEpoch 313 complete! \tTraining Loss:  1956.1419556451613\n",
      "\tEpoch 314 complete! \tTraining Loss:  1955.975552419355\n",
      "\tEpoch 315 complete! \tTraining Loss:  1956.396133064516\n",
      "\tEpoch 316 complete! \tTraining Loss:  1956.1914193548387\n",
      "\tEpoch 317 complete! \tTraining Loss:  1956.1054798387097\n",
      "\tEpoch 318 complete! \tTraining Loss:  1956.019947580645\n",
      "\tEpoch 319 complete! \tTraining Loss:  1956.3599112903225\n",
      "\tEpoch 320 complete! \tTraining Loss:  1956.1487620967741\n",
      "\tEpoch 321 complete! \tTraining Loss:  1956.6619959677419\n",
      "\tEpoch 322 complete! \tTraining Loss:  1956.6096612903225\n",
      "\tEpoch 323 complete! \tTraining Loss:  1955.7250120967742\n",
      "\tEpoch 324 complete! \tTraining Loss:  1956.4049193548387\n",
      "\tEpoch 325 complete! \tTraining Loss:  1955.3576532258064\n",
      "\tEpoch 326 complete! \tTraining Loss:  1955.9545725806452\n",
      "\tEpoch 327 complete! \tTraining Loss:  1956.1125040322581\n",
      "\tEpoch 328 complete! \tTraining Loss:  1956.0425725806451\n",
      "\tEpoch 329 complete! \tTraining Loss:  1955.7673870967742\n",
      "\tEpoch 330 complete! \tTraining Loss:  1955.7955564516128\n",
      "\tEpoch 331 complete! \tTraining Loss:  1956.0280564516129\n",
      "\tEpoch 332 complete! \tTraining Loss:  1956.446439516129\n",
      "\tEpoch 333 complete! \tTraining Loss:  1956.3028991935485\n",
      "\tEpoch 334 complete! \tTraining Loss:  1955.7595806451613\n",
      "\tEpoch 335 complete! \tTraining Loss:  1955.6016532258066\n",
      "\tEpoch 336 complete! \tTraining Loss:  1955.9861733870969\n",
      "\tEpoch 337 complete! \tTraining Loss:  1956.2817620967742\n",
      "\tEpoch 338 complete! \tTraining Loss:  1956.0544475806453\n",
      "\tEpoch 339 complete! \tTraining Loss:  1956.1434153225807\n",
      "\tEpoch 340 complete! \tTraining Loss:  1956.4493427419354\n",
      "\tEpoch 341 complete! \tTraining Loss:  1956.2782379032258\n",
      "\tEpoch 342 complete! \tTraining Loss:  1955.9843588709678\n",
      "\tEpoch 343 complete! \tTraining Loss:  1955.936310483871\n",
      "\tEpoch 344 complete! \tTraining Loss:  1955.9897096774193\n",
      "\tEpoch 345 complete! \tTraining Loss:  1956.1887258064517\n",
      "\tEpoch 346 complete! \tTraining Loss:  1955.604181451613\n",
      "\tEpoch 347 complete! \tTraining Loss:  1955.9820483870967\n",
      "\tEpoch 348 complete! \tTraining Loss:  1955.9225403225807\n",
      "\tEpoch 349 complete! \tTraining Loss:  1956.1146491935483\n",
      "\tEpoch 350 complete! \tTraining Loss:  1955.8857056451614\n",
      "torch.Size([10000, 3072])\n",
      "4\n",
      "\tEpoch 1 complete! \tTraining Loss:  1987.139278846154\n",
      "\tEpoch 2 complete! \tTraining Loss:  1985.2773557692308\n",
      "\tEpoch 3 complete! \tTraining Loss:  1984.9605048076924\n",
      "\tEpoch 4 complete! \tTraining Loss:  1984.1166442307692\n",
      "\tEpoch 5 complete! \tTraining Loss:  1983.9127548076924\n",
      "\tEpoch 6 complete! \tTraining Loss:  1983.741375\n",
      "\tEpoch 7 complete! \tTraining Loss:  1983.5649230769232\n",
      "\tEpoch 8 complete! \tTraining Loss:  1983.3839951923078\n",
      "\tEpoch 9 complete! \tTraining Loss:  1983.1939951923077\n",
      "\tEpoch 10 complete! \tTraining Loss:  1983.0511490384615\n",
      "\tEpoch 11 complete! \tTraining Loss:  1982.8274807692308\n",
      "\tEpoch 12 complete! \tTraining Loss:  1982.786278846154\n",
      "\tEpoch 13 complete! \tTraining Loss:  1982.5401201923078\n",
      "\tEpoch 14 complete! \tTraining Loss:  1982.4331826923076\n",
      "\tEpoch 15 complete! \tTraining Loss:  1982.3070528846154\n",
      "\tEpoch 16 complete! \tTraining Loss:  1982.6190432692308\n",
      "\tEpoch 17 complete! \tTraining Loss:  1982.2003701923077\n",
      "\tEpoch 18 complete! \tTraining Loss:  1981.9107019230769\n",
      "\tEpoch 19 complete! \tTraining Loss:  1981.9491009615385\n",
      "\tEpoch 20 complete! \tTraining Loss:  1982.0108990384615\n",
      "\tEpoch 21 complete! \tTraining Loss:  1981.8172548076923\n",
      "\tEpoch 22 complete! \tTraining Loss:  1981.6220528846154\n",
      "\tEpoch 23 complete! \tTraining Loss:  1981.7017019230768\n",
      "\tEpoch 24 complete! \tTraining Loss:  1981.7350625\n",
      "\tEpoch 25 complete! \tTraining Loss:  1981.4030817307691\n",
      "\tEpoch 26 complete! \tTraining Loss:  1981.492923076923\n",
      "\tEpoch 27 complete! \tTraining Loss:  1981.6079134615384\n",
      "\tEpoch 28 complete! \tTraining Loss:  1981.1631105769231\n",
      "\tEpoch 29 complete! \tTraining Loss:  1981.1595865384616\n",
      "\tEpoch 30 complete! \tTraining Loss:  1981.346485576923\n",
      "\tEpoch 31 complete! \tTraining Loss:  1981.2409471153846\n",
      "\tEpoch 32 complete! \tTraining Loss:  1981.2280913461539\n",
      "\tEpoch 33 complete! \tTraining Loss:  1980.8608798076923\n",
      "\tEpoch 34 complete! \tTraining Loss:  1980.7596057692308\n",
      "\tEpoch 35 complete! \tTraining Loss:  1980.9317163461537\n",
      "\tEpoch 36 complete! \tTraining Loss:  1981.1078557692308\n",
      "\tEpoch 37 complete! \tTraining Loss:  1980.675701923077\n",
      "\tEpoch 38 complete! \tTraining Loss:  1980.714048076923\n",
      "\tEpoch 39 complete! \tTraining Loss:  1980.4034182692308\n",
      "\tEpoch 40 complete! \tTraining Loss:  1980.931403846154\n",
      "\tEpoch 41 complete! \tTraining Loss:  1980.9801009615385\n",
      "\tEpoch 42 complete! \tTraining Loss:  1980.4495721153846\n",
      "\tEpoch 43 complete! \tTraining Loss:  1980.4803173076923\n",
      "\tEpoch 44 complete! \tTraining Loss:  1980.8438798076922\n",
      "\tEpoch 45 complete! \tTraining Loss:  1980.1853028846153\n",
      "\tEpoch 46 complete! \tTraining Loss:  1980.2455288461538\n",
      "\tEpoch 47 complete! \tTraining Loss:  1980.5095432692308\n",
      "\tEpoch 48 complete! \tTraining Loss:  1980.5471971153845\n",
      "\tEpoch 49 complete! \tTraining Loss:  1980.2606586538461\n",
      "\tEpoch 50 complete! \tTraining Loss:  1980.4911538461538\n",
      "\tEpoch 51 complete! \tTraining Loss:  1980.3590673076924\n",
      "\tEpoch 52 complete! \tTraining Loss:  1980.2724567307691\n",
      "\tEpoch 53 complete! \tTraining Loss:  1980.135639423077\n",
      "\tEpoch 54 complete! \tTraining Loss:  1980.2756394230769\n",
      "\tEpoch 55 complete! \tTraining Loss:  1980.214798076923\n",
      "\tEpoch 56 complete! \tTraining Loss:  1980.4265673076923\n",
      "\tEpoch 57 complete! \tTraining Loss:  1980.1466442307692\n",
      "\tEpoch 58 complete! \tTraining Loss:  1980.2594375\n",
      "\tEpoch 59 complete! \tTraining Loss:  1980.2293509615386\n",
      "\tEpoch 60 complete! \tTraining Loss:  1979.8330721153845\n",
      "\tEpoch 61 complete! \tTraining Loss:  1980.0065817307693\n",
      "\tEpoch 62 complete! \tTraining Loss:  1980.3055048076924\n",
      "\tEpoch 63 complete! \tTraining Loss:  1979.7864519230768\n",
      "\tEpoch 64 complete! \tTraining Loss:  1979.9378798076923\n",
      "\tEpoch 65 complete! \tTraining Loss:  1979.7865865384615\n",
      "\tEpoch 66 complete! \tTraining Loss:  1979.8591634615384\n",
      "\tEpoch 67 complete! \tTraining Loss:  1979.7854903846153\n",
      "\tEpoch 68 complete! \tTraining Loss:  1979.8607163461538\n",
      "\tEpoch 69 complete! \tTraining Loss:  1979.4640961538462\n",
      "\tEpoch 70 complete! \tTraining Loss:  1979.6116490384616\n",
      "\tEpoch 71 complete! \tTraining Loss:  1979.6094086538462\n",
      "\tEpoch 72 complete! \tTraining Loss:  1979.4474663461538\n",
      "\tEpoch 73 complete! \tTraining Loss:  1979.9008076923076\n",
      "\tEpoch 74 complete! \tTraining Loss:  1979.60075\n",
      "\tEpoch 75 complete! \tTraining Loss:  1979.7371057692308\n",
      "\tEpoch 76 complete! \tTraining Loss:  1979.7136923076923\n",
      "\tEpoch 77 complete! \tTraining Loss:  1979.7845769230769\n",
      "\tEpoch 78 complete! \tTraining Loss:  1979.466908653846\n",
      "\tEpoch 79 complete! \tTraining Loss:  1979.3310817307693\n",
      "\tEpoch 80 complete! \tTraining Loss:  1979.627576923077\n",
      "\tEpoch 81 complete! \tTraining Loss:  1979.3738221153847\n",
      "\tEpoch 82 complete! \tTraining Loss:  1979.2747884615385\n",
      "\tEpoch 83 complete! \tTraining Loss:  1979.6154182692308\n",
      "\tEpoch 84 complete! \tTraining Loss:  1979.4373509615384\n",
      "\tEpoch 85 complete! \tTraining Loss:  1978.9773942307693\n",
      "\tEpoch 86 complete! \tTraining Loss:  1979.3622884615384\n",
      "\tEpoch 87 complete! \tTraining Loss:  1979.1384759615385\n",
      "\tEpoch 88 complete! \tTraining Loss:  1978.9857115384616\n",
      "\tEpoch 89 complete! \tTraining Loss:  1979.2374086538462\n",
      "\tEpoch 90 complete! \tTraining Loss:  1979.504048076923\n",
      "\tEpoch 91 complete! \tTraining Loss:  1978.920110576923\n",
      "\tEpoch 92 complete! \tTraining Loss:  1979.425639423077\n",
      "\tEpoch 93 complete! \tTraining Loss:  1979.424596153846\n",
      "\tEpoch 94 complete! \tTraining Loss:  1978.8582692307693\n",
      "\tEpoch 95 complete! \tTraining Loss:  1978.9839278846155\n",
      "\tEpoch 96 complete! \tTraining Loss:  1979.1902740384614\n",
      "\tEpoch 97 complete! \tTraining Loss:  1979.0863125\n",
      "\tEpoch 98 complete! \tTraining Loss:  1978.8384663461538\n",
      "\tEpoch 99 complete! \tTraining Loss:  1978.607110576923\n",
      "\tEpoch 100 complete! \tTraining Loss:  1978.8474423076923\n",
      "\tEpoch 101 complete! \tTraining Loss:  1979.326375\n",
      "\tEpoch 102 complete! \tTraining Loss:  1978.6223653846155\n",
      "\tEpoch 103 complete! \tTraining Loss:  1979.1427259615384\n",
      "\tEpoch 104 complete! \tTraining Loss:  1978.654408653846\n",
      "\tEpoch 105 complete! \tTraining Loss:  1979.1915336538461\n",
      "\tEpoch 106 complete! \tTraining Loss:  1978.9497596153847\n",
      "\tEpoch 107 complete! \tTraining Loss:  1978.8503798076922\n",
      "\tEpoch 108 complete! \tTraining Loss:  1978.8444134615384\n",
      "\tEpoch 109 complete! \tTraining Loss:  1979.1586586538463\n",
      "\tEpoch 110 complete! \tTraining Loss:  1978.4026634615384\n",
      "\tEpoch 111 complete! \tTraining Loss:  1978.8676682692308\n",
      "\tEpoch 112 complete! \tTraining Loss:  1978.6871153846155\n",
      "\tEpoch 113 complete! \tTraining Loss:  1978.9205576923077\n",
      "\tEpoch 114 complete! \tTraining Loss:  1978.8164326923077\n",
      "\tEpoch 115 complete! \tTraining Loss:  1978.9092211538461\n",
      "\tEpoch 116 complete! \tTraining Loss:  1979.0616971153845\n",
      "\tEpoch 117 complete! \tTraining Loss:  1978.577610576923\n",
      "\tEpoch 118 complete! \tTraining Loss:  1978.7372884615384\n",
      "\tEpoch 119 complete! \tTraining Loss:  1978.9719615384615\n",
      "\tEpoch 120 complete! \tTraining Loss:  1978.5113461538463\n",
      "\tEpoch 121 complete! \tTraining Loss:  1978.6836057692308\n",
      "\tEpoch 122 complete! \tTraining Loss:  1978.981639423077\n",
      "\tEpoch 123 complete! \tTraining Loss:  1978.661610576923\n",
      "\tEpoch 124 complete! \tTraining Loss:  1978.5003557692307\n",
      "\tEpoch 125 complete! \tTraining Loss:  1978.8199182692308\n",
      "\tEpoch 126 complete! \tTraining Loss:  1978.5022548076922\n",
      "\tEpoch 127 complete! \tTraining Loss:  1978.2095528846153\n",
      "\tEpoch 128 complete! \tTraining Loss:  1978.4299903846154\n",
      "\tEpoch 129 complete! \tTraining Loss:  1978.4905528846155\n",
      "\tEpoch 130 complete! \tTraining Loss:  1978.3455336538461\n",
      "\tEpoch 131 complete! \tTraining Loss:  1978.520451923077\n",
      "\tEpoch 132 complete! \tTraining Loss:  1978.3843798076923\n",
      "\tEpoch 133 complete! \tTraining Loss:  1978.1673846153847\n",
      "\tEpoch 134 complete! \tTraining Loss:  1978.5485\n",
      "\tEpoch 135 complete! \tTraining Loss:  1978.3443317307692\n",
      "\tEpoch 136 complete! \tTraining Loss:  1978.2403317307692\n",
      "\tEpoch 137 complete! \tTraining Loss:  1978.4939471153846\n",
      "\tEpoch 138 complete! \tTraining Loss:  1978.2387067307693\n",
      "\tEpoch 139 complete! \tTraining Loss:  1978.4667115384616\n",
      "\tEpoch 140 complete! \tTraining Loss:  1978.4405528846153\n",
      "\tEpoch 141 complete! \tTraining Loss:  1978.2351634615384\n",
      "\tEpoch 142 complete! \tTraining Loss:  1978.4333076923076\n",
      "\tEpoch 143 complete! \tTraining Loss:  1978.1011009615384\n",
      "\tEpoch 144 complete! \tTraining Loss:  1978.2364567307693\n",
      "\tEpoch 145 complete! \tTraining Loss:  1978.4966971153847\n",
      "\tEpoch 146 complete! \tTraining Loss:  1978.1289903846155\n",
      "\tEpoch 147 complete! \tTraining Loss:  1978.3849519230769\n",
      "\tEpoch 148 complete! \tTraining Loss:  1977.9405432692308\n",
      "\tEpoch 149 complete! \tTraining Loss:  1977.9277980769232\n",
      "\tEpoch 150 complete! \tTraining Loss:  1977.8953317307692\n",
      "\tEpoch 151 complete! \tTraining Loss:  1978.2665769230769\n",
      "\tEpoch 152 complete! \tTraining Loss:  1978.2951778846154\n",
      "\tEpoch 153 complete! \tTraining Loss:  1978.0475384615384\n",
      "\tEpoch 154 complete! \tTraining Loss:  1978.2787067307693\n",
      "\tEpoch 155 complete! \tTraining Loss:  1978.0653461538461\n",
      "\tEpoch 156 complete! \tTraining Loss:  1977.8981394230768\n",
      "\tEpoch 157 complete! \tTraining Loss:  1978.2091009615385\n",
      "\tEpoch 158 complete! \tTraining Loss:  1978.0280192307691\n",
      "\tEpoch 159 complete! \tTraining Loss:  1978.2218990384615\n",
      "\tEpoch 160 complete! \tTraining Loss:  1978.0979711538462\n",
      "\tEpoch 161 complete! \tTraining Loss:  1977.9288221153847\n",
      "\tEpoch 162 complete! \tTraining Loss:  1977.7957740384616\n",
      "\tEpoch 163 complete! \tTraining Loss:  1978.1633365384616\n",
      "\tEpoch 164 complete! \tTraining Loss:  1977.7448846153845\n",
      "\tEpoch 165 complete! \tTraining Loss:  1977.963673076923\n",
      "\tEpoch 166 complete! \tTraining Loss:  1978.0568365384615\n",
      "\tEpoch 167 complete! \tTraining Loss:  1977.741673076923\n",
      "\tEpoch 168 complete! \tTraining Loss:  1977.9059951923077\n",
      "\tEpoch 169 complete! \tTraining Loss:  1978.1541153846154\n",
      "\tEpoch 170 complete! \tTraining Loss:  1977.7359471153845\n",
      "\tEpoch 171 complete! \tTraining Loss:  1978.1620721153847\n",
      "\tEpoch 172 complete! \tTraining Loss:  1978.161466346154\n",
      "\tEpoch 173 complete! \tTraining Loss:  1977.8970192307693\n",
      "\tEpoch 174 complete! \tTraining Loss:  1977.6840288461538\n",
      "\tEpoch 175 complete! \tTraining Loss:  1977.8731346153845\n",
      "\tEpoch 176 complete! \tTraining Loss:  1978.27175\n",
      "\tEpoch 177 complete! \tTraining Loss:  1977.635514423077\n",
      "\tEpoch 178 complete! \tTraining Loss:  1977.6027403846153\n",
      "\tEpoch 179 complete! \tTraining Loss:  1978.0957692307693\n",
      "\tEpoch 180 complete! \tTraining Loss:  1977.9071682692309\n",
      "\tEpoch 181 complete! \tTraining Loss:  1977.6343653846154\n",
      "\tEpoch 182 complete! \tTraining Loss:  1977.8177115384615\n",
      "\tEpoch 183 complete! \tTraining Loss:  1977.9574807692309\n",
      "\tEpoch 184 complete! \tTraining Loss:  1978.001576923077\n",
      "\tEpoch 185 complete! \tTraining Loss:  1977.744576923077\n",
      "\tEpoch 186 complete! \tTraining Loss:  1977.718423076923\n",
      "\tEpoch 187 complete! \tTraining Loss:  1977.7478557692307\n",
      "\tEpoch 188 complete! \tTraining Loss:  1977.985014423077\n",
      "\tEpoch 189 complete! \tTraining Loss:  1977.9170625\n",
      "\tEpoch 190 complete! \tTraining Loss:  1977.551110576923\n",
      "\tEpoch 191 complete! \tTraining Loss:  1977.9056538461539\n",
      "\tEpoch 192 complete! \tTraining Loss:  1977.910360576923\n",
      "\tEpoch 193 complete! \tTraining Loss:  1977.6504423076924\n",
      "\tEpoch 194 complete! \tTraining Loss:  1977.6230913461538\n",
      "\tEpoch 195 complete! \tTraining Loss:  1977.4539182692308\n",
      "\tEpoch 196 complete! \tTraining Loss:  1977.5885913461539\n",
      "\tEpoch 197 complete! \tTraining Loss:  1977.6337307692309\n",
      "\tEpoch 198 complete! \tTraining Loss:  1977.307985576923\n",
      "\tEpoch 199 complete! \tTraining Loss:  1977.7308365384615\n",
      "\tEpoch 200 complete! \tTraining Loss:  1977.6398942307692\n",
      "\tEpoch 201 complete! \tTraining Loss:  1977.1606009615384\n",
      "\tEpoch 202 complete! \tTraining Loss:  1977.507528846154\n",
      "\tEpoch 203 complete! \tTraining Loss:  1977.6319567307692\n",
      "\tEpoch 204 complete! \tTraining Loss:  1977.3145961538462\n",
      "\tEpoch 205 complete! \tTraining Loss:  1977.4152692307691\n",
      "\tEpoch 206 complete! \tTraining Loss:  1977.2576009615384\n",
      "\tEpoch 207 complete! \tTraining Loss:  1977.2811923076922\n",
      "\tEpoch 208 complete! \tTraining Loss:  1977.239653846154\n",
      "\tEpoch 209 complete! \tTraining Loss:  1977.5404326923076\n",
      "\tEpoch 210 complete! \tTraining Loss:  1977.244125\n",
      "\tEpoch 211 complete! \tTraining Loss:  1977.3670625\n",
      "\tEpoch 212 complete! \tTraining Loss:  1977.2507067307693\n",
      "\tEpoch 213 complete! \tTraining Loss:  1977.3706826923076\n",
      "\tEpoch 214 complete! \tTraining Loss:  1977.2881201923076\n",
      "\tEpoch 215 complete! \tTraining Loss:  1977.1909182692307\n",
      "\tEpoch 216 complete! \tTraining Loss:  1977.3696442307692\n",
      "\tEpoch 217 complete! \tTraining Loss:  1977.4580528846154\n",
      "\tEpoch 218 complete! \tTraining Loss:  1977.4428076923077\n",
      "\tEpoch 219 complete! \tTraining Loss:  1977.2809375\n",
      "\tEpoch 220 complete! \tTraining Loss:  1977.3505865384616\n",
      "\tEpoch 221 complete! \tTraining Loss:  1977.517360576923\n",
      "\tEpoch 222 complete! \tTraining Loss:  1977.3449278846153\n",
      "\tEpoch 223 complete! \tTraining Loss:  1977.198033653846\n",
      "\tEpoch 224 complete! \tTraining Loss:  1977.2858413461538\n",
      "\tEpoch 225 complete! \tTraining Loss:  1977.2942740384615\n",
      "\tEpoch 226 complete! \tTraining Loss:  1977.4135528846155\n",
      "\tEpoch 227 complete! \tTraining Loss:  1977.1631971153847\n",
      "\tEpoch 228 complete! \tTraining Loss:  1977.2831875\n",
      "\tEpoch 229 complete! \tTraining Loss:  1976.9899182692307\n",
      "\tEpoch 230 complete! \tTraining Loss:  1977.31175\n",
      "\tEpoch 231 complete! \tTraining Loss:  1977.0546586538462\n",
      "\tEpoch 232 complete! \tTraining Loss:  1977.2766634615384\n",
      "\tEpoch 233 complete! \tTraining Loss:  1977.599951923077\n",
      "\tEpoch 234 complete! \tTraining Loss:  1977.4496394230769\n",
      "\tEpoch 235 complete! \tTraining Loss:  1977.0418653846154\n",
      "\tEpoch 236 complete! \tTraining Loss:  1977.0794903846154\n",
      "\tEpoch 237 complete! \tTraining Loss:  1977.1606394230769\n",
      "\tEpoch 238 complete! \tTraining Loss:  1977.083889423077\n",
      "\tEpoch 239 complete! \tTraining Loss:  1977.3103990384616\n",
      "\tEpoch 240 complete! \tTraining Loss:  1976.952423076923\n",
      "\tEpoch 241 complete! \tTraining Loss:  1977.033826923077\n",
      "\tEpoch 242 complete! \tTraining Loss:  1977.0495240384616\n",
      "\tEpoch 243 complete! \tTraining Loss:  1976.8912932692308\n",
      "\tEpoch 244 complete! \tTraining Loss:  1977.0310240384615\n",
      "\tEpoch 245 complete! \tTraining Loss:  1976.872423076923\n",
      "\tEpoch 246 complete! \tTraining Loss:  1977.0690817307693\n",
      "\tEpoch 247 complete! \tTraining Loss:  1976.9518701923078\n",
      "\tEpoch 248 complete! \tTraining Loss:  1977.2395721153846\n",
      "\tEpoch 249 complete! \tTraining Loss:  1976.7231153846153\n",
      "\tEpoch 250 complete! \tTraining Loss:  1977.08625\n",
      "\tEpoch 251 complete! \tTraining Loss:  1977.053889423077\n",
      "\tEpoch 252 complete! \tTraining Loss:  1977.0056298076922\n",
      "\tEpoch 253 complete! \tTraining Loss:  1977.230716346154\n",
      "\tEpoch 254 complete! \tTraining Loss:  1976.8481778846153\n",
      "\tEpoch 255 complete! \tTraining Loss:  1977.0153942307693\n",
      "\tEpoch 256 complete! \tTraining Loss:  1977.1559471153846\n",
      "\tEpoch 257 complete! \tTraining Loss:  1977.062076923077\n",
      "\tEpoch 258 complete! \tTraining Loss:  1976.7581826923076\n",
      "\tEpoch 259 complete! \tTraining Loss:  1976.8610432692308\n",
      "\tEpoch 260 complete! \tTraining Loss:  1977.189875\n",
      "\tEpoch 261 complete! \tTraining Loss:  1977.0388798076924\n",
      "\tEpoch 262 complete! \tTraining Loss:  1977.2387067307693\n",
      "\tEpoch 263 complete! \tTraining Loss:  1977.091701923077\n",
      "\tEpoch 264 complete! \tTraining Loss:  1977.1936201923077\n",
      "\tEpoch 265 complete! \tTraining Loss:  1976.9361634615384\n",
      "\tEpoch 266 complete! \tTraining Loss:  1976.9844038461538\n",
      "\tEpoch 267 complete! \tTraining Loss:  1976.6144903846155\n",
      "\tEpoch 268 complete! \tTraining Loss:  1976.6768798076923\n",
      "\tEpoch 269 complete! \tTraining Loss:  1976.7073028846153\n",
      "\tEpoch 270 complete! \tTraining Loss:  1976.9766490384616\n",
      "\tEpoch 271 complete! \tTraining Loss:  1976.8484375\n",
      "\tEpoch 272 complete! \tTraining Loss:  1976.581639423077\n",
      "\tEpoch 273 complete! \tTraining Loss:  1976.6878942307692\n",
      "\tEpoch 274 complete! \tTraining Loss:  1976.9621105769231\n",
      "\tEpoch 275 complete! \tTraining Loss:  1977.2245480769232\n",
      "\tEpoch 276 complete! \tTraining Loss:  1976.6327548076922\n",
      "\tEpoch 277 complete! \tTraining Loss:  1976.7112980769232\n",
      "\tEpoch 278 complete! \tTraining Loss:  1976.6932403846154\n",
      "\tEpoch 279 complete! \tTraining Loss:  1976.9215336538462\n",
      "\tEpoch 280 complete! \tTraining Loss:  1976.569735576923\n",
      "\tEpoch 281 complete! \tTraining Loss:  1976.7633125\n",
      "\tEpoch 282 complete! \tTraining Loss:  1976.8035721153847\n",
      "\tEpoch 283 complete! \tTraining Loss:  1976.9314567307692\n",
      "\tEpoch 284 complete! \tTraining Loss:  1976.961591346154\n",
      "\tEpoch 285 complete! \tTraining Loss:  1976.550826923077\n",
      "\tEpoch 286 complete! \tTraining Loss:  1976.5422211538462\n",
      "\tEpoch 287 complete! \tTraining Loss:  1976.5206778846155\n",
      "\tEpoch 288 complete! \tTraining Loss:  1977.027076923077\n",
      "\tEpoch 289 complete! \tTraining Loss:  1976.8691586538462\n",
      "\tEpoch 290 complete! \tTraining Loss:  1976.8837259615384\n",
      "\tEpoch 291 complete! \tTraining Loss:  1976.6662692307693\n",
      "\tEpoch 292 complete! \tTraining Loss:  1976.7192692307692\n",
      "\tEpoch 293 complete! \tTraining Loss:  1976.7973509615385\n",
      "\tEpoch 294 complete! \tTraining Loss:  1976.8423076923077\n",
      "\tEpoch 295 complete! \tTraining Loss:  1976.3885336538463\n",
      "\tEpoch 296 complete! \tTraining Loss:  1976.5823557692308\n",
      "\tEpoch 297 complete! \tTraining Loss:  1976.6354567307692\n",
      "\tEpoch 298 complete! \tTraining Loss:  1976.990576923077\n",
      "\tEpoch 299 complete! \tTraining Loss:  1976.695658653846\n",
      "\tEpoch 300 complete! \tTraining Loss:  1976.3733846153846\n",
      "\tEpoch 301 complete! \tTraining Loss:  1976.5874038461538\n",
      "\tEpoch 302 complete! \tTraining Loss:  1976.4365865384616\n",
      "\tEpoch 303 complete! \tTraining Loss:  1976.762860576923\n",
      "\tEpoch 304 complete! \tTraining Loss:  1976.1766730769232\n",
      "\tEpoch 305 complete! \tTraining Loss:  1976.5973221153847\n",
      "\tEpoch 306 complete! \tTraining Loss:  1976.3028365384616\n",
      "\tEpoch 307 complete! \tTraining Loss:  1976.1764375\n",
      "\tEpoch 308 complete! \tTraining Loss:  1976.4618798076924\n",
      "\tEpoch 309 complete! \tTraining Loss:  1976.3632067307692\n",
      "\tEpoch 310 complete! \tTraining Loss:  1976.5132451923077\n",
      "\tEpoch 311 complete! \tTraining Loss:  1976.5211394230769\n",
      "\tEpoch 312 complete! \tTraining Loss:  1976.032375\n",
      "\tEpoch 313 complete! \tTraining Loss:  1976.399889423077\n",
      "\tEpoch 314 complete! \tTraining Loss:  1976.7300384615385\n",
      "\tEpoch 315 complete! \tTraining Loss:  1976.3882019230768\n",
      "\tEpoch 316 complete! \tTraining Loss:  1976.1354230769232\n",
      "\tEpoch 317 complete! \tTraining Loss:  1976.5735240384615\n",
      "\tEpoch 318 complete! \tTraining Loss:  1976.0682403846154\n",
      "\tEpoch 319 complete! \tTraining Loss:  1976.5376826923077\n",
      "\tEpoch 320 complete! \tTraining Loss:  1976.2795240384614\n",
      "\tEpoch 321 complete! \tTraining Loss:  1976.5682259615385\n",
      "\tEpoch 322 complete! \tTraining Loss:  1976.3954038461538\n",
      "\tEpoch 323 complete! \tTraining Loss:  1976.267096153846\n",
      "\tEpoch 324 complete! \tTraining Loss:  1976.5261682692308\n",
      "\tEpoch 325 complete! \tTraining Loss:  1976.415860576923\n",
      "\tEpoch 326 complete! \tTraining Loss:  1976.1940769230769\n",
      "\tEpoch 327 complete! \tTraining Loss:  1976.6138942307691\n",
      "\tEpoch 328 complete! \tTraining Loss:  1976.2231586538462\n",
      "\tEpoch 329 complete! \tTraining Loss:  1976.3597403846154\n",
      "\tEpoch 330 complete! \tTraining Loss:  1976.6234711538461\n",
      "\tEpoch 331 complete! \tTraining Loss:  1976.3533461538461\n",
      "\tEpoch 332 complete! \tTraining Loss:  1976.1559423076924\n",
      "\tEpoch 333 complete! \tTraining Loss:  1976.391\n",
      "\tEpoch 334 complete! \tTraining Loss:  1976.403735576923\n",
      "\tEpoch 335 complete! \tTraining Loss:  1976.3881394230768\n",
      "\tEpoch 336 complete! \tTraining Loss:  1976.827076923077\n",
      "\tEpoch 337 complete! \tTraining Loss:  1975.897826923077\n",
      "\tEpoch 338 complete! \tTraining Loss:  1976.3105480769232\n",
      "\tEpoch 339 complete! \tTraining Loss:  1976.279389423077\n",
      "\tEpoch 340 complete! \tTraining Loss:  1976.575375\n",
      "\tEpoch 341 complete! \tTraining Loss:  1976.2233701923076\n",
      "\tEpoch 342 complete! \tTraining Loss:  1976.4452884615384\n",
      "\tEpoch 343 complete! \tTraining Loss:  1976.2317307692308\n",
      "\tEpoch 344 complete! \tTraining Loss:  1976.6364615384616\n",
      "\tEpoch 345 complete! \tTraining Loss:  1975.9709663461538\n",
      "\tEpoch 346 complete! \tTraining Loss:  1976.3592067307693\n",
      "\tEpoch 347 complete! \tTraining Loss:  1976.1443413461539\n",
      "\tEpoch 348 complete! \tTraining Loss:  1976.0278173076922\n",
      "\tEpoch 349 complete! \tTraining Loss:  1976.1114375\n",
      "\tEpoch 350 complete! \tTraining Loss:  1976.1706971153847\n",
      "torch.Size([10000, 3072])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utilsADCN1 import svhnLoader, plotPerformance\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.autograd.variable import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "num_workers = 0\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import vae\n",
    "import svhn_vae1\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "seed = 54 \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                         transforms.Grayscale(),                      \n",
    "\n",
    "                     ])\n",
    "unlabeledData = datasets.SVHN(root='./data/svhn', split='train', download=True,\n",
    "                                        transform=transform)\n",
    "extra = datasets.SVHN(root='./data/svhn', split='extra', download=True,\n",
    "                                        transform=transform)\n",
    "labeledData = datasets.SVHN(root='./data/svhn', split='test', download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "\n",
    "ex_svhn,unused = random_split(extra,[20000,len(extra)-20000])\n",
    "svhnData = torch.utils.data.ConcatDataset((unlabeledData,ex_svhn))\n",
    "\n",
    "\n",
    "## Generate the datastream\n",
    "dataStream = svhnLoader(labeledData, svhnData, nEachClassSamples = 500)\n",
    "\n",
    "\n",
    "dataStream.createTask(nTask = 5, taskList = [[0,1],[2,3],[4,5],[6,7],[8,9]], taskType = 3)\n",
    "\n",
    "X = torch.transpose(dataStream.unlabeledData[2],1,2)\n",
    "X = torch.transpose(X,2,3)\n",
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X[5\n",
    "            ])\n",
    "\n",
    "### Dataloader for different tasks\n",
    "def trn_loader(task_id,batch_size):\n",
    "  # task_id = 0\n",
    "  x_samples = dataStream.unlabeledData[task_id].to(torch.float32)\n",
    "#   x_samples = F.adjust_sharpness(x_samples, 5.0)\n",
    "  y_samples = dataStream.unlabeledLabel[task_id].to(torch.uint8)\n",
    "\n",
    "  train_data = TensorDataset(x_samples,y_samples)\n",
    "  train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "  return train_loader, train_data\n",
    "\n",
    "\n",
    "def metrics_aa_gm(ypred, ytrue):\n",
    "    cm = confusion_matrix(ytrue, ypred)\n",
    "    # print(confusion_matrix(ytrue, ypred))\n",
    "    sum_classes = np.sum(cm, axis=1)\n",
    "    true_pred = np.diagonal(cm)\n",
    "    tp_rate = true_pred/sum_classes\n",
    "    ACSA = np.mean(tp_rate)\n",
    "    GM = np.prod(tp_rate)**(1/cm.shape[0])\n",
    "    return ACSA, GM\n",
    "\n",
    "\n",
    "\n",
    "def s_loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "#     reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    w_loss = x.mean() -x_hat.mean() #mean(x,x_hat) # K.mean(y_true * y_pred)\n",
    "    beta = 1.5\n",
    "\n",
    "    return reproduction_loss + beta*KLD + w_loss\n",
    "\n",
    "def vae_train(train_loader, model,optimzr,epochs,batchSize,x_dim):\n",
    "  # Training\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "      train_loss = 0\n",
    "      for batch_idx, (x, _) in enumerate(train_loader):\n",
    "          x = x.view(batchSize, x_dim)\n",
    "          x = x.to(DEVICE)\n",
    "          x_hat, mean, log_var = model(x)\n",
    "          loss = s_loss_function(x, x_hat, mean, log_var)\n",
    "\n",
    "          train_loss += loss.item()\n",
    "          loss.backward()\n",
    "          optimzr.step()\n",
    "          optimzr.zero_grad()\n",
    "      print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tTraining Loss: \", train_loss / (batch_idx*batchSize),)\n",
    "  return model\n",
    "\n",
    "def asgn_clust_labl(km, tr_lbls):\n",
    "    asgn_lbls = {}\n",
    "#     print(km)\n",
    "\n",
    "    for i in range(km.n_clusters):\n",
    "\n",
    "        lbls = []\n",
    "        idx = np.where(km.labels_ == i)\n",
    "        if (len(idx[0]) > 0):\n",
    "            lbls.append(tr_lbls[idx])\n",
    "            if (len(lbls[0]) == 1):   \n",
    "                cnts = np.bincount(lbls[0])\n",
    "            else:\n",
    "                cnts = np.bincount(np.squeeze(lbls))\n",
    "\n",
    "#             print(i, cnts)\n",
    "            if np.argmax(cnts) in asgn_lbls:\n",
    "#                 print(i)\n",
    "                asgn_lbls[np.argmax(cnts)].append(i)\n",
    "            else:\n",
    "#                 print(i)\n",
    "                asgn_lbls[np.argmax(cnts)] = [i]\n",
    "\n",
    "    return asgn_lbls\n",
    "\n",
    "def data_lbls(X_lbls, clust_lbls,tsk_id):\n",
    "\n",
    "    p_lbls = np.ones(len(X_lbls)).astype(np.uint8)*2*tsk_id\n",
    "\n",
    "    for i, clus in enumerate(X_lbls):\n",
    "        for key,value in clust_lbls.items():\n",
    "#             print(key,value)\n",
    "            if clus in value:\n",
    "                p_lbls[i] = key\n",
    "\n",
    "    return p_lbls\n",
    "\n",
    "\n",
    "def latent_data(trn_load_ful,model,x_dim):\n",
    "  for batch_idx, (x_ful, y_ful) in enumerate(trn_load_ful):\n",
    "    x_ful = x_ful.to(DEVICE)\n",
    "    y_ful = y_ful.to(DEVICE)\n",
    "  x_ful = x_ful.view(x_ful.shape[0], x_dim)\n",
    "  print(x_ful.shape)\n",
    "\n",
    "  model.eval()\n",
    "  x_out, z_mean_vae, _ = model(x_ful)\n",
    "  return x_ful, y_ful, x_out, z_mean_vae\n",
    "\n",
    "\n",
    "#### Structure of data capturing\n",
    "def structure_data(real_data):\n",
    "\n",
    "  P_cov = torch.cov(real_data, correction=0)\n",
    "  D, V = torch.linalg.eigh(P_cov)\n",
    "  idx = torch.argsort(D, dim=0,descending = True)\n",
    "  d = D[idx]\n",
    "  V = V[:,idx]\n",
    "   \n",
    "  return d, V\n",
    "\n",
    "X = torch.transpose(dataStream.unlabeledData[2],1,2)\n",
    "X = torch.transpose(X,2,3)\n",
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X[7554])\n",
    "\n",
    "## Expert Training\n",
    "task_id = 0\n",
    "# VAE Model Hyperparameters\n",
    "#batch_size = 100\n",
    "inputdim = dataStream.unlabeledData[0].shape[1]*dataStream.unlabeledData[0].shape[2]*dataStream.unlabeledData[0].shape[3]\n",
    "hidden_dim1 = 1500\n",
    "hidden_dim2 = 700\n",
    "latent_dim = [128,128, 128,128,128]\n",
    "lr = 1e-4 # [1e-4,1e-4,1e-4,1e-4,1e-4]\n",
    "s_encoder = svhn_vae1.Encoder(input_dim =inputdim , hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, latent_dim=latent_dim[task_id])\n",
    "s_decoder = svhn_vae1.Decoder(latent_dim=latent_dim[task_id], hidden_dim1=hidden_dim1,hidden_dim2=hidden_dim2, output_dim=inputdim)\n",
    "s_task_model = svhn_vae1.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "optimizer_VAE = Adam(s_task_model.parameters(), lr=lr)\n",
    "batchsize = 500 #300 #200 #ataStream.batchSize\n",
    "task_clust_lbls = {}\n",
    "epch = [350, 350 , 350, 350 , 350]\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for task_id in range(5):\n",
    "\n",
    "  print(task_id)\n",
    "  data_loader, trn_data = trn_loader(task_id, batchsize)\n",
    "\n",
    "  s_task_model = vae_train(data_loader,s_task_model,optimizer_VAE ,epch[task_id],batchsize,inputdim)\n",
    "  torch.save(s_task_model.state_dict(),'s_task'+str(task_id)+'_model')\n",
    "\n",
    "\n",
    "  for i in range(1):\n",
    "    (x_lbl,y_lbl) = next(iter(data_loader))\n",
    "\n",
    "  x_lbl = x_lbl.view(batchsize, inputdim)\n",
    "  s_task_model.eval()\n",
    "  _,z_lbl,_ = s_task_model(x_lbl.to(DEVICE))\n",
    "\n",
    "  # dataStream.unlabeledData[0].shape[0]\n",
    "  train_loader_ful = DataLoader(dataset=trn_data, batch_size=10000, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "  x_ful, y_ful, x_out, z_mean_vae = latent_data(train_loader_ful,s_task_model,inputdim)\n",
    "\n",
    "  X = z_mean_vae.cpu().detach().numpy()\n",
    "\n",
    "  clust_lbl =  torch.unique(y_lbl).detach().numpy()\n",
    "  n_clust = len(clust_lbl)\n",
    "\n",
    "  if task_id ==0 :\n",
    "    z_mn = {} \n",
    "    z_EVal = {} \n",
    "    z_EVec = {}\n",
    "  \n",
    "  kmeans = MiniBatchKMeans(n_clusters= 325, random_state=3, init = 'k-means++', n_init=\"auto\", max_iter=15000, tol=0.00002,  ).fit(z_lbl.cpu().detach().numpy())\n",
    "\n",
    "  clus_lbls = asgn_clust_labl(kmeans, y_lbl.detach().numpy())\n",
    "  task_clust_lbls[task_id] = clus_lbls\n",
    "\n",
    "  for i in range(n_clust):\n",
    "    zxmean =  torch.mean(z_lbl[y_lbl == clust_lbl[i]],0)\n",
    "    eVal, eVec = structure_data(z_lbl[y_lbl == clust_lbl[i]].T)\n",
    "    # if task_id == 0:\n",
    "    z_mn[task_id*n_clust+i] = zxmean\n",
    "    z_EVal[task_id*n_clust+i] = eVal\n",
    "    z_EVec[task_id*n_clust+i] = eVec\n",
    "  \n",
    "  if task_id == 0:\n",
    "    # tmodel0 = task_model\n",
    "    filename = 'Spl0_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 1:\n",
    "    # tmodel1 = task_model\n",
    "    filename = 'Spl1_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 2:\n",
    "    # tmodel2 = task_model\n",
    "    filename = 'Spl2_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 3:\n",
    "    # tmodel3 = task_model\n",
    "    filename = 'Spl3_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "  elif task_id == 4:\n",
    "    # tmodel5 = task_model\n",
    "    filename = 'Spl4_clust_model.sav'\n",
    "    pickle.dump(kmeans, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk_count = 5\n",
    "arr_cnt = len(z_mn)\n",
    "\n",
    "sample_size = 700 # Sample size for the structured data\n",
    "\n",
    "struct_samp_arr = torch.zeros((arr_cnt*sample_size,dataStream.unlabeledData[0][1].shape[0],\n",
    "                               dataStream.unlabeledData[0][1].shape[1],dataStream.unlabeledData[0][1].shape[2]))\n",
    "tsk_arr = torch.zeros((arr_cnt*sample_size))\n",
    "\n",
    "### Structured data generation\n",
    "\n",
    "for n_cnt in range(len(z_mn)):\n",
    "  s_encoder = svhn_vae1.Encoder(input_dim =inputdim , hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2,latent_dim=latent_dim[n_cnt//2])\n",
    "  s_decoder = svhn_vae1.Decoder(latent_dim=latent_dim[n_cnt//2],hidden_dim1=hidden_dim1,hidden_dim2=hidden_dim2, output_dim=inputdim)\n",
    "  savd_model = svhn_vae1.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "\n",
    "  std_vec = torch.randn(sample_size, latent_dim[n_cnt//2]).to(DEVICE) #torch.empty(sample_size, latent_dim).normal_(mean=0,std=1.0) #\n",
    "  a_vec = torch.mul(std_vec,z_EVal[n_cnt])\n",
    "  a_vec = torch.matmul(z_EVec[n_cnt],a_vec.T).T+z_mn[n_cnt][None,:]\n",
    "\n",
    "  savd_model.load_state_dict(torch.load('s_task'+str(n_cnt//2)+'_model'))\n",
    "  savd_model = savd_model.to(DEVICE)\n",
    "  struct_samp_arr[(sample_size)*(n_cnt):(sample_size)*(n_cnt+1),:,:,:] = (savd_model.Decoder(a_vec)).view(-1,3,32,32)\n",
    "  tsk_arr[n_cnt*sample_size:(n_cnt+1)*sample_size] = n_cnt//2\n",
    "\n",
    "\n",
    "ta_dataset = TensorDataset(struct_samp_arr,tsk_arr)\n",
    "\n",
    "\n",
    "# In[32]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7000, 3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_samp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  5 \n",
      "avg acc =  0.37520114071914856 GM = 0.3442526432733308\n",
      "Epoch =  10 \n",
      "avg acc =  0.9388697423462302 GM = 0.9375142621491646\n",
      "Epoch =  15 \n",
      "avg acc =  0.9699726919673877 GM = 0.9698835127428799\n",
      "Epoch =  20 \n",
      "avg acc =  0.9861193920571253 GM = 0.9860343881441447\n",
      "Epoch =  25 \n",
      "avg acc =  0.9840965271698104 GM = 0.9840614529378184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Divide the structured dataset into gTrain and gVal\n",
    "gTrain, gValid = random_split(ta_dataset,[6000,1000])\n",
    "gbatch = 50 #100(52.p)\n",
    "gtrain_loader = DataLoader(dataset=gTrain, batch_size=gbatch, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "gvalid_loader = DataLoader(dataset=gValid, batch_size=1000, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "class Guide(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_task):\n",
    "\n",
    "        super(Guide, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, n_task),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            # nn.GELU(),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "ta_model = Guide(n_task = dataStream.nTask)\n",
    "\n",
    "ta_model.to(DEVICE)\n",
    "\n",
    "learning_rate = 0.0001  #0.000016 #0.0000001\n",
    "\n",
    "ta_optimizer = torch.optim.RMSprop(ta_model.parameters(), lr=learning_rate)\n",
    "ta_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "  ta_model.train()\n",
    "  clf_error = 0.0\n",
    "  clf_valid_error = 0.0\n",
    "  for batch_idx, (x_in,labels) in enumerate(gtrain_loader):\n",
    "    \n",
    "    x_in  = x_in.to(DEVICE)\n",
    "    x_in = x_in.to(torch.float32)\n",
    "    #labels = labels.view(batch_size_CLF, 1)\n",
    "    labels  = labels.to(torch.long)#.to(DEVICE)\n",
    "\n",
    "    ta_optimizer.zero_grad()\n",
    "\n",
    "    pred_task = ta_model(x_in.detach())\n",
    "    error_gde = ta_criterion(pred_task, labels.to(DEVICE).detach())\n",
    "    \n",
    "    error_gde.backward(retain_graph=True)\n",
    "\n",
    "    ta_optimizer.step()\n",
    "    gc.collect()\n",
    "   \n",
    "  if (epoch >= 5 and epoch%5==0):\n",
    "    \n",
    "    ta_model.eval()\n",
    "    for i in range(1):\n",
    "      (x_val,t_val) = next(iter(gvalid_loader))\n",
    "\n",
    "    output = ta_model((x_val.to(DEVICE)))\n",
    "    task_pred = torch.argmax(output,dim=1)\n",
    "    # Evaluation metrics\n",
    "    acsa, gm = metrics_aa_gm(task_pred.cpu().detach(), t_val.cpu().detach())\n",
    "    print (\"Epoch = \",epoch,\"\\n\"\"avg acc = \",acsa, \"GM =\",gm,)\n",
    "    gc.collect()\n",
    "  # return model\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: 0\n",
      "[[279 101]\n",
      " [ 53 320]]\n",
      "Task acc: 0.599\n",
      "Task: 1\n",
      "[[351  88]\n",
      " [156 205]]\n",
      "Task acc: 0.5559999999999999\n",
      "Task: 2\n",
      "[[382  53]\n",
      " [104 293]]\n",
      "Task acc: 0.675\n",
      "Task: 3\n",
      "[[257  61]\n",
      " [ 61 342]]\n",
      "Task acc: 0.599\n",
      "Task: 4\n",
      "[[277  75]\n",
      " [144 237]]\n",
      "Task acc: 0.514\n",
      "Average accuracy = 0.5886\n",
      "Time taken= 1434.0279500484467\n"
     ]
    }
   ],
   "source": [
    "# #### Testing\n",
    "\n",
    "### Test data for guidance block\n",
    "\n",
    "samples_per_task = 500\n",
    "x_testdata = torch.zeros(dataStream.nTask*samples_per_task,dataStream.labeledData[0].shape[1],dataStream.labeledData[0].shape[2],dataStream.labeledData[0].shape[3])\n",
    "y_testdata = torch.zeros(dataStream.nTask*samples_per_task)\n",
    "t_testdata = torch.zeros(dataStream.nTask*samples_per_task)\n",
    "\n",
    "for task_id in range(dataStream.nTask):\n",
    "  x_testsamples = dataStream.labeledData[task_id].to(torch.float32)\n",
    "  y_testsamples = dataStream.labeledLabel[task_id].to(torch.uint8)\n",
    "  indx_arr = torch.randint(x_testsamples.shape[0], (samples_per_task,))\n",
    "  x_testdata[samples_per_task*task_id:(samples_per_task)*(task_id+1),:,:,:] = x_testsamples[indx_arr]\n",
    "  y_testdata[samples_per_task*task_id:samples_per_task*(task_id+1),] = y_testsamples[indx_arr]\n",
    "  t_testdata[samples_per_task*task_id:samples_per_task*(task_id+1),] = task_id\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "t_testdata = t_testdata.to(torch.long)\n",
    "gtest_data = TensorDataset(x_testdata,t_testdata,y_testdata)\n",
    "gtest_loader = DataLoader(dataset=gtest_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "torch.sum(t_testdata ==4)\n",
    "\n",
    "for batch_idx, (xtest, t_test,y_test) in enumerate(gtest_loader):\n",
    "        # xtest = xtest.view(5000, x_dim)\n",
    "        xtest = xtest.to(DEVICE)\n",
    "        #ytest = ytest.view(1000, 1)\n",
    "        t_test = t_test.to(DEVICE)\n",
    "        y_test = y_test.to(DEVICE)\n",
    "\n",
    "\n",
    "ta_model .eval()\n",
    "tst_p = ta_model((xtest))\n",
    "task_pred = torch.argmax(tst_p,dim=1)\n",
    "# Evaluation metrics\n",
    "acsa, gm = metrics_aa_gm(task_pred.cpu().detach(), t_test.cpu().detach())\n",
    "\n",
    "# confusion_matrix(t_test.cpu().detach(), task_pred.cpu().detach())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(t_test.cpu().detach(), task_pred.cpu().detach())\n",
    "\n",
    "sum_tr = 0\n",
    "ta_model.eval()\n",
    "for task_id in range(5):\n",
    "  Xtest1 = dataStream.labeledData[task_id].to(DEVICE)#[0:200]\n",
    "  # Xtest2 =  transforms.GaussianBlur(kernel_size=(3,3), sigma=(5,5))(Xtest1)\n",
    "  ytest1 = dataStream.labeledLabel[task_id].to(DEVICE)#[0:200]\n",
    "  tst_p = ta_model((Xtest1))\n",
    "  task_pred = torch.argmax(tst_p,dim=1)\n",
    "  loc_vec = torch.argwhere(task_pred == task_id)\n",
    "  print('Task:', task_id)\n",
    "  task_asngr_acc = len(loc_vec)/len(ytest1)\n",
    "#   print(task_asngr_acc)\n",
    "\n",
    "  # Xtest1[task_pred == 0]\n",
    "  s_encoder = svhn_vae1.Encoder(input_dim =inputdim , hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, latent_dim=latent_dim[task_id])\n",
    "  s_decoder = svhn_vae1.Decoder(latent_dim=latent_dim[task_id], hidden_dim1=hidden_dim1,hidden_dim2=hidden_dim2, output_dim=inputdim)\n",
    "  savd_model = svhn_vae1.Model(Encoder=s_encoder, Decoder=s_decoder).to(DEVICE)\n",
    "  savd_model.load_state_dict(torch.load('s_task'+str(task_id)+'_model'))\n",
    "  savd_model.eval()\n",
    "  _,ZXtest1,_ = savd_model(Xtest1[task_pred == task_id].view(Xtest1[task_pred == task_id].shape[0],inputdim))\n",
    "\n",
    "  filename = 'Spl'+str(task_id)+'_clust_model.sav'\n",
    "#   print(filename)\n",
    "  kmn_model = pickle.load(open(filename, 'rb'))\n",
    "  \n",
    "  tst1_out = kmn_model.predict(ZXtest1.cpu().detach().numpy())\n",
    "  pred_lbls = data_lbls(tst1_out, task_clust_lbls[task_id],task_id)\n",
    "  \n",
    "  newvec = torch.ones(len(loc_vec))*(task_id*2)\n",
    "\n",
    "  newvec[(ytest1[task_pred == task_id])==(task_id*2+1) ] = (task_id*2+1)\n",
    "  \n",
    "  newvec =  (newvec.cpu().detach().numpy())\n",
    "\n",
    "  print(confusion_matrix(newvec, pred_lbls,))\n",
    "  print('Task acc:',np.trace(confusion_matrix(newvec, pred_lbls,)/ytest1.shape[0]))\n",
    "  sum_tr += np.trace(confusion_matrix(newvec, pred_lbls,))\n",
    "sum_tr /= 5000\n",
    "print('Average accuracy =',sum_tr,)\n",
    "print('Time taken=', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stam_env",
   "language": "python",
   "name": "stam_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

